[
    {
        "number": 35,
        "UID": "O-35",
        "forum": "https://openreview.net/forum?id=WQIK8OJVQe",
        "Track": "Full paper",
        "Session": "Oral 1.A: Segmentation",
        "Final Decision": "Oral",
        "title": "CASC-AI: Consensus-aware Self-corrective AI Agents for Noise Cell Segmentation",
        "abstract": "Multi-class cell segmentation in high-resolution gigapixel whole slide images (WSI) is crucial for various clinical applications. However, training such models typically requires labor-intensive, pixel-wise annotations by domain experts. Recent efforts have democratized this process by involving lay annotators without medical expertise. However, conventional non-agent-based approaches struggle to handle annotation noise adaptively, as they lack mechanisms to mitigate false positives (FP) and false negatives (FN) at both the image-feature and pixel levels. In this paper, we propose a consensus-aware self-corrective AI agent that leverages the Consensus Matrix to guide its learning process. The Consensus Matrix defines regions where both the AI and annotators agree on cell and non-cell annotations, which are prioritized with stronger supervision. Conversely, areas of disagreement are adaptively weighted based on their feature similarity to high-confidence agreement regions, with more similar regions receiving greater attention. Additionally, contrastive learning is employed to separate features of noisy regions from those of reliable agreement regions by maximizing their dissimilarity. This paradigm enables the AI to iteratively refine noisy labels, enhancing its robustness. Validated on one real-world lay-annotated cell dataset and two simulated noisy datasets, our method demonstrates improved segmentation performance, effectively correcting FP and FN errors and showcasing its potential for training robust models on noisy datasets. The official implementation and cell annotations are publicly available at https://github.com/ddrrnn123/CASC-AI.",
        "authors": "Ruining Deng, Yihe Yang, David J Pisapia, Benjamin L Liechty, Junchao Zhu, Juming Xiong, Junlin Guo, Zhengyi Lu, Jiacheng Wang, Xing Yao, Runxuan Yu, Rendong Zhang, Gaurav Rudravaram, Mengmeng Yin, Pinaki Sarder, Haichun Yang, Yuankai Huo, Mert R. Sabuncu",
        "Time": "Day 1 \u2014 9:00am-10:15am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.01"
    },
    {
        "number": 35,
        "UID": "F-35",
        "forum": "https://openreview.net/forum?id=WQIK8OJVQe",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "CASC-AI: Consensus-aware Self-corrective AI Agents for Noise Cell Segmentation",
        "abstract": "Multi-class cell segmentation in high-resolution gigapixel whole slide images (WSI) is crucial for various clinical applications. However, training such models typically requires labor-intensive, pixel-wise annotations by domain experts. Recent efforts have democratized this process by involving lay annotators without medical expertise. However, conventional non-agent-based approaches struggle to handle annotation noise adaptively, as they lack mechanisms to mitigate false positives (FP) and false negatives (FN) at both the image-feature and pixel levels. In this paper, we propose a consensus-aware self-corrective AI agent that leverages the Consensus Matrix to guide its learning process. The Consensus Matrix defines regions where both the AI and annotators agree on cell and non-cell annotations, which are prioritized with stronger supervision. Conversely, areas of disagreement are adaptively weighted based on their feature similarity to high-confidence agreement regions, with more similar regions receiving greater attention. Additionally, contrastive learning is employed to separate features of noisy regions from those of reliable agreement regions by maximizing their dissimilarity. This paradigm enables the AI to iteratively refine noisy labels, enhancing its robustness. Validated on one real-world lay-annotated cell dataset and two simulated noisy datasets, our method demonstrates improved segmentation performance, effectively correcting FP and FN errors and showcasing its potential for training robust models on noisy datasets. The official implementation and cell annotations are publicly available at https://github.com/ddrrnn123/CASC-AI.",
        "authors": "Ruining Deng, Yihe Yang, David J Pisapia, Benjamin L Liechty, Junchao Zhu, Juming Xiong, Junlin Guo, Zhengyi Lu, Jiacheng Wang, Xing Yao, Runxuan Yu, Rendong Zhang, Gaurav Rudravaram, Mengmeng Yin, Pinaki Sarder, Haichun Yang, Yuankai Huo, Mert R. Sabuncu",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.01"
    },
    {
        "number": 137,
        "UID": "O-137",
        "forum": "https://openreview.net/forum?id=VQX4B2A2Y0",
        "Track": "Full paper",
        "Session": "Oral 1.A: Segmentation",
        "Final Decision": "Oral",
        "title": "Training-Free Dataset Pruning for Polyp Segmentation via Community Detection in Similarity Networks",
        "abstract": "Recent advances in deep learning have been driven by the availability of larger datasets and more complex models; however, this progress comes at the expense of substantial computational and annotation costs. To address these issues, we introduce a novel, training-free dataset pruning method, PRIME, targeting polyp segmentation in medical imaging. To this end, PRIME constructs a similarity network among the images in the target dataset and then applies community detection to retain a much smaller, yet representative subset of images from the original dataset. Unlike existing methods that require model training for dataset pruning, our PRIME completely avoids model training, thus significantly reducing computational demands. The reduction in the training dataset cuts 56.2% data annotation costs and enables 2.3$\\times$ faster training of polyp segmentation models, with only a 0.5% drop in the DICE score. Consequently, our PRIME enables efficient training, fine-tuning, and domain adaptation across medical centers, thus offering a cost-effective solution for deep learning in polyp segmentation.",
        "authors": "Md Mostafijur Rahman, Radu Marculescu",
        "Time": "Day 1 \u2014 9:00am-10:15am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.02"
    },
    {
        "number": 137,
        "UID": "F-137",
        "forum": "https://openreview.net/forum?id=VQX4B2A2Y0",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Training-Free Dataset Pruning for Polyp Segmentation via Community Detection in Similarity Networks",
        "abstract": "Recent advances in deep learning have been driven by the availability of larger datasets and more complex models; however, this progress comes at the expense of substantial computational and annotation costs. To address these issues, we introduce a novel, training-free dataset pruning method, PRIME, targeting polyp segmentation in medical imaging. To this end, PRIME constructs a similarity network among the images in the target dataset and then applies community detection to retain a much smaller, yet representative subset of images from the original dataset. Unlike existing methods that require model training for dataset pruning, our PRIME completely avoids model training, thus significantly reducing computational demands. The reduction in the training dataset cuts 56.2% data annotation costs and enables 2.3$\\times$ faster training of polyp segmentation models, with only a 0.5% drop in the DICE score. Consequently, our PRIME enables efficient training, fine-tuning, and domain adaptation across medical centers, thus offering a cost-effective solution for deep learning in polyp segmentation.",
        "authors": "Md Mostafijur Rahman, Radu Marculescu",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.02"
    },
    {
        "number": 138,
        "UID": "O-138",
        "forum": "https://openreview.net/forum?id=zi0EIXZk7V",
        "Track": "Full paper",
        "Session": "Oral 1.A: Segmentation",
        "Final Decision": "Oral",
        "title": "FLAIRBrainSeg: Fine-Grained Brain Segmentation Using FLAIR MRI Only",
        "abstract": "This paper introduces a novel method for brain segmentation using only FLAIR MRIs, specifically targeting cases where access to other imaging modalities is limited. By leveraging existing automatic segmentation methods, we train a network to approximate segmentations, typically obtained from T1-weighted MRIs. Our method, called FLAIRBrainSeg, produces segmentations of 132 structures and is robust to multiple sclerosis lesions. Experiments on both in-domain and out-of-domain datasets demonstrate that our method outperforms modality-agnostic approaches based on image synthesis, the only currently available alternative for performing brain parcellation using FLAIR MRI alone. This technique holds promise for scenarios where T1-weighted MRIs are unavailable and offers a valuable alternative for clinicians and researchers in need of reliable anatomical segmentation.",
        "authors": "Le Bot Edern, R√©mi Giraud, Boris Mansencal, Thomas tourdias, Jose V Manjon, Pierrick Coupe",
        "Time": "Day 1 \u2014 9:00am-10:15am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.03"
    },
    {
        "number": 138,
        "UID": "F-138",
        "forum": "https://openreview.net/forum?id=zi0EIXZk7V",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "FLAIRBrainSeg: Fine-Grained Brain Segmentation Using FLAIR MRI Only",
        "abstract": "This paper introduces a novel method for brain segmentation using only FLAIR MRIs, specifically targeting cases where access to other imaging modalities is limited. By leveraging existing automatic segmentation methods, we train a network to approximate segmentations, typically obtained from T1-weighted MRIs. Our method, called FLAIRBrainSeg, produces segmentations of 132 structures and is robust to multiple sclerosis lesions. Experiments on both in-domain and out-of-domain datasets demonstrate that our method outperforms modality-agnostic approaches based on image synthesis, the only currently available alternative for performing brain parcellation using FLAIR MRI alone. This technique holds promise for scenarios where T1-weighted MRIs are unavailable and offers a valuable alternative for clinicians and researchers in need of reliable anatomical segmentation.",
        "authors": "Le Bot Edern, R√©mi Giraud, Boris Mansencal, Thomas tourdias, Jose V Manjon, Pierrick Coupe",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.03"
    },
    {
        "number": 178,
        "UID": "O-178",
        "forum": "https://openreview.net/forum?id=lK0CklgxQd",
        "Track": "Full paper",
        "Session": "Oral 1.A: Segmentation",
        "Final Decision": "Oral",
        "title": "Two Heads Are Enough: DualU-Net, a Fast and Efficient Architecture for Nuclei Instance Segmentation",
        "abstract": "Accurate detection and classification of cell nuclei in histopathological images are critical for both clinical diagnostics and large-scale digital pathology workflows. In this work, we introduce DualU-Net, a fully convolutional, multi-task architecture designed to streamline nuclei classification and segmentation. Unlike the widely adopted three-decoder paradigm of HoVer-Net, DualU-Net employs only two output heads: a segmentation decoder that predicts pixel-wise classification maps and a detection decoder that estimates Gaussian-based centroid density maps. By leveraging these two outputs, our model effectively reconstructs instance-level segmentations. The proposed architecture results in significantly faster inference, reducing processing time by up to x5 compared to HoVer-Net, while achieving classification and detection performance comparable to State-of-the-Art models. Additionally, our approach demonstrates greater computational efficiency than CellViT and NuLite. We further show that DualU-Net is more robust to staining variations, a common challenge in digital pathology workflows. The model has been successfully deployed in clinical settings as part of the DigiPatICS initiative, operating across eight hospitals within the Institut Catal√† de la Salut (ICS) network, highlighting the practical viability of DualU-Net as an efficient and scalable solution for nuclei segmentation and classification in real-world pathology applications. The code and pretrained model weights are publicly available on https://github.com/davidanglada/DualU-Net.",
        "authors": "David Anglada-Rotger, Berta Jansat, Ferran Marques, Montse Pard√†s",
        "Time": "Day 1 \u2014 9:00am-10:15am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.04"
    },
    {
        "number": 178,
        "UID": "F-178",
        "forum": "https://openreview.net/forum?id=lK0CklgxQd",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Two Heads Are Enough: DualU-Net, a Fast and Efficient Architecture for Nuclei Instance Segmentation",
        "abstract": "Accurate detection and classification of cell nuclei in histopathological images are critical for both clinical diagnostics and large-scale digital pathology workflows. In this work, we introduce DualU-Net, a fully convolutional, multi-task architecture designed to streamline nuclei classification and segmentation. Unlike the widely adopted three-decoder paradigm of HoVer-Net, DualU-Net employs only two output heads: a segmentation decoder that predicts pixel-wise classification maps and a detection decoder that estimates Gaussian-based centroid density maps. By leveraging these two outputs, our model effectively reconstructs instance-level segmentations. The proposed architecture results in significantly faster inference, reducing processing time by up to x5 compared to HoVer-Net, while achieving classification and detection performance comparable to State-of-the-Art models. Additionally, our approach demonstrates greater computational efficiency than CellViT and NuLite. We further show that DualU-Net is more robust to staining variations, a common challenge in digital pathology workflows. The model has been successfully deployed in clinical settings as part of the DigiPatICS initiative, operating across eight hospitals within the Institut Catal√† de la Salut (ICS) network, highlighting the practical viability of DualU-Net as an efficient and scalable solution for nuclei segmentation and classification in real-world pathology applications. The code and pretrained model weights are publicly available on https://github.com/davidanglada/DualU-Net.",
        "authors": "David Anglada-Rotger, Berta Jansat, Ferran Marques, Montse Pard√†s",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.04"
    },
    {
        "number": 211,
        "UID": "O-211",
        "forum": "https://openreview.net/forum?id=YuHh2JioCs",
        "Track": "Full paper",
        "Session": "Oral 1.A: Segmentation",
        "Final Decision": "Oral",
        "title": "Mesh-Prompted Anatomy Segmentation",
        "abstract": "We present a novel technique for segmenting anatomical structures in medical images by using a canonical mesh as a prompt for the structure to be segmented. Unlike point-prompted segmentation methods, such as those based on Segment-Anything Models, mesh prompting reduces the ambiguity associated with point prompts and provides a stronger shape prior, which is particularly advantageous for many medical applications. Our approach performs mesh-prompted segmentation by registering the signed distance function (SDF) of the mesh to the target image using a vector-field attention network trained with boundary-based loss terms. Before registration, the prompted mesh is roughly aligned with the structure in the target image using a center prompt provided by the user. This method allows for independent initialization of each structure's position and the prediction of deformation fields specific to each structure, which offers advantages over segmentation via direct image registration that typically relies on a single deformation field to accommodate all structures. Additionally, it preserves surface correspondence better than image registration using region-based loss terms. We evaluate our method on two CT datasets featuring common ear and body structures. A comparison of our technique with image registration and other state-of-the-art segmentation methods shows that our approach achieves superior segmentation accuracy.",
        "authors": "Dingjie Su, Yihao Liu, Lianrui Zuo, Benoit Dawant",
        "Time": "Day 1 \u2014 9:00am-10:15am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.05"
    },
    {
        "number": 211,
        "UID": "F-211",
        "forum": "https://openreview.net/forum?id=YuHh2JioCs",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Mesh-Prompted Anatomy Segmentation",
        "abstract": "We present a novel technique for segmenting anatomical structures in medical images by using a canonical mesh as a prompt for the structure to be segmented. Unlike point-prompted segmentation methods, such as those based on Segment-Anything Models, mesh prompting reduces the ambiguity associated with point prompts and provides a stronger shape prior, which is particularly advantageous for many medical applications. Our approach performs mesh-prompted segmentation by registering the signed distance function (SDF) of the mesh to the target image using a vector-field attention network trained with boundary-based loss terms. Before registration, the prompted mesh is roughly aligned with the structure in the target image using a center prompt provided by the user. This method allows for independent initialization of each structure's position and the prediction of deformation fields specific to each structure, which offers advantages over segmentation via direct image registration that typically relies on a single deformation field to accommodate all structures. Additionally, it preserves surface correspondence better than image registration using region-based loss terms. We evaluate our method on two CT datasets featuring common ear and body structures. A comparison of our technique with image registration and other state-of-the-art segmentation methods shows that our approach achieves superior segmentation accuracy.",
        "authors": "Dingjie Su, Yihao Liu, Lianrui Zuo, Benoit Dawant",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - O.05"
    },
    {
        "number": 87,
        "UID": "F-87",
        "forum": "https://openreview.net/forum?id=WkgOskruDd",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "RCSegNeXt: Efficient multi-scale ConvNeXt for rectal cancer segmentation from sagittal MRI scans",
        "abstract": "Rectal cancer remains a critical global health challenge, significantly contributing to mor-\nbidity and mortality worldwide. Magnetic resonance imaging (MRI) in a sagittal plane\noffers distinct advantages for rectal cancer diagnosis by providing detailed visualization of\nthe rectum and its surrounding anatomy. However, automated segmentation of the rectum\nand associated tumors remains difficult due to tumor heterogeneity and complex anatom-\nical structure, which necessitate multi-scale feature extraction. This study proposes RC-\nSegNeXt, a novel non-uniform pure-convolutional rectal cancer segmentation architecture\nthat combines shallow anisotropic stages with deep isotropic stages. The anisotropic stages\nleverage AniNeXt blocks, designed with customized convolutional kernels and pooling op-\nerations to address the uneven spatial resolution inherent in MRI data. In the isotropic\nstages, an IsoNeXt block with a Scale-Aware Integration Module (SAIM) enables efficient\nmulti-scale feature fusion by directing information flow through constrained pathways. This\ndesign enhances computational efficiency while achieving superior segmentation accuracy.\nExperiments on two in-house datasets demonstrate the proposed method‚Äôs state-of-the-art\nperformances. Code will be open upon acceptance.",
        "authors": "Wang Bo, Ting Xue, Leyang Pan, Dingfu Huang, Yi Xiao, Li Fan, Zaiyi Liu, Shiyuan Liu, S Kevin Zhou",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.01"
    },
    {
        "number": 91,
        "UID": "F-91",
        "forum": "https://openreview.net/forum?id=AUv6NhK9aH",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Zero-shot capability of 2D SAM-family models for bone segmentation in CT scans",
        "abstract": "The Segment Anything Model (SAM) and similar models build a family of promptable foundation models (FMs) for image and video segmentation. The object of interest is identified using prompts---user provided input such as bounding boxes or points---and the models have shown very promising results when it comes to generalization to new tasks. However, extensive evaluation studies are required for medical applications, to assess their strengths and weaknesses in clinical settings. As the performance of those models is highly dependent on the quality and quantity of their prompts, it is necessary to thoroughly benchmark the different options. Currently, no dedicated evaluation studies exist specifically for bone segmentation in CT scans. Leveraging high-quality private and public datasets on four skeletal regions, we test the zero-shot capabilities of SAM-family models for bone CT segmentation, using non-interactive prompting strategies, composed of bounding box, points and combinations of the two. Additionally, we design a guideline for informed decision-making in 2D non-interactive prompting based on our insights on segmentation performance and inference time. Our results show that SAM and SAM2 currently outperform medically fine-tuned FMs, and prompted with a bounding box together with a center point have the best performance across all tested settings.",
        "authors": "Caroline Magg, Clara I. S√°nchez, Hoel Kervadec",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.02"
    },
    {
        "number": 93,
        "UID": "F-93",
        "forum": "https://openreview.net/forum?id=erHgJGtptZ",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Data-Agnostic Augmentations for Unknown Variations: Out-of-Distribution Generalisation in MRI Segmentation",
        "abstract": "Medical image segmentation models are often trained on curated datasets, leading to performance degradation when deployed in real-world clinical settings due to mismatches between training and test distributions. While data augmentation techniques are widely used to address these challenges, traditional visually consistent augmentation strategies lack the robustness needed for diverse real-world scenarios. In this work, we systematically evaluate alternative augmentation strategies, focusing on MixUp and Auxiliary Fourier Augmentation. These methods mitigate the effects of multiple variations without explicitly targeting specific sources of distribution shifts. We demonstrate how these techniques significantly improve out-of-distribution generalization and robustness to imaging variations across a wide range of transformations in cardiac cine MRI and prostate MRI segmentation. We quantitatively find that these augmentation methods enhance learned feature representations by promoting separability and compactness. Additionally, we highlight how their integration into nnU-Net training pipelines provides an easy-to-implement, effective solution for enhancing the reliability of medical segmentation models in real-world applications. Our code is available at: https://github.com/MIAGroupUT/augmentations-for-the-unknown.",
        "authors": "Mei Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.03"
    },
    {
        "number": 103,
        "UID": "F-103",
        "forum": "https://openreview.net/forum?id=ZQdMGQRELW",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Evaluation of Virtual Stain Multiplexed CD68 for Macrophage Detection in NSCLC PD-L1 Slides",
        "abstract": "Manual reading of tissue slides by pathologists serves both as a foundation for clinical decision-making and as a source of ground truth for training artificial intelligence (AI) models. However, challenges such as inter-observer variability, limited tissue availability, and complex annotation tasks often compromise reliability and scalability. This study exemplifies a broader trend in pathology: leveraging virtual staining and other AI-based methodologies to address these challenges. \nWe applied virtual stain multiplexing to a challenging annotation task - macrophage identification in non-small cell lung cancer tissue PD-L1 IHC stains, demonstrating its ability to improve pathologist performance and inter-observer agreement. In six challenging regions selected from 49 curated whole slide images, virtual staining significantly increased macrophage detection consistency, with Fleiss' kappa improving from -0.1 to 0.62, and enhanced overall accuracy, with the F1 score increasing from 0.13 to 0.65.\nThese results highlight the potential use of AI-based virtual staining to assist pathologists reading slides, thereby improving consistency, enhancing accuracy, and alleviating the dependence on additional costly staining. Virtual stain multiplexing demonstrates a generalizable approach to improving pathologist performance through measurement-based AI tools, addressing broader needs for reproducibility and efficiency in diagnostic pathology.",
        "authors": "Elad Arbel, Oded Ben-David, Itay Remer, Amir Ben-Dor, Daniela Rabkin, Sarit Aviel-Ronen, Frederik Aidt, Tine Hagedorn-Olsen, Lars Jacobsen, Kristopher Kersch, Jim Christian, Quyen Nguyen, Anya Tsalenko",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.04"
    },
    {
        "number": 122,
        "UID": "F-122",
        "forum": "https://openreview.net/forum?id=nICgVRhKgG",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Equivariant Imaging Biomarkers for Robust Unsupervised Segmentation of Histopathology",
        "abstract": "Histopathology evaluation of tissue specimens through microscopic examination is essential for accurate disease diagnosis and prognosis. However, traditional manual analysis by specially trained pathologists is time-consuming, labor-intensive, cost-inefficient, and prone to inter-rater variability, potentially affecting diagnostic consistency and accuracy. As digital pathology images continue to proliferate, there is a pressing need for automated analysis to address these challenges. Recent advancements in artificial intelligence-based tools such as machine learning (ML) models, have significantly enhanced the precision and efficiency of analyzing histopathological slides. However, despite their impressive performance, ML models are invariant only to translation, lacking invariance to rotation and reflection. This limitation restricts their ability to generalize effectively, particularly in histopathology, where images intrinsically lack meaningful orientation. In this study, we develop robust, equivariant histopathological biomarkers through a novel symmetric convolutional kernel via unsupervised segmentation. The approach is validated using prostate tissue micro-array (TMA) images from 50 patients in the Gleason 2019 Challenge public dataset. The biomarkers extracted through this approach demonstrate enhanced robustness and generalizability against rotation compared to models using standard convolution kernels, holding promise for enhancing the accuracy, consistency, and robustness of ML models in digital pathology. Ultimately, this work aims to improve diagnostic and prognostic capabilities of histopathology beyond prostate cancer through equivariant imaging. The code will be made publicly available upon acceptance.",
        "authors": "Fuyao Chen, Yuexi Du, Tal Zeevi, Nicha C Dvornek, John A Onofrey",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.05"
    },
    {
        "number": 126,
        "UID": "F-126",
        "forum": "https://openreview.net/forum?id=KrcJljKdOy",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "MAIS: Memory-Attention for Interactive Segmentation",
        "abstract": "Interactive medical segmentation reduces annotation effort by refining predictions through user feedback. Vision Transformer (ViT)-based models, such as the Segment Anything Model (SAM), achieve state-of-the-art performance using user clicks and prior masks as prompts. However, existing methods treat interactions as independent events, leading to redundant corrections and limited refinement gains. We address this by introducing MAIS, a Memory-Attention mechanism for Interactive Segmentation that stores past user inputs and segmentation states, enabling temporal context integration. Our approach enhances ViT-based segmentation across diverse imaging modalities, achieving more efficient and accurate refinements.",
        "authors": "Mauricio Orbes, Oeslle Lucena, Sebastien Ourselin, M. Jorge Cardoso",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.06"
    },
    {
        "number": 143,
        "UID": "F-143",
        "forum": "https://openreview.net/forum?id=ozFfz2PCtT",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "SurgicalSemiSeg: A Semi-Supervised Framework for Laparoscopic Image Segmentation",
        "abstract": "Deep learning applications in surgery are heavily reliant on large-scale datasets with high-quality annotations, which are costly and time-consuming to obtain. Self-supervised learning (SSL) has shown significant potential for reducing reliance on labelled data.\nThis work investigates the use of SSL for semantic segmentation in laparoscopic cholecystectomy (LC) surgery. Through evaluation of existing SSL methods, we find that pixel-level objectives enable the most effective representation learning for laparoscopic imaging, characterised by highly variable and deformable anatomy. Building on this insight, we develop a tailored masked denoising autoencoder with a carefully optimised masking ratio and patch size for semantic segmentation. Our method achieves state-of-the-art performance across three LC datasets. Of note, it significantly improves segmentation accuracy for critical anatomical structures that are under-represented in training datasets. Furthermore, our approach achieves generalisability, with pre-trained representations performing effectively across fine-tuning datasets from different LC datasets.",
        "authors": "Yuning Zhou, Henry Badgery, Matthew Read, James Bailey, Catherine E Davey",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.07"
    },
    {
        "number": 146,
        "UID": "F-146",
        "forum": "https://openreview.net/forum?id=VdtyUxL2u1",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "How to select slices for annotation to train best-performing deep learning segmentation models for cross-sectional medical images?",
        "abstract": "Automated segmentation of medical images heavily relies on the availability of precise manual annotations. However, generating these annotations is often time-consuming, expensive, and sometimes requires specialized expertise (especially for cross-sectional medical images). Therefore, it is essential to optimize the use of annotation resources to ensure efficiency and effectiveness. In this paper, we systematically address the question: \"in a non-interactive annotation pipeline, how should slices from cross-sectional medical images be selected for annotation to maximize the performance of the resulting deep learning segmentation models?\"\nWe conducted experiments on 4 medical imaging segmentation tasks with varying annotation budgets, numbers of annotated cases, numbers of annotated slices per volume, slice selection techniques, and mask interpolations. \nWe found that:\n1) It is almost always preferable to annotate fewer slices per volume and more volumes given an annotation budget. \n2) Selecting slices for annotation by unsupervised active learning (UAL) is not superior to selecting slices randomly or at fixed intervals, provided that each volume is allocated the same number of annotated slices. \n3) Interpolating masks between annotated slices rarely enhances model performance, with exceptions of some specific configuration for 3D models.",
        "authors": "Yixin Zhang, Kevin Kramer, Maciej A Mazurowski",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.08"
    },
    {
        "number": 156,
        "UID": "F-156",
        "forum": "https://openreview.net/forum?id=LahvcrntPI",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Enhancing Low Back Pain Assessment with Diffusion Models for Lumbar Spine MRI Segmentation",
        "abstract": "This study introduces a diffusion based framework for robust and accurate se4 mantic segmentation of lumbar spine MRI scans from patients with low back pain (LBP), regardless of whether the scans are T1w or T2- weighted. We compared with advanced models for segmenting vertebrae, intervertebral discs (IVDs), and spinal canal using the SPIDER dataset. The results showed that SpineSegDiff achieved state-of-the-art performance, particularly in the identification of degenerated IVDs.\n In addition, the uncertainty maps generated by our model provide valuable insights for clinical  review, enhancing the robustness and reliability of the segmentation results. The potential of diffusion models to enhance the diagnosis and management of LBP through more precise analysis of pathological spine MRI is underscored by our findings.",
        "authors": "Maria Monzon, Thomas Iff, Ender Konukoglu, Catherine R Jutzeler",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.09"
    },
    {
        "number": 185,
        "UID": "F-185",
        "forum": "https://openreview.net/forum?id=aTFkMKn66q",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Cardiac Computed Tomography Angiography Plane Prediction and Comprehensive LV Segmentation",
        "abstract": "The use of cardiac computed tomography angiography (CCTA) has dramatically increased over the past decade, with an increasingly recognized role for functional assessment; however, reformatting these datasets into standard cardiac planes and performing quantitative\nanalysis remains time consuming and disruptive to clinical workflows. Here, we propose a fully automated, volumetric, end-to-end trained network for simultaneous detection of standard cardiac planes and comprehensive left ventricular (LV) segmentation in the predicted short axis coordinate system. The architecture consists of a coarse segmentation module, a transformation module, and a fine segmentation module. The coarse segmentation module provides an initial segmentation of the full field of view (FOV) axial images at low resolution. The transformation module predicts the rotations corresponding to the standard cardiac planes (short axis, SAX; two chamber, 2CH; three chamber, 3CH; and four chamber, 4CH) and reformats the source volume into the predicted SAX coordinate system at high resolution. Finally, the fine segmentation module segments the narrow FOV, high resolution SAX volume. The dataset consisted of 313 CCTA studies partitioned into training, validation, and testing in an 80:10:10 split. Architectural decisions are justified using ablation experiments. On the test set, the proposed architecture achieved high quality segmentations (Dice scores of 0.955, 0.928, and 0.808 for the bloodpool, myocardium, and trabeculations, respectively) and accurate plane predictions (mean angle errors of $9.126^\\circ$, $9.480^\\circ$, $9.030^\\circ$, and $8.840^\\circ$ for the SAX, 2CH, 3CH, and 4CH planes, respectively). This fully automated pipeline has the potential to replace current manual workflows, expediting the availability of standard cardiac planes and quantitative analysis for interpretation.",
        "authors": "Davis Marc Vigneault, Ashish Manohar, Abraham Hernandez, Krista Tin Chi Wong, Fanwei Kong, Tea Gegenava, Dominik Fleischmann, Koen Nieman",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.10"
    },
    {
        "number": 190,
        "UID": "F-190",
        "forum": "https://openreview.net/forum?id=0BQ6JPGwZa",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Adversarial Perturbations Improve Generalization of Confidence Prediction in Medical Image Segmentation",
        "abstract": "Trustworthy methods for medical image segmentation should come with a reliable mechanism to estimate the quality of their results. Training a separate component for confidence prediction is relatively fast, and can easily be adapted to different quality metrics. However, the resulting estimates are usually not sufficiently reliable under domain shifts, for example when images are taken with different devices. We introduce a novel adversarial strategy for training confidence predictors for the widely used U-Net architecture that greatly improves such generalization. It is based on creating adversarial image perturbations, aimed at substantially decreasing segmentation quality, via the gradients of the confidence predictor, leading to images outside of the original training distribution. We observe that these perturbations initially have little effect on segmentation quality. However, including them in the training gradually improves the confidence predictor's understanding of what actually affects segmentation quality when moving outside of the training distribution. On two different medical image segmentation tasks, we demonstrate that this strategy substantially improves estimates of volumetric and surface Dice on out-of-distribution images.",
        "authors": "Jonathan Lennartz, Thomas Schultz",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.11"
    },
    {
        "number": 207,
        "UID": "F-207",
        "forum": "https://openreview.net/forum?id=bidt9WsLiE",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Effective Disjoint Representational Learning for Anatomical Segmentation",
        "abstract": "In the wake of the limited availability of pertinent datasets, the application of computer vision methods for semantic segmentation of abdominal structures is mainly constrained to surgical instruments or organ-specific segmentations. Multi-organ segmentation has the potential to furnish supplementary assistance in multifarious domains in healthcare, for instance, robot-assisted laparoscopic surgery. However, in addition to the complexity involved in discriminating anatomical structures due to their visual attributes and operative conditions, the representation bias pertaining to organ size results in poor segmentation performance on organs with smaller pixel proportions. In this work, we focus on alleviating the influence of representation bias by involving different encoder-decoder frameworks for learning organ-specific features. In particular, we investigate the effect of organ-specific decoders on binary segmentation of anatomical structures in abdominal surgery. Additionally, we analyze the effect of organ-specific pretraining on the multi-label segmentation in two model training settings including knowledge sharing and disjoint learning, in relation to the contextual feature sharing between organ-specific decoders. Our results illustrate the significant gain in segmentation performance by incorporating organ-specific decoders, especially for less represented organs.",
        "authors": "Priya Tomar, Aditya Parikh, Philipp Feodorovici, Jan Arensmeyer, Hanno Matthaei, Christian Bauckhage, Helen Schneider, Rafet Sifa",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.12"
    },
    {
        "number": 218,
        "UID": "F-218",
        "forum": "https://openreview.net/forum?id=7IHGjaOCch",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Multi-centric Comparison of Deep Learning Models for Lesion Detection in Breast MRI",
        "abstract": "Breast magnetic resonance imaging (MRI) is a common modality for diagnostic imaging\nin breast cancer, creating a need for automated image analysis to assist in early detection\nand diagnosis.\n\nIn this study, we compare multiple deep learning-based segmentation and detection\nalgorithms for lesion detection in dynamic contrast-enhanced (DCE) breast MRI. We utilized a large multi-centric dataset comprising T1-weighted DCE MR images from nine clinical sites across seven countries, encompassing diverse imaging characteristics and scanner\ntypes. We evaluated several models, including the standard nnU-Net, an adapted nnU-Net with modifications to reduce false positives, a coarse-resolution version thereof, the\ntransformer-based SwinUNETR-V2, and nnDetection.\n\nThe standard nnU-Net achieved a high lesion-level sensitivity of 83.8% but produced\nan average of 3.334 false positives per case, which is impractical for clinical use. The\nadapted (coarse) nnU-Net significantly reduced false positives to 0.666 (0.397) per case with\na slight decrease in sensitivity to 79.9% (75.8%). SwinUNETR-V2 achieved comparable\nperformance to the adapted nnU-Net. nnDetection outperformed nnU-Net in the high-sensitivity region, but performed worse than the adapted models in the lower-sensitivity\nregion, with respect to false positives. To conclude, the nnU-Net again provides a good\nbaseline, but our lesion detection task motivates adaptations to reduce the number of false\npositives.",
        "authors": "Kai Gei√üler, Markus Wenzel, Susanne Diekmann, Heinrich von Busch, Robert Grimm, Hans Meine",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.13"
    },
    {
        "number": 238,
        "UID": "F-238",
        "forum": "https://openreview.net/forum?id=uQaPr1wU1W",
        "Track": "Full paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "SACP: Spatially-Aware Conformal Prediction in Uncertainty Quantification of Medical Image Segmentation",
        "abstract": "Conformal Prediction provides statistical coverage guarantees for uncertainty quantification but fails to account for spatially varying importance of predictive uncertainty in medical image segmentation. This paper introduces a spatially-aware conformal prediction framework that enhances uncertainty quantification by incorporating spatial context near critical anatomical interfaces such as a vessel or critical organ. Our framework consists of three key components: (1) a base nonconformity score derived from segmentation model probabilities, (2) a calibration mechanism that applies structure-specific importance weights based on spatial proximity, and (3) a prediction set construction method that preserves mathematical coverage guarantees while providing targeted uncertainty quantification in critical regions. The calibration mechanism employs a distance-weighted scoring function that exponentially decays with distance from key interfaces, allowing for structure-specific importance factors and adaptive uncertainty estimation. We develop pooled and domain-specific calibration strategies to handle multi-center variability, enabling robust performance across different imaging protocols and populations. We validate our approach on tumor segmentation in pancreatic adenocarcinoma imaging from two medical centers. Results demonstrate that our method achieves the desired coverage levels while generating prediction sets that adaptively expand near critical interfaces.",
        "authors": "Jacqueline Isabel Bereska, Hamed Karimi, Reza Samavi",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - F.14"
    },
    {
        "number": 49,
        "UID": "S-49",
        "forum": "https://openreview.net/forum?id=W8CXkHBrZV",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Multi-phase renal tumor segmentation with Slide-SAM assisted network",
        "abstract": "Non-contrast CT(Computed Tomography) scans often suffer from low tissue contrast and indistinct tumor boundaries, making accurate segmentation challenging. To address this, we propose SSA (Slide-SAM assisted network), a segmentation framework that leverages the pretrained Slide-SAM model guided by box prompts from contrast-enhanced CT. By transferring spatial priors across phases, SSA significantly improves segmentation accuracy on plain-phase images and achieves additional gains on enhanced phases. Experimental results highlight the effectiveness of combining vision foundation models with inter-phase guidance for robust medical image segmentation.",
        "authors": "Ruiyang Jin, Quan Quan, S Kevin Zhou",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.01"
    },
    {
        "number": 54,
        "UID": "S-54",
        "forum": "https://openreview.net/forum?id=dlnlmPxgvm",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Uncertainty aware pectoral muscle segmentation based on heteroscedastic regression",
        "abstract": "Accurate segmentation of the pectoral muscle is crucial for improving breast cancer diagnosis in mammograms. While modern deep learning models excel in segmentation, they often lack uncertainty quantification, which is essential for reliable clinical decisions. In this\nwork, we propose a novel method for modeling uncertainty in pectoral muscle segmentation by combining the prediction of probabilistic heatmaps with heteroscedastic regression. For that, we investigate both an existing and a novel loss function derived from the heteroscedastic Laplace distribution, and show that our loss function is more robust in our setting for pectoral muscle segmentation. Further, we demonstrate that our method is capable of producing heatmaps with high-likelihood predictive distributions within a single model while outperforming an ensemble baseline in terms of accuracy.",
        "authors": "Paul Zech, Christian Huemmer, Christopher Syben, Ludwig Ritschl, Sebastian Stober",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.02"
    },
    {
        "number": 67,
        "UID": "S-67",
        "forum": "https://openreview.net/forum?id=NnN0GBwH68",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Segmenting Carotid Plaques with Different SAM Prompts",
        "abstract": "Large foundation models have been built to address B-mode ultrasound segmentation for regions with homogeneous morphology. However, most of the foundation models still rely on human input to provide interactive prompting during inference. We explored the prompt type and generation when utilizing the ultrasound foundation model, UltraSAM for the plaque segmentation problem and compared the prompt performance with the ground truth baselines. The results showed that automatic generated prompt variants can reach similar performance with ground truth prompting in the bounding box segmentation of ultrasound plaque segmentation.",
        "authors": "Zhiwei Zhang, Yimeng Dou, Robert J Dempsey, Carol C. Mitchell, Tomy Varghese",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.03"
    },
    {
        "number": 70,
        "UID": "S-70",
        "forum": "https://openreview.net/forum?id=ToMKN2GkDZ",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Segmentation of Optic Nerve and Lateral Ventricles in Low-Dose Non-Contrast CT with nnU-Net: A Pilot Study",
        "abstract": "In this study, we evaluate the nnU-Net, a self-configuring deep learning framework, for automated segmentation of the optic nerve and lateral ventricles from pediatric low-dose non-contrast brain CT scans. The segmentation performance is assessed using the Dice score via cross-validation. The overall mean Dice scores of 0.90 for the ventricles and 0.85 for the optic nerve indicate that nnU-Net can achieve promising segmentation performance in this context. In addition, we had human raters review the segmentations and provide revisions for even slight defects. The segmentations require few or no revisions, supporting the feasibility of structure-specific nnU-Net segmentation in low-dose CT and its potential to streamline annotation in future practical applications.",
        "authors": "Hao Li, Barite Gutama, Erin N Abbott, Annika Coleman, Matthew Pontell, Ipek Oguz",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.04"
    },
    {
        "number": 78,
        "UID": "S-78",
        "forum": "https://openreview.net/forum?id=uTTOhthEDR",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Pruning nnU-Net with Minimal Performance Loss",
        "abstract": "nnU-Net is widely known for its accurate and robust segmentation performance in medical imaging tasks. However, the trained networks are typically heavily parameterized, and the high computational demand limit their deployment on devices with constrained resources. In this paper, we show that more than $80\\%$ of the trained nnU-Net weights can be removed without significant performance degradation, maintaining a proxy Dice score of $>0.95$. This applies to both 2D and 3D configurations across four different medical image segmentation datasets. Interestingly, we observe that critical weights consistently concentrate near the U-Net encoder and decoder ends, while the bottleneck layers can be heavily pruned. These findings highlight the significant weight redundancy in nnU-Net and suggest opportunities for further optimization, to facilitate deployment of the model on devices with limited resources.",
        "authors": "Tongyun Yang, Yidong Zhao, Qian Tao",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.05"
    },
    {
        "number": 79,
        "UID": "S-79",
        "forum": "https://openreview.net/forum?id=rvJhX1firb",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "ReCoSeg: Residual-Guided Cross-Modal Diffusion for Efficient Brain Tumor Segmentation",
        "abstract": "Precise segmentation of brain tumors from MRI scans is important in clinical practice for effective diagnosis and proper treatment planning. Diffusion models have been highly effective in image generation and segmentation. In this work, we introduce ReCoSeg, a novel semi supervised framework that combines cross-modal diffusion based synthesis with residual guided segmentation to improve accuracy. First, a diffusion model synthesizes the T1ce modality from existing FLAIR, T1, and T2 MRI scans. The synthesized and the real image difference captured as residuals‚Äîhighlights potential tumor regions. Residuals are then used as attention cues in a lightweight U-Net for segmentation, reducing the reliance on dense labels with an enhanced segmentation performance.",
        "authors": "SARA YAVARI, Rahul Pandya, Jacob Furst",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.06"
    },
    {
        "number": 90,
        "UID": "S-90",
        "forum": "https://openreview.net/forum?id=74Ox2Ivqkf",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "End-to-End Patch-Based Semantic Segmentation for Paramagnetic Rim Lesion Detection",
        "abstract": "Paramagnetic Rim Lesions (PRLs) are an emerging biomarker of chronic active inflammation in Multiple Sclerosis (MS) but their visual identification on susceptibility-sensitive MRI remains challenging and time-intensive. Due to the scarcity of PRLs, existing automated methods rely on patch-based classification, where a lesion-centered 3D patch is classified as PRL or non-PRL. However, MS lesions often occur in clusters, so a single patch may contain multiple types of lesions. Moreover, this approach requires prior extraction of lesion-centered patches which complicates the reconstruction of whole-brain predictions. To overcome this, we propose an end-to-end, whole-brain pipeline that generates patches on the fly and directly delineates PRLs within them, eliminating the need for lesion-centered extraction and enabling more precise and user-friendly automated PRL detection.",
        "authors": "Zineb El yamani, Antoine Theberge, Manon Edde, Olivier Grimard, Emmanuelle Lapointe, Stefano Magon, Muhamed Barakovic, Maxime Descoteaux, Francois Rheault",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.07"
    },
    {
        "number": 95,
        "UID": "S-95",
        "forum": "https://openreview.net/forum?id=mf46VVoZoG",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Is Medical Pretraining Enough When the Modality Is Different? A Study on Endoscopic Polyp Segmentation",
        "abstract": "Using pretrained models for fine-tuning is a widely adopted strategy in medical imaging, where labeled data is scarce. ImageNet remains the standard for pretraining in computer vision tasks, including medical imaging. RadImageNet, a medical-specific alternative trained on radiological data, has shown promising results in radiology-focused applications; however, its effectiveness in non-radiological modalities, such as endoscopy, remains unexplored. In this study, we conduct a focused evaluation of how transfer learning from ImageNet and RadImageNet affects performance in endoscopic segmentation. We compare two backbone architectures---ResNet-50 and ViT-Small---each integrated into a DeepLabV3+ decoder, and evaluate their performance on three public polyp segmentation datasets: CVC-ClinicDB, Kvasir-SEG and SUN-SEG. Our results show that ImageNet-pretrained models consistently outperform those pretrained on RadImageNet. These findings challenge the notion that medical-domain pretraining is universally beneficial and underscore the importance of modality alignment when selecting pretrained models for medical image analysis.",
        "authors": "Dipika Boro, Yu Cao, Benyuan Liu, Qilei Chen",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.08"
    },
    {
        "number": 100,
        "UID": "S-100",
        "forum": "https://openreview.net/forum?id=sBUhppId5Q",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with SAM2",
        "abstract": "Current methods for pathology image segmentation typically treat 2D slices independently, ignoring valuable cross-slice information. We present PathSeqSAM, a novel approach that treats 2D pathology slices as sequential video frames using SAM2's memory mechanisms. Our method introduces a distance-aware attention mechanism that accounts for variable physical distances between slices and employs LoRA for domain adaptation. Evaluated on the KPI Challenge 2024 dataset for glomeruli segmentation, PathSeqSAM demonstrates improved segmentation quality, particularly in challenging cases that benefit from cross-slice context. We have publicly released our code at [PathSeqSAM](https://github.com/JackyyyWang/PathSeqSAM).",
        "authors": "Mingyang Zhu, Yinting Liu, Mingyu Li, Jiacheng Wang",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.09"
    },
    {
        "number": 101,
        "UID": "S-101",
        "forum": "https://openreview.net/forum?id=vVQlsfd3tr",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Privacy-Preserving Operating Room Workflow Analysis using Digital Twins",
        "abstract": "The operating room (OR) is a complex environment where optimizing workflows is critical to reduce costs and improve patient outcomes. While computer vision approaches for automatic recognition of perioperative events can identify bottlenecks for OR optimization, privacy concerns limit the use of OR videos for automated event detection. We propose a two-stage pipeline for privacy-preserving OR video analysis and event detection. First, we leverage vision foundation models for depth estimation and semantic segmentation to generate de-identified Digital Twins (DT) of the OR from conventional RGB videos. Second, we employ the SafeOR model, a fused two-stream approach that processes segmentation masks and depth maps for OR event detection. Evaluation on an internal dataset of 38 simulated surgical trials with five event classes shows that our DT-based approach achieves performance on par with‚Äîand sometimes better than‚Äîraw RGB video-based models for OR event detection. Digital Twins enable privacy-preserving OR workflow analysis, facilitating the sharing of de-identified data across institutions and potentially enhancing model generalizability by mitigating domain-specific appearance differences.",
        "authors": "Alejandra Perez, Han Zhang, Yu-Chun Ku, Lalithkumar Seenivasan, Roger D. Soberanis-Mukul, Jose L. Porras, Richard Day, Jeffrey K Jopling, Peter Najjar, Mathias Unberath",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.10"
    },
    {
        "number": 103,
        "UID": "S-103",
        "forum": "https://openreview.net/forum?id=ZkgmlmyJ8o",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "MAESTRO: Supporting Real-Time Segmentation Networks on Surgical Systems via Edge Computing",
        "abstract": "Deep neural networks (DNNs) enable accurate segmentation of surgical video streams,\nbut their high computational and memory demands pose challenges for deployment on\nresource-constrained surgical systems. We present MAESTRO, an adaptive edge comput-\ning architecture that supports real-time execution of segmentation networks on surgical\nplatforms. MAESTRO uses split learning to partition inference between the surgical de-\nvice and an edge server, dynamically selecting the optimal cut layer to balance latency,\nenergy consumption, and data privacy. We evaluate MAESTRO using a YOLOv11 model\ntrained on the Dresden Surgical Anatomy Dataset (DSAD) and tested on Da Vinci robotic\nsurgery videos. Experiments demonstrate up to 43% latency reduction and 56% energy\nsavings compared to full offloading, while maintaining low data leakage risk. MAESTRO\nprovides a flexible and efficient solution for deploying segmentation networks in real-time,\nprivacy-sensitive surgical environments, and generalizes to other low-resource applications.",
        "authors": "Francesco Fiorella, Andrea Pinto, Lin Guo, Alessio Sacco, Mohammad Mahmoud, Flavio Esposito",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.11"
    },
    {
        "number": 109,
        "UID": "S-109",
        "forum": "https://openreview.net/forum?id=y6g0cu8q19",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Why is patch size important? Dealing with context variability in segmentation networks",
        "abstract": "Deep learning models have made significant advancements in medical image segmentation. Patch-based training is the standard practice for 3D segmentation models. During model deployment in hospital settings, resource constraints may require performing inference with\nreduced patch sizes. However, this might lead to a decrease in performance. In this study we demonstrate that patch size augmentation is a straightforward and effective approach to enhance the robustness of a 3D U-Net to different patch sizes during inference. Furthermore,\nwe show that using a hypernetwork to adapt the U-Net to diverse patch sizes further enhances performance across the patch size spectrum.",
        "authors": "Marc Balle Sanchez, Samuel JOUTARD, Mohammad Farid Azampour, Raphael Prevost",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.12"
    },
    {
        "number": 110,
        "UID": "S-110",
        "forum": "https://openreview.net/forum?id=BXCmwipKRb",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Uncertainty Estimation and Calibration in nnU-Net-Based Ischemic Stroke Lesion Segmentation",
        "abstract": "nnU-Net has become widely recognized as a state-of-the-art semantic segmentation framework. However, deep learning models are often poorly calibrated, resulting in unreliable probability estimates. Additionally, they lack meaningful uncertainty quantification. We trained an nnU-Net model to segment ischemic stroke lesions on acute-phase Diffusion-Weighted Imaging (DWI) MRI and applied a Bayesian posterior sampling approach to estimate uncertainty and improve model calibration. Our findings show that the Bayesian posterior sampling approach yields better calibration compared to a conventional nnU-Net, while providing uncertainty estimates and maintaining comparable segmentation performance.",
        "authors": "Ewout Heylen, Jelle Demeestere, Anke Wouters, Pierre Seners, Soren Christensen, Nicole Yuen, Michael Mlynash, Stephanie Kemp, Jeremy J. Heit, Gregory W. Albers, Seena Dehkharghani, Robin Lemmens, Maarten Lansberg, Frederik Maes",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.13"
    },
    {
        "number": 114,
        "UID": "S-114",
        "forum": "https://openreview.net/forum?id=HPEGd5CBrF",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Stochastic co-teaching for robust cardiac segmentation in ultrasound with noisy labels",
        "abstract": "In this work, we propose a label noise-robust segmentation framework for left ventricle blood pool segmentation in echocardiography. Based on the stochastic co-teaching approach, our method extends pixel-level filtering of label noise with additional image-level filtering to more effectively prevent noisy labels from backpropagating. We evaluate our framework on the EchoNet-Dynamic dataset, and simulate diverse noisy label scenarios, including over- and undersegmented (i.e., biased) labels. Our results demonstrate that the incorporation of image-based rejection enhances the Dice coefficient by 1.5% points and ejection fraction estimation by 2.3% points with respect to the pixel-based co-teaching framework under heavily biased label noise conditions, and thereby maintains the same performance as on clean data.",
        "authors": "Gino Jansen, Mark Schuuring, Berto Bouma, Ivana Isgum",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.14"
    },
    {
        "number": 115,
        "UID": "S-115",
        "forum": "https://openreview.net/forum?id=aJeswKFG7q",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Automated Intraoperative Lumpectomy Margin Detection using SAM-Incorporated Forward-Forward Contrastive Learning",
        "abstract": "Complete removal of cancerous tumors with a negative specimen margin during lumpectomy is essential to reduce breast cancer recurrence. However, interpretation of 2D specimen radiography (SR) by radiologists, the current method used to assess intraoperative margin status, has limited accuracy. This study aims to improve positive margin detection performance on SRs by leveraging cutting-edge deep learning models. We developed a novel lumpectomy margin assessment method using an innovative pre-training technique, Forward-Forward Contrastive Learning (FFCL), followed by few-shot segmentation training leveraging the Segment Anything Model 2 (SAM 2). Experimental results on independent annotated breast SRs demonstrate the effectiveness of the proposed FFCL-SAM method in classifying and segmenting positive margins in SRs.",
        "authors": "Tyler Ward, Braxton McFarland, Sahar Nozad, Talal Arshad, Hafsa Nebbache, Jin Chen, Xiaoqin Wang, Abdullah-Al-Zubaer Imran",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.15"
    },
    {
        "number": 121,
        "UID": "S-121",
        "forum": "https://openreview.net/forum?id=EhlNywQ3L5",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Partial Fourier loss for semantic segmentation: experiments in the spectral domain",
        "abstract": "In this short paper, we experiment with the Fourier transform as a loss for neural networks, in place of more common losses (cross-entropy, Dice, Boundary loss). We start from a simple assumption: the annotator bias and noise in the annotation is located in the high-frequencies of the annotation, whereas the low-frequency information contains the core of what we want to learn. Experiments on two different datasets show that a very small fraction of the low-frequencies are enough to successfully train a neural network, despite those frequencies not being sufficient to reconstruct the segmentation directly.",
        "authors": "Hoel Kervadec",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.16"
    },
    {
        "number": 127,
        "UID": "S-127",
        "forum": "https://openreview.net/forum?id=nX5HUjhtSj",
        "Track": "Short paper",
        "Session": "Poster 1.A: Segmentation",
        "Final Decision": "Poster",
        "title": "Detegmentation of Organs-at-Risk for Head and Neck Cancer",
        "abstract": "Segmentation of organs-at-risk (OARs) is a critical step in radiation therapy planning for head and neck cancer (HNC). We present a practical Detegmentation framework that integrates a detection network to autonomously generate box prompts for training and testing a foundation segmentation model for OARs in HNC. Our method achieves state-of-the-art performance without clinician intervention, demonstrating its strong potential for clinical application.",
        "authors": "Shan Wang, Jax Luo, Xu Qiao, Nazim Haouchine, Scott Raymond",
        "Time": "Day 1 \u2014 10:30am-11:30am",
        "Poster time": "Day 1 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 1.A - S.17"
    },
    {
        "number": 32,
        "UID": "O-32",
        "forum": "https://openreview.net/forum?id=VQrbxORlvv",
        "Track": "Full paper",
        "Session": "Oral 1.B: Generative Al",
        "Final Decision": "Oral",
        "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology",
        "abstract": "Challenges such as the lack of high-quality annotations, long-tailed data distributions, and inconsistent staining styles pose significant obstacles to training neural networks to detect abnormal cells in cytopathology robustly. This paper proposes a style-aligned image composition (SAIC) method that composes high-fidelity and style-preserved pathological images to enhance the effectiveness and robustness of detection models. Without additional training, SAIC first selects an appropriate candidate from the abnormal cell bank based on attribute guidance. Then, it employs a high-frequency feature reconstruction to achieve a style-aligned and high-fidelity composition of abnormal cells and pathological backgrounds. Finally, it introduces a large vision-language model to filter high-quality synthesis images. Experimental results demonstrate that incorporating SAIC-synthesized images effectively enhances the performance and robustness of abnormal cell detection for tail categories and styles, thereby improving overall detection performance. The comprehensive quality evaluation further confirms the generalizability and practicality of SAIC in clinical application scenarios. Our code will be released at https://github.com/Joey-Qi/SAIC.",
        "authors": "Qiuyi Qi, Xin Li, Ming Kong, Zikang Xu, Bingdi Chen, Qiang Zhu, S Kevin Zhou",
        "Time": "Day 1 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.01"
    },
    {
        "number": 32,
        "UID": "F-32",
        "forum": "https://openreview.net/forum?id=VQrbxORlvv",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology",
        "abstract": "Challenges such as the lack of high-quality annotations, long-tailed data distributions, and inconsistent staining styles pose significant obstacles to training neural networks to detect abnormal cells in cytopathology robustly. This paper proposes a style-aligned image composition (SAIC) method that composes high-fidelity and style-preserved pathological images to enhance the effectiveness and robustness of detection models. Without additional training, SAIC first selects an appropriate candidate from the abnormal cell bank based on attribute guidance. Then, it employs a high-frequency feature reconstruction to achieve a style-aligned and high-fidelity composition of abnormal cells and pathological backgrounds. Finally, it introduces a large vision-language model to filter high-quality synthesis images. Experimental results demonstrate that incorporating SAIC-synthesized images effectively enhances the performance and robustness of abnormal cell detection for tail categories and styles, thereby improving overall detection performance. The comprehensive quality evaluation further confirms the generalizability and practicality of SAIC in clinical application scenarios. Our code will be released at https://github.com/Joey-Qi/SAIC.",
        "authors": "Qiuyi Qi, Xin Li, Ming Kong, Zikang Xu, Bingdi Chen, Qiang Zhu, S Kevin Zhou",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.01"
    },
    {
        "number": 100,
        "UID": "O-100",
        "forum": "https://openreview.net/forum?id=8CNssOg7fk",
        "Track": "Full paper",
        "Session": "Oral 1.B: Generative Al",
        "Final Decision": "Oral",
        "title": "Unveiling Differences: A Vision Encoder-Decoder Model for Difference Medical Visual Question Answering",
        "abstract": "Difference Medical Visual Question Answering (Diff-VQA), a specialized subfield of Medical VQA, tackles the critical task of identifying and describing differences between pairs of medical images. This study introduces a novel Vision Encoder-Decoder (VED) architecture tailored for this task, focusing on the comparison of chest X-ray images to detect and explain changes. The proposed model incorporates two key innovations: (1) a light-weight Transformer text decoder architecture capable of generating precise and contextually relevant answers to complex medical questions, and (2) an enhanced fusion mechanism that improves the model‚Äôs ability to distinguish between two input images, enabling more accurate comparison of radiological findings. Our approach excels in identifying significant changes, such as pneumonia and lung opacity, demonstrating its utility in automating preliminary radiological assessments. By leveraging large-scale, domain-specific datasets and employing advanced training strategies, our VED architecture achieves state-of-the-art performance on standard VQA metrics, setting a new benchmark in diagnostic accuracy. These advancements highlight the potential of Diff-VQA to enhance clinical workflows and support radiologists in making more precise, informed decisions.",
        "authors": "Luis-Jesus Marhuenda, Miquel Obrador-Reina, Mohamed Aas-Alas, Alberto Albiol, Roberto Paredes",
        "Time": "Day 1 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.02"
    },
    {
        "number": 100,
        "UID": "F-100",
        "forum": "https://openreview.net/forum?id=8CNssOg7fk",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Unveiling Differences: A Vision Encoder-Decoder Model for Difference Medical Visual Question Answering",
        "abstract": "Difference Medical Visual Question Answering (Diff-VQA), a specialized subfield of Medical VQA, tackles the critical task of identifying and describing differences between pairs of medical images. This study introduces a novel Vision Encoder-Decoder (VED) architecture tailored for this task, focusing on the comparison of chest X-ray images to detect and explain changes. The proposed model incorporates two key innovations: (1) a light-weight Transformer text decoder architecture capable of generating precise and contextually relevant answers to complex medical questions, and (2) an enhanced fusion mechanism that improves the model‚Äôs ability to distinguish between two input images, enabling more accurate comparison of radiological findings. Our approach excels in identifying significant changes, such as pneumonia and lung opacity, demonstrating its utility in automating preliminary radiological assessments. By leveraging large-scale, domain-specific datasets and employing advanced training strategies, our VED architecture achieves state-of-the-art performance on standard VQA metrics, setting a new benchmark in diagnostic accuracy. These advancements highlight the potential of Diff-VQA to enhance clinical workflows and support radiologists in making more precise, informed decisions.",
        "authors": "Luis-Jesus Marhuenda, Miquel Obrador-Reina, Mohamed Aas-Alas, Alberto Albiol, Roberto Paredes",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.02"
    },
    {
        "number": 153,
        "UID": "O-153",
        "forum": "https://openreview.net/forum?id=jNmvKHniMe",
        "Track": "Full paper",
        "Session": "Oral 1.B: Generative Al",
        "Final Decision": "Oral",
        "title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders",
        "abstract": "Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain.\n\nCode: https://github.com/StanfordMIMI/MedVAE",
        "authors": "Maya Varma, Ashwin Kumar, Rogier Van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Joseph Marcel Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay S Chaudhari",
        "Time": "Day 1 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.03"
    },
    {
        "number": 153,
        "UID": "F-153",
        "forum": "https://openreview.net/forum?id=jNmvKHniMe",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders",
        "abstract": "Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain.\n\nCode: https://github.com/StanfordMIMI/MedVAE",
        "authors": "Maya Varma, Ashwin Kumar, Rogier Van der Sluijs, Sophie Ostmeier, Louis Blankemeier, Pierre Joseph Marcel Chambon, Christian Bluethgen, Jip Prince, Curtis Langlotz, Akshay S Chaudhari",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.03"
    },
    {
        "number": 230,
        "UID": "O-230",
        "forum": "https://openreview.net/forum?id=UpJMAlZNuo",
        "Track": "Full paper",
        "Session": "Oral 1.B: Generative Al",
        "Final Decision": "Oral",
        "title": "PRISM: High-Resolution & Precise Counterfactual Medical Image Generation using Language-guided Stable Diffusion",
        "abstract": "Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures robust to the unique complexities posed by medical imaging data. The rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at https://github.com/Amarkr1/PRISM.",
        "authors": "Amar Kumar, Anita Kriz, Mohammad Havaei, Tal Arbel",
        "Time": "Day 1 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.04"
    },
    {
        "number": 230,
        "UID": "F-230",
        "forum": "https://openreview.net/forum?id=UpJMAlZNuo",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "PRISM: High-Resolution & Precise Counterfactual Medical Image Generation using Language-guided Stable Diffusion",
        "abstract": "Developing reliable and generalizable deep learning systems for medical imaging faces significant obstacles due to spurious correlations, data imbalances, and limited text annotations in datasets. Addressing these challenges requires architectures robust to the unique complexities posed by medical imaging data. The rapid advancements in vision-language foundation models within the natural image domain prompt the question of how they can be adapted for medical imaging tasks. In this work, we present PRISM, a framework that leverages foundation models to generate high-resolution, language-guided medical image counterfactuals using Stable Diffusion. Our approach demonstrates unprecedented precision in selectively modifying spurious correlations (the medical devices) and disease features, enabling the removal and addition of specific attributes while preserving other image characteristics. Through extensive evaluation, we show how PRISM advances counterfactual generation and enables the development of more robust downstream classifiers for clinically deployable solutions. To facilitate broader adoption and research, we make our code publicly available at https://github.com/Amarkr1/PRISM.",
        "authors": "Amar Kumar, Anita Kriz, Mohammad Havaei, Tal Arbel",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.04"
    },
    {
        "number": 241,
        "UID": "O-241",
        "forum": "https://openreview.net/forum?id=npYk02a6Qj",
        "Track": "Full paper",
        "Session": "Oral 1.B: Generative Al",
        "Final Decision": "Oral",
        "title": "Sequence models for continuous cell cycle stage prediction from brightfield images",
        "abstract": "Understanding cell cycle dynamics is crucial for studying biological processes such as growth, development and disease progression. While fluorescent protein reporters like the FUCCI system allow live monitoring of cell cycle phases, they require genetic engineering and occupy additional fluorescence channels, limiting broader applicability in complex experiments. In this study, we conduct a comprehensive evaluation of deep learning methods for predicting continuous FUCCI signals using non-fluorescence brightfield imaging, a widely available label-free modality.  To that end, we generated a large dataset of 1.3 M images of dividing RPE1 cells with full cell cycle trajectories to quantitatively compare the predictive performance of distinct model categories including single time-frame models, causal state space models and bidirectional transformer models. We show that both causal and transformer-based models significantly outperform single- and fixed frame approaches, enabling the prediction of visually imperceptible transitions like  G1/S within 1h resolution. Our findings underscore the importance of sequence models for accurate predictions of cell cycle dynamics and highlight their potential for label-free imaging.",
        "authors": "Louis-Alexandre Leger, Maxine Leonardi, Andrea Salati, Felix Naef, Martin Weigert",
        "Time": "Day 1 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.05"
    },
    {
        "number": 241,
        "UID": "F-241",
        "forum": "https://openreview.net/forum?id=npYk02a6Qj",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Sequence models for continuous cell cycle stage prediction from brightfield images",
        "abstract": "Understanding cell cycle dynamics is crucial for studying biological processes such as growth, development and disease progression. While fluorescent protein reporters like the FUCCI system allow live monitoring of cell cycle phases, they require genetic engineering and occupy additional fluorescence channels, limiting broader applicability in complex experiments. In this study, we conduct a comprehensive evaluation of deep learning methods for predicting continuous FUCCI signals using non-fluorescence brightfield imaging, a widely available label-free modality.  To that end, we generated a large dataset of 1.3 M images of dividing RPE1 cells with full cell cycle trajectories to quantitatively compare the predictive performance of distinct model categories including single time-frame models, causal state space models and bidirectional transformer models. We show that both causal and transformer-based models significantly outperform single- and fixed frame approaches, enabling the prediction of visually imperceptible transitions like  G1/S within 1h resolution. Our findings underscore the importance of sequence models for accurate predictions of cell cycle dynamics and highlight their potential for label-free imaging.",
        "authors": "Louis-Alexandre Leger, Maxine Leonardi, Andrea Salati, Felix Naef, Martin Weigert",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - O.05"
    },
    {
        "number": 59,
        "UID": "F-59",
        "forum": "https://openreview.net/forum?id=IIuULGCHLY",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Towards Resource-Efficient Streaming of Large-Scale Medical Image Datasets for Deep Learning",
        "abstract": "Large-scale medical imaging datasets have accelerated deep learning (DL) for medical image analysis. However, the large scale of these datasets poses a challenge for researchers, resulting in increased storage and bandwidth requirements for hosting and accessing them. Since different researchers have different use cases and require different resolutions or formats for DL, it is neither feasible to anticipate every researcher's needs nor practical to store data in multiple resolutions and formats. To that end, we propose the Medical Image Streaming Toolkit (MIST), a format-agnostic database that enables streaming of medical images at different resolutions and formats from a single high-resolution copy. We evaluated MIST across eight popular, large-scale medical imaging datasets spanning different body parts, modalities, and formats. Our results showed that our framework reduced the storage and bandwidth requirements for hosting and downloading datasets without impacting image quality. We demonstrate that MIST addresses the challenges posed by large-scale medical imaging datasets by building a data-efficient and format-agnostic database to meet the diverse needs of researchers and reduce barriers to DL research in medical imaging.",
        "authors": "Pranav Kulkarni, Adway Kanhere, Eliot Siegel, Paul Yi, Vishwa Sanjay Parekh",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.01"
    },
    {
        "number": 80,
        "UID": "F-80",
        "forum": "https://openreview.net/forum?id=0Jn1d4gYRS",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Visual Prompt Engineering for Vision Language Models in Radiology",
        "abstract": "Medical image classification plays a crucial role in clinical decision-making, yet most models are constrained to a fixed set of predefined classes, limiting their adaptability to new conditions. Contrastive Language-Image Pretraining (CLIP) offers a promising solution\nby enabling zero-shot classification through multimodal large-scale pretraining. However, while CLIP effectively captures global image content, radiology requires a more localized focus on specific pathology regions to enhance both interpretability and diagnostic accuracy.\nTo address this, we explore the potential of incorporating visual cues into zero-shot classification, embedding visual markers‚Äîsuch as arrows, bounding boxes, and circles‚Äîdirectly into radiological images to guide model attention. Evaluating across four public chest X-ray\ndatasets, we demonstrate that visual markers improve AUROC by up to 0.185, highlighting their effectiveness in enhancing classification performance. Furthermore, attention map analysis confirms that visual cues help models focus on clinically relevant areas, leading to\nmore interpretable predictions. To support further research, we use public datasets and will release our code and preprocessing pipeline, providing a reference point for future work on localized classification in medical imaging.",
        "authors": "Stefan Denner, Markus Ralf Bujotzek, Dimitrios Bounias, David Zimmerer, Raphael Stock, Klaus Maier-Hein",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.02"
    },
    {
        "number": 111,
        "UID": "F-111",
        "forum": "https://openreview.net/forum?id=zQLrITbcxJ",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "SegResMamba: An Efficient Architecture for 3D Medical Image Segmentation",
        "abstract": "The Transformer architecture has opened a new paradigm in the domain of deep learning with its ability to model long-range dependencies and capture global context and has outpaced the traditional Convolution Neural Networks (CNNs) in many aspects. However, applying Transformer models to 3D medical image datasets presents significant challenges due to their high training time, and memory requirements, which not only hinder scalability but also contribute to elevated CO$_2$ footprint. This has led to an exploration of alternative models that can maintain or even improve performance while being more efficient and environmentally sustainable. Recent advancements in Structured State Space Models (SSMs) effectively address some of the inherent limitations of Transformers, particularly their high memory and computational demands. Inspired by these advancements, we propose an efficient 3D segmentation model for medical imaging called SegResMamba, designed to reduce computation complexity, memory usage, training time, and environmental impact while maintaining high performance. Our model uses less than half the memory during training compared to other state-of-the-art (SOTA) architectures, achieving comparable performance with significantly reduced resource demands.",
        "authors": "Badhan Kumar Das, Ajay Singh, Saahil Islam, Gengyan Zhao, Andreas Maier",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.03"
    },
    {
        "number": 116,
        "UID": "F-116",
        "forum": "https://openreview.net/forum?id=yT6AozW8Oq",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Curriculum Learning for Language-guided, Multi-modal Detection of Various Pathologies",
        "abstract": "Pathology detection in medical imaging is crucial for radiologists, yet current approaches that train specialized models for each region of interest often lack efficiency and robustness. Furthermore, the scarcity of annotated medical data, particularly for diverse phenotypes, poses significant challenges in achieving generalizability. To address these challenges, we present a novel language-guided object detection pipeline for medical imaging that leverages curriculum learning strategies, chosen for their ability to progressively train models on increasingly complex samples, thereby improving generalization across pathologies, phenotypes, and modalities. We developed a unified pipeline to convert segmentation datasets into bounding box annotations, and applied two curriculum learning approaches - teacher curriculum and bounding box size curriculum - to train a Grounding DINO model. Our method was evaluated on different tumor types in MRI and CT scans and showed significant improvements in detection accuracy. The teacher and bounding box size curriculum learning approaches yielded a 4.9% AP and 5.2% AP increase over baseline, respectively. The results highlight the potential of curriculum learning to optimize medical image analysis and clinical workflow by providing a versatile and efficient detection algorithm.",
        "authors": "Laurenz Adrian Heidrich, Aditya Rastogi, Priyank Upadhya, Gianluca Brugnara, Martha Foltyn-Dumitru, Benedikt Wiestler, Philipp Vollmuth",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.04"
    },
    {
        "number": 139,
        "UID": "F-139",
        "forum": "https://openreview.net/forum?id=63igmyYaDc",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Enhancing Contrastive Learning for Retinal Imaging via Adjusted Augmentation Scales",
        "abstract": "Contrastive learning, a typical self-supervised learning strategy, operates on bringing similar data together while pushing dissimilar data apart in latent space. This approach extracts robust and discriminative representations, thus being widely used in natural computer vision tasks, such as object classification. However, unlike natural images, medical images (e.g., retinal images) tend to share substantial similarities in imaging area and anatomical tissues, leading to a denser distribution in latent space. As a result, the default use of strong augmentations in contrastive learning potentially exacerbates this intensive distribution in retinal images, making it difficult to distinguish between genuinely similar and dissimilar data, and therefore hindering model pre-training convergence. In this paper, we hypothesise that weaker augmentations are better suited to contrastive learning for medical image applications, and we investigate model performance under various augmentation strategies. Our study includes six publicly available retinal datasets covering multiple clinically relevant tasks. We assess the models' performance and generalizability via extensive experiments. The model pre-trained with weak augmentation outperforms the one pre-trained with strong augmentation, achieving approximately a 6\\% increase in AUPR ($P$$<$0.001) and a 12.5\\% increase in sensitivity ($P$$<$0.001) on MESSIDOR-2. Similar improvements are observed across other datasets. Our findings suggest that optimizing the scale of augmentation is critical for enhancing the efficacy of contrastive learning in medical imaging. The model weights and relevant code are available at: https://github.com/ziijiecheng/Enhance-contrastive-SSL-for-Retinal-Imaging.",
        "authors": "zijie cheng, Boxuan Li, Andre Altmann, Pearse Keane, Yukun Zhou",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.05"
    },
    {
        "number": 140,
        "UID": "F-140",
        "forum": "https://openreview.net/forum?id=Rd93NNCOnW",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Enhancing Wrist Fracture Detection through LLM-Powered Data Extraction and Knowledge-Based Ensemble Learning",
        "abstract": "The accuracy and generalization of deep learning models for fracture detection and classification in wrist radiographs is often limited by the scarcity of high-quality annotated data and class imbalances. Traditional annotation methods are time-consuming, expensive and prone to inter-observer variability \\cite{rajpurkar2017mura}.  To address these challenges, we developed an automated, cost-free approach to extract structured information from radiology reports, such as fracture type, location and severity. Our technique incorporates methods introduced by MedPrompt \\cite{nori2023can}, and leverages domain expertise for group based sampling \\cite{khan2024knowledge}. Using these structured language labels alongside a pre-trained YOLO v7 backbone \\cite{nagy2022pediatric, ciri2023bonefracture}, which initially demonstrated low accuracy scores on our clinical data, we were able to selectively finetune the model in pseudo-blind manner. This approach utilized the extracted language labels without requiring expert annotations for training. We curated a large dataset of almost 3,000 pediatric wrist X-ray images and their corresponding radiology reports. Validation and testing were conducted on a smaller subset of 300 expert-annotated images.\nOur findings indicate that this pseudo-blind training strategy significantly enhances the base accuracy of the pre-trained model, achieving performance comparable to models fine-tuned with meticulously labeled expert annotations. Specifically, we improved the mean Average Precision (mAP) detection score for true positives related to fractures from 76\\% to 83\\%. Additionally, we observed improvements in precision and recall metrics for fracture detection. By integrating prompt-based information extraction with knowledge-based grouping, we achieved a robust and effective model for fracture detection.",
        "authors": "Serge Didenko Vasylechko, Andy Tsai, Onur Afacan, Sila Kurugol",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.06"
    },
    {
        "number": 145,
        "UID": "F-145",
        "forum": "https://openreview.net/forum?id=CM6VBbB2cb",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Unsupervised Cellular Anomaly Detection in Toxicological Histopathology",
        "abstract": "Irregularities in cellular representation play a crucial role in assessing drug-induced tissue alterations in toxicological histopathology studies. However, the process of annotating rare abnormal cellular variations for training supervised deep learning models presents significant challenges and lacks scalability. While anomaly detection is well-suited for this purpose, it has not yet been explored for cellular-level analysis. In this study, we evaluate cellular anomaly detection using datasets derived from the kidneys and livers of Wistar rats. Our findings indicate that a KNN-distance-based anomaly detection method significantly benefits from employing a feature extractor that has been pre-trained on extensive unsupervised histopathology datasets. When utilizing the best-performing feature extractor, the KNN-distance method surpasses state-of-the-art anomaly detection models by over 4.84% (AUC), including the denoising diffusion probabilistic model, in detecting cellular anomalies. Additionally, we assess the effectiveness of this method in identifying variations in anomalous cell counts between control and treated animal tissues within a toxicological study, revealing a statistically significant difference between the two dosage groups.",
        "authors": "Saketh Juturu, Geetank Raipuria, Raghav Amaravadi, Aman Srivastava, Malini Roy, Nitin Singhal",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.07"
    },
    {
        "number": 165,
        "UID": "F-165",
        "forum": "https://openreview.net/forum?id=SpHsR20XjU",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Scaling Supervision for Free: Leveraging Universal Segmentation Models for Enhanced Medical Image Diagnosis",
        "abstract": "Deep learning-based medical image analysis has been constrained by the limited availability of large-scale annotated data. While recent advances in large language models have enabled scaling automatic extraction of diagnostic labels from reports, we propose that scaling other form of supervision could be an equally important yet unexplored direction. Inspired by the success of foundation models, we leverage modern universal segmentation model to scale anatomical segmentation as an additional supervision signal during training. \nThrough extensive experiments on three large-scale CT datasets totaling 58K+ volumes, we demonstrate that incorporating this ``free\" anatomical supervision consistently improves the performance of various mainstream architectures (ResNet, ViT, and Swin Transformer) by up to 12.74\\%, with particularly significant gains for Transformer-based models and anatomically-localized abnormalities, while maintaining inference efficiency as the segmentation branch is only used during training. This work opens up new direction for scaling in medical imaging and demonstrates how existing universal segmentation models can be repurposed to enhance diagnostic models at virtually no additional cost.",
        "authors": "Yingtai Li, Shuai Ming, Haoran Lai, Fenghe Tang, Wei Wei, S Kevin Zhou",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.08"
    },
    {
        "number": 187,
        "UID": "F-187",
        "forum": "https://openreview.net/forum?id=73GUgAhllx",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "An Unsupervised Approach for Artifact Severity Scoring in Multi-Contrast MR Images",
        "abstract": "Quality assurance (QA) in magnetic resonance (MR) imaging is critical but remains a challenging and time-intensive process, particularly when working with large-scale, multi-site imaging datasets. Manual QA methods are subjective, prone to inter-rater variability, and impractical for high-throughput workflows. Existing automated QA methods often lack generalizability to diverse datasets or fail to provide interpretable insights into the causes of poor image quality. To address these limitations, we introduce an unsupervised and interpretable QA framework for multi-contrast MR images that quantifies artifact severity. By assigning a numerical score to each image, our method enables objective, consistent evaluation of image quality and highlights specific levels of artifact presence that can impair downstream analysis. Our framework employs an unsupervised contrastive learning approach, leveraging simulated artifact transformations, including random bias, noise, anisotropy, and ghosting, to train the model without requiring manual labels or preprocessing. A margin-based contrastive loss further enables differentiation between varying levels of artifact severity. We validate our framework using simulated artifacts on a public dataset and real artifacts on a private clinical dataset, demonstrating its robustness and generalizability for automatic MR image QA. By efficiently evaluating image quality and identifying artifacts prior to data processing, our approach streamlines QA workflows and enhances the reliability of subsequent analyses in both research and clinical settings.",
        "authors": "Savannah Hays, Lianrui Zuo, Blake E. Dewey, Samuel Remedios, Jinwei Zhang, Ellen M. Mowry, Scott D. Newsome, Aaron Carass, Jerry L Prince",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.09"
    },
    {
        "number": 195,
        "UID": "F-195",
        "forum": "https://openreview.net/forum?id=DyycxAdJil",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Biologically-Constrained Multi-Label Classification with Learnable Domain Knowledge",
        "abstract": "Although recent foundation models trained in a self-supervised setting have shown promise in cellular image analysis, they often produce biologically impossible predictions when handling multiple concurrent abnormalities. This is a problem, as the biological information that may be needed for the different clinical-oriented problems is not directly presented in the images. In this study, we present a novel and modular approach to enforce biological constraints in multi-label medical imaging classification. Building on the powerful and rich representations of the DinoBloom hematological foundation model, our method combines learnable constraint matrices with adaptive thresholding, effectively preventing contradictory predictions while maintaining high sensitivity.\nExtensive experiments on three datasets, two public and one in-house on neutrophil classification, demonstrate significant improvements over different foundation models and the state-of-the-art methods. Through detailed ablation studies and hyperparameter interpretation, we show that our approach successfully captures biological relationships between different abnormalities.",
        "authors": "Nabil Mouadden, V√©ronique Verg√©, Ahmadreza ARBAB, Jean-Baptiste MICOL, Elsa BERNARD, Aline RENNEVILLE, Stergios Christodoulidis, Maria Vakalopoulou",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.10"
    },
    {
        "number": 196,
        "UID": "F-196",
        "forum": "https://openreview.net/forum?id=ZkmVQinyAE",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Prostate Cancer Detection in Bi-Parametric MRI using Zonal Anatomy-Guided U-Mamba with Multi-Task Learning",
        "abstract": "Prostate cancer (PCa) remains a leading cause of cancer-related morbidity, emphasizing the need for accurate and non-invasive diagnostic tools. While deep learning models have advanced PCa detection in magnetic resonance imaging (MRI), they often fail to integrate anatomical knowledge. This study evaluates U-Mamba, a deep learning architecture designed to enhance long-range dependency modeling with linear time complexity, for PCa detection. Furthermore, a multi-task learning (MTL) extension, U-Mamba MTL, is introduced to incorporate prostate zonal anatomy, aligning with clinical diagnostic workflows. The models were assessed using diverse datasets, including the PI-CAI hidden tuning cohort (N=100) and an in-house collected out-of-distribution cohort (N=200). Results demonstrate that U-Mamba achieves state-of-the-art detection performance, while U-Mamba MTL further improves PCa detection through the auxiliary zonal segmentation task. These findings highlight the potential of integrating U-Mamba with anatomical context to improve PCa detection. The code and model weights are available at https://github.com/mokkalokka/U-MambaMTL.",
        "authors": "Michael S. Larsen, Syed Farhan Abbas, Gabriel Kiss, Mattijs Elschot, Tone F. Bathen, Frank Lindseth",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.11"
    },
    {
        "number": 255,
        "UID": "F-255",
        "forum": "https://openreview.net/forum?id=JspipsZKuo",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Foundation Model Ensemble for Out-of-Distribution Generalization: Predicting Lymph Node Metastasis in Early Gastric Cancer Using Whole-Slide Imaging",
        "abstract": "Recent advances in deep learning have improved the practicality of automated analysis for large-scale whole-slide imaging. However, challenges remain in image analysis due to variations in imaging equipment, tissue preparation, staining protocols, and other variables. These variations hinder the generalizability of trained models to out-of-distribution datasets. Foundation models, trained through self-supervised learning on large-scale datasets, have the potential to address this issue. Since each model was trained on diverse datasets using distinct methodologies, they learn unique representations. These differences suggest that combining multiple models, rather than relying on a single one, could enhance generalization and robustness. In this study, we investigate foundation model ensembles for predicting lymph node metastasis in early gastric cancer across three different datasets. By comparing individual models with model ensembles, we demonstrate that ensembling multiple models improves performance on unseen data in whole-slide imaging.",
        "authors": "Woojin Chung, Yujun Park, Yoonho Nam",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.12"
    },
    {
        "number": 261,
        "UID": "F-261",
        "forum": "https://openreview.net/forum?id=hMmDw7VVZm",
        "Track": "Full paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Data Augmentation for Medical Imaging: Counterfactual Simulation of Acquisition Parameters via Conditional Diffusion Model",
        "abstract": "Deep learning (DL) models in medical imaging face challenges in generalizability and robustness due to variations in image acquisition parameters (IAP). In this work, we introduce a novel method using conditional denoising diffusion generative models (cDDGMs) to generate counterfactual medical images that simulate different IAP without altering patient anatomy. We demonstrate that using these counterfactual images for magnetic resonance (MR) data augmentation can improve segmentation accuracy in out-of-distribution settings, enhancing the overall generalizability and robustness of DL models across diverse imaging conditions. Our approach shows promise in addressing domain and covariate shifts in medical imaging. The code is publicly available at https://anonymous.4open.science/r/Counterfactual-MRI-Data-Augmentation",
        "authors": "Pedro A. Mor√£o, Yasna Forghani, Nuno Lou√ß√£o, Pedro Gouveia, Mario A. T. Figueiredo, Joao Santinha",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - F.13"
    },
    {
        "number": 1,
        "UID": "S-1",
        "forum": "https://openreview.net/forum?id=7402G6pGEj",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Are the Latent Representations of Foundation Models for Pathology Invariant to Rotation?",
        "abstract": "Self-supervised foundation models for digital pathology encode small patches from H\\&E whole slide images into latent representations used for downstream tasks. However, the invariance of these representations to patch rotation remains unexplored. This study investigates the rotational invariance of latent representations across twelve foundation models by quantifying the alignment between non-rotated and rotated patches using mutual $k$-nearest neighbours and cosine distance. Models that incorporated rotation augmentation during self-supervised training exhibited significantly greater invariance to  rotations. We hypothesise that the absence of rotational inductive bias in the transformer architecture necessitates rotation augmentation during training to achieve learned invariance. Code: https://github.com/MatousE/rot-invariance-analysis.",
        "authors": "Matou≈° Elphick, Guang Yang, Samra Turajlic",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.01"
    },
    {
        "number": 12,
        "UID": "S-12",
        "forum": "https://openreview.net/forum?id=5hS3YCPNdp",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Automating Reproducibility in Medical Imaging with Deep Learning",
        "abstract": "Reproducibility remains a critical challenge in deep learning for medical imaging, limiting the reliability and clinical adoption of published research. An automated framework is presented to assess key reproducibility factors --- dependencies, training/evaluation code, weights, documentation and licensing --- by analyzing GitHub repositories. Validated on manually annotated MIDL 2024 submissions, the system achieves 66.8%-96.9% accuracy across criteria. Applied to 3,682 papers from MIDL, MICCAI, Nature, and arXiv reveals widespread gaps, particularly in sharing model weights and documentation. This approach enables scalable, objective reproducibility assessments and lays the groundwork for integration into peer review workflows. The source code and a live demo is available online (https://huggingface.co/spaces/attilasimko/reproduce)",
        "authors": "Attila Simk√≥",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.02"
    },
    {
        "number": 15,
        "UID": "S-15",
        "forum": "https://openreview.net/forum?id=IJJJT1vIdX",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation",
        "abstract": "Vision-language models and their adaptations to image segmentation tasks present enormous potential for producing highly accurate and interpretable results. However, implementations based on CLIP and BiomedCLIP are still lagging behind more sophisticated architectures such as CRIS. In this work, instead of focusing on text prompt engineering as is the norm, we attempt to narrow this gap by showing how to ensemble vision-language segmentation models (VLSMs) with a low-complexity CNN. By doing so, we achieve a significant performance average Dice gain of 6.3% with the ensembled BiomedCLIPSeg on the BKAI polyp dataset. Furthermore, we provide initial experimental results on the other four radiology and non-radiology datasets. We conclude that ensembling works differently across these datasets (from outperforming to underperforming the CRIS model), indicating a topic for future investigation by the community. The code will be released at\nhttps://github.com/juliadietlmeier/VLSM-Ensemble.",
        "authors": "Julia Dietlmeier, Oluwabukola Grace Adegboro, Vayangi Vishmi Vishara Ganepola, Claudia Mazo, \"Noel OConnor\"",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.03"
    },
    {
        "number": 18,
        "UID": "S-18",
        "forum": "https://openreview.net/forum?id=enWkUsJyjf",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "FundusAdapter: few-shot adaptation of fundus image foundation model for fundus image diagnosis",
        "abstract": "Fundus images exhibit significant source gaps, limiting the performance of foundation models across different scenarios. Due to the scarcity of labeled training data, few-shot adaptation is essential for effective diagnosis. However, existing few-shot adapters have primarily focused on global image features, which are insufficient for distinguishing fundus diseases that require detailed texture information. In this paper, we propose FundusAdapter, the first few-shot adaptation model of fundus image foundation model for fundus image diagnosis. By leveraging hierarchical feature extraction, FundusAdapter effectively integrates both global and local features, enhancing the detection of subtle lesions. The use of cross-attention and gate memory guidance improves the interaction between features, leading to more accurate adaptation. Our model achieves state-of-the-art performance on public fundus benchmarks. Code is available at https://github.com/Yifan-Chang/CrossFundus.",
        "authors": "Yifan Chang, Zihang Jiang, Kun Zhang, S Kevin Zhou",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.04"
    },
    {
        "number": 23,
        "UID": "S-23",
        "forum": "https://openreview.net/forum?id=i9Ray4JAEn",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Ensemble Object Detection Methodology for Automated Detection of Inflammatory Cells in Kidney Biopsies",
        "abstract": "Automated detection of inflammatory cells in kidney biopsies is essential for kidney disease diagnosis. To address this, we participated in the Machine-learning for Optimal detection of iNflammatory cells in KidnEY (MONKEY) challenge, where the main challenges were\nto detect inflammatory cells and further classify them as monocyte and lymphocyte. We employed an ensemble of DETR and YOLOv5-L object detection models, achieving the 3rd place on both leaderboards with Free Response Receiver Operating Characteristic (FROC)\nscores of 0.3517 (Task 1) and 0.4471/0.1906 (Task 2). Our approach demonstrated the power of combining transformer-based and convolutional architectures to enhance diagnostic precision in digital pathology, offering a cost-effective alternative to immunohistochemistry (IHC) staining while advancing transplant rejection analysis.",
        "authors": "Gunjan Deotale, Abhishek Ambast, Lavish Ramchandani, Dev Kumar Das, Tijo Thomas",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.05"
    },
    {
        "number": 29,
        "UID": "S-29",
        "forum": "https://openreview.net/forum?id=LFlH9hSTYH",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "BEAMRAD: A tool for Creating and Assessing Medical Dataset Documentation",
        "abstract": "Medical datasets drive deep learning in medical imaging but may introduce biases that impact model performance and clinical applicability. To address these bias challenges, we introduce BEAMRAD, a dynamic tool to create and assess medical dataset documentation. BEAMRAD systematically evaluates documentation, and links insufficient reporting to potential biases. Through an exemplary assessment of publicly available medical datasets, we highlight gaps in dataset documentation, including inconsistencies in data annotation, error quantification, and dataset limitations reporting. We propose to address these issues with three key improvements: stricter repository oversight, reflective documentation practices, and adaptable documentation.",
        "authors": "Maria Galanty, Dieuwertje Luitse, Alexander P. Vlaar, Clara I. S√°nchez, Tobias Blanke, Ivana Isgum",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.06"
    },
    {
        "number": 42,
        "UID": "S-42",
        "forum": "https://openreview.net/forum?id=fEAAi7wwXb",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "A Bidirectional Loss Approach to Imparting Order Sensitivity to Multi-Image Chest X-ray Encoders",
        "abstract": "Longitudinal chest X-ray (CXR) analysis is a critical step in assessing disease progression, yet existing deep learning methods often fail to account for the inherent temporal directionality of serial images, yielding inconsistent predictions when their order is reversed. In this work, we propose a bidirectional loss framework that enforces order sensitivity in multi-image CXR encoders. Leveraging large language models (LLMs), we obtain fine-grained interval change labels‚Äîresolved, improved, stable, worsened, and new‚Äîby comparing prior and current radiology reports across five common thoracic findings. We then exploit the symmetric nature of these labels by reversing image order and inverting labels (e.g., improved - worsened) during training. Experiments on the MIMIC-CXR and CheXpert datasets show that our method surpasses baselines for most findings, effectively embedding order awareness while retaining a simple, efficient design.",
        "authors": "Doowoong Choi, Hanbin Ko, Jung Hoon Lee, Chang Min Park",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.07"
    },
    {
        "number": 46,
        "UID": "S-46",
        "forum": "https://openreview.net/forum?id=lCIsJOIhrj",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Racial Disparities Persist Beyond Data Representation in Medical Imaging - even Predictive Uncertainty Fails to Capture them",
        "abstract": "Balanced training sets are often promoted to mitigate racial performance disparities of\nDeep Learning (DL) models in medical imaging. However, our preliminary findings on\ntwo medical imaging datasets show that while racial training set representation\naffects model performance, there is more at play, as large racial disparities\nremain regardless of training set composition. Moreover, predictive uncertainty is\nshown to be completely insensitive to these performance disparities. From this, we derive\na series of open problems for safe and fair image-guided diagnostics.",
        "authors": "Tareen Dawood, Gloria Stucchi, Aasa Feragen",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.08"
    },
    {
        "number": 56,
        "UID": "S-56",
        "forum": "https://openreview.net/forum?id=Fl44mi5dFn",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "RevisedMedYOLO: Unlocking Model Performance by Careful Training Code Inspection",
        "abstract": "The MedYOLO architecture, adapting YOLOv5 for 3D medical object detection, was reported by its original authors to have failed to train effectively in LIDC for the detection of lung lesions and to fail in BraTS for the detection of brain tumors when using a large model configuration. This work introduces RevisedMedYOLO, achieved by carefully reviewing and correcting the original training implementation. We fixed critical bugs related to dataset shuffling, initialization of bias parameters, and bounding box clamping during zoom augmentation. Consequently, RevisedMedYOLO demonstrates successful learning on these datasets (AP@$0.5>0$), unlike the original implementation. This study underscores the crucial role of careful code implementation and debugging in enabling deep learning model performance for challenging tasks in medical image analysis.",
        "authors": "Kai Geissler, Jan Hendrik Moltz, Hans Meine, Markus Wenzel",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.09"
    },
    {
        "number": 68,
        "UID": "S-68",
        "forum": "https://openreview.net/forum?id=18nhqwH1Yq",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation",
        "abstract": "Evaluating long-context radiology report generation is challenging. NLG metrics fail to capture clinical correctness, while LLM-based metrics often lack generalizability. Clinical accuracy metrics are more relevant but are sensitive to class imbalance, frequently favoring trivial predictions. We propose the CRG Score, a distribution-aware and adaptable metric that evaluates only clinically relevant abnormalities explicitly described in reference reports. CRG supports both binary and structured labels (e.g., type, location) and can be paired with any LLM for feature extraction. By balancing penalties based on label distribution, it enables fairer, more robust evaluation and serves as a clinically aligned reward function.",
        "authors": "Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Bernhard Kainz, Bjoern Menze",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.10"
    },
    {
        "number": 77,
        "UID": "S-77",
        "forum": "https://openreview.net/forum?id=3PVEvBmdxc",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Segmentation-Informed Captioning: A Multi-Stage Pipeline for Surgical Vision‚ÄìLanguage Dataset Generation",
        "abstract": "General surgical understanding aims to develop models that generalize across procedures and tasks, in contrast to fully-supervised, task-specific approaches. Recent work has explored Vision Language Models (VLMs) for this purpose. However, their effectiveness‚Äìparticularly on tasks requiring fine-grained scene understanding‚Äìis often constrained by noisy and misaligned datasets, typically based on transcribed audio. In this paper, we propose a five-stage pipeline to construct more accurate and less noisy vision-language datasets from existing segmentation datasets. Our method applies rule-based heuristics to extract spatial, and interaction cues, which are then used to prompt a large language model (LLM) to produce naturally sound, clinically coherent captions. Evaluation by three medical experts on how well the captions met stage-specific expectation found that 95\\% of the generated captions scored 3 or higher on a Likert scale.",
        "authors": "Mohamed Hamdy, Fatmaelzahraa Ali Ahmed, Mariam Ahmed, Mohannad Natheef AbuHaweeleh, Muraam Abdel-Ghani, Muhammad Arsalan, Abdulaziz Al-Ali, Shidin Balakrishnan",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.11"
    },
    {
        "number": 80,
        "UID": "S-80",
        "forum": "https://openreview.net/forum?id=yLplH8pVmH",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Fine-tuning Vision Foundation Models for Multi-Modal Prostate MR Sequence Classification",
        "abstract": "Assigning MRI sequence types is essential yet remains a tedious, manual step in prostate imaging workflows. Current automated approaches relying solely on images or DICOM metadata often struggle with protocol variability and metadata inaccuracies, limiting their generalizability. We propose fine-tuning vision foundation models within different fusion strategies integrating image and metadata. We achieve state-of-the-art F1-score of 1.00 and 0.98 on internal and external test sets, respectively, demonstrating robust generalization.",
        "authors": "Stefan Denner, Balint Kovacs, David Zimmerer, Deepa Krishnaswamy, Dimitrios Bounias, Raphael Stock, Markus Ralf Bujotzek, Fergus Imrie, Andriy Fedorov, Klaus Maier-Hein",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.12"
    },
    {
        "number": 83,
        "UID": "S-83",
        "forum": "https://openreview.net/forum?id=EQ0cbPZ4Ii",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Can Natural Domain Foundation Models Be Applied to Cardiac MRI Reconstruction?",
        "abstract": "The field of computer vision has experienced a paradigm shift with the emergence of general-\npurpose foundation models, which exhibit strong generalization capabilities across a wide\nrange of tasks. However, their applicability to specialized medical imaging tasks, particularly cardiac MRI reconstruction, remains underexplored. In this work, we investigate\nthe transferability of state-of-the-art vision foundation models like CLIP and DINOv2 for\ncardiac MRI reconstruction. We propose a novel framework that leverages frozen vision\nfoundation models as image encoders, combined with a UNETR-based trainable decoder.\nWe validate our framework on the CMRxRecon2024 dataset, demonstrating improved performance over the traditional state-of-the-art U-Net under acceleration factor (√ó4), despite\nrelying on frozen natural-domain foundation model and significantly fewer trainable parameters. Authors will disclose the code upon acceptance.",
        "authors": "Anam Hashmi, Julia Dietlmeier, Mayug Maniparambil, Carles Garcia-Cabrera, Kathleen M Curran, \"Noel OConnor\"",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.13"
    },
    {
        "number": 120,
        "UID": "S-120",
        "forum": "https://openreview.net/forum?id=gqLXT8Edf3",
        "Track": "Short paper",
        "Session": "Poster 1.B: Generative Al",
        "Final Decision": "Poster",
        "title": "Obscure to Observe: A Lesion-Aware MAE for Glaucoma Detection from Retinal Context",
        "abstract": "Self-supervised learning (SSL) offers a powerful paradigm for medical image representation\nlearning, particularly in low-label regimes. However, standard pretext tasks often over-\nlook domain-specific cues vital for diseases like glaucoma‚Äîa leading cause of irreversible\nblindness that manifests as subtle structural changes in the optic disc (OD) region. Un-\nderstanding the broader retinal context is essential, yet traditional models tend to overfit\nto localized features, limiting generalizability. We propose a glaucoma-aware SSL frame-\nwork using a Deconvolutional Masked Autoencoder (Deconv-MAE) with a ViT-B encoder,\ntrained to reconstruct clean fundus images from inputs degraded by Gaussian noise and\nanatomically-aware OD masking. This lesion-focused corruption compels the model to learn\nrobust, context-rich representations. Pretrained on EYEPACS and fine-tuned on ORIGA-\nlight, our method outperforms both standard MAE and supervised baselines, highlighting\nthe value of anatomically informed pretext tasks in retinal diagnostics.",
        "authors": "Siddhant Bharadwaj, Pratinav Seth, Chandra Sekhar Seelamantula",
        "Time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 1 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 1.B - S.14"
    },
    {
        "number": 29,
        "UID": "O-29",
        "forum": "https://openreview.net/forum?id=nKLCB8d3Ko",
        "Track": "Full paper",
        "Session": "Oral 2.A: Responsible Al",
        "Final Decision": "Oral",
        "title": "STNAGNN: Data-driven Spatio-temporal Brain Connectivity beyond FC",
        "abstract": "In recent years, graph neural networks (GNNs) have been widely applied in the analysis of brain fMRI, yet defining the connectivity between ROIs remains a challenge in noisy fMRI data. Among all approaches, Functional Connectome (FC) is the most popular\nmethod. Computed by the correlation coefficients between ROI time series, FC is a powerful and computationally efficient way to estimate ROI connectivity. However, it is well known for neglecting structural connections and causality in ROI interactions. Also, FC\nbecomes much more noisy in the short spatio-temporal sliding-window subsequences of fMRI. Effective Connectome (EC) is proposed as a directional alternative, but it is difficult to accurately estimate. Furthermore, for optimal GNN performance, usually only a small percentage of the strongest connections are selected as sparse edges, resulting in oversimplification of complex brain connections. To tackle these challenges, we propose the Spatio-Temporal Node Attention Graph Neural Network (STNAGNN) as a data-driven alternative that combines sparse predefined FC with dense data-driven spatio-temporal connections, allowing for flexible and spatio-temporal learning of ROI interaction patterns.",
        "authors": "Jiyao Wang, Nicha C Dvornek, Peiyu Duan, Lawrence H. Staib, Pamela Ventola, James s Duncan",
        "Time": "Day 2 \u2014 9:00am-10:15am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.01"
    },
    {
        "number": 29,
        "UID": "F-29",
        "forum": "https://openreview.net/forum?id=nKLCB8d3Ko",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "STNAGNN: Data-driven Spatio-temporal Brain Connectivity beyond FC",
        "abstract": "In recent years, graph neural networks (GNNs) have been widely applied in the analysis of brain fMRI, yet defining the connectivity between ROIs remains a challenge in noisy fMRI data. Among all approaches, Functional Connectome (FC) is the most popular\nmethod. Computed by the correlation coefficients between ROI time series, FC is a powerful and computationally efficient way to estimate ROI connectivity. However, it is well known for neglecting structural connections and causality in ROI interactions. Also, FC\nbecomes much more noisy in the short spatio-temporal sliding-window subsequences of fMRI. Effective Connectome (EC) is proposed as a directional alternative, but it is difficult to accurately estimate. Furthermore, for optimal GNN performance, usually only a small percentage of the strongest connections are selected as sparse edges, resulting in oversimplification of complex brain connections. To tackle these challenges, we propose the Spatio-Temporal Node Attention Graph Neural Network (STNAGNN) as a data-driven alternative that combines sparse predefined FC with dense data-driven spatio-temporal connections, allowing for flexible and spatio-temporal learning of ROI interaction patterns.",
        "authors": "Jiyao Wang, Nicha C Dvornek, Peiyu Duan, Lawrence H. Staib, Pamela Ventola, James s Duncan",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.01"
    },
    {
        "number": 85,
        "UID": "O-85",
        "forum": "https://openreview.net/forum?id=28Vb5ffnoC",
        "Track": "Full paper",
        "Session": "Oral 2.A: Responsible Al",
        "Final Decision": "Oral",
        "title": "CountXplain: Interpretable Cell Counting with Prototype-Based Density Map Estimation",
        "abstract": "Cell counting in biomedical imaging is pivotal for various clinical applications, yet the interpretability of deep learning models in this domain remains a significant challenge. We propose a novel prototype-based method for interpretable cell counting via density map estimation. Our approach integrates a prototype layer into the density estimation network, enabling the model to learn representative visual patterns for both cells and background artifacts. The learned prototypes were evaluated through a survey of biologists, who confirmed the relevance of the visual patterns identified, further validating the interpretability of the model. By generating interpretations that highlight regions in the input image most similar to each prototype, our method offers a clear understanding of how the model identifies and counts cells. Extensive experiments on two public datasets demonstrate that our method achieves interpretability without compromising counting effectiveness. This work provides researchers and clinicians with a transparent and reliable tool for cell counting, potentially increasing trust and accelerating the adoption of deep learning in critical biomedical applications. Code is available at https://github.com/abdumhmd/countxplain.",
        "authors": "Abdurahman Ali Mohammed, Wallapak Tavanapong, Catherine Fonder, Donald Sakaguchi",
        "Time": "Day 2 \u2014 9:00am-10:15am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.02"
    },
    {
        "number": 85,
        "UID": "F-85",
        "forum": "https://openreview.net/forum?id=28Vb5ffnoC",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "CountXplain: Interpretable Cell Counting with Prototype-Based Density Map Estimation",
        "abstract": "Cell counting in biomedical imaging is pivotal for various clinical applications, yet the interpretability of deep learning models in this domain remains a significant challenge. We propose a novel prototype-based method for interpretable cell counting via density map estimation. Our approach integrates a prototype layer into the density estimation network, enabling the model to learn representative visual patterns for both cells and background artifacts. The learned prototypes were evaluated through a survey of biologists, who confirmed the relevance of the visual patterns identified, further validating the interpretability of the model. By generating interpretations that highlight regions in the input image most similar to each prototype, our method offers a clear understanding of how the model identifies and counts cells. Extensive experiments on two public datasets demonstrate that our method achieves interpretability without compromising counting effectiveness. This work provides researchers and clinicians with a transparent and reliable tool for cell counting, potentially increasing trust and accelerating the adoption of deep learning in critical biomedical applications. Code is available at https://github.com/abdumhmd/countxplain.",
        "authors": "Abdurahman Ali Mohammed, Wallapak Tavanapong, Catherine Fonder, Donald Sakaguchi",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.02"
    },
    {
        "number": 124,
        "UID": "O-124",
        "forum": "https://openreview.net/forum?id=BAEwCzDmPB",
        "Track": "Full paper",
        "Session": "Oral 2.A: Responsible Al",
        "Final Decision": "Oral",
        "title": "A knowledge-based method for detecting network-induced shape artifacts in synthetic images",
        "abstract": "The adoption of synthetic medical images for training or testing without thorough quality assessment risks introducing artifacts and unrealistic features that can mislead machine learning models and compromise clinical utility. This work introduces a novel knowledge-based method for detecting network-induced shape artifacts in synthetic images. The method can identify anatomically unrealistic images, detect shape artifacts irrespective of the generative model, and offer interpretability through its knowledge-driven design. We validated the method using two synthetic mammography datasets and demonstrated its effectiveness in flagging images with network-induced artifacts. A reader study further confirmed these findings and showed that the most anomalous images identified by the method were also flagged by human readers. This method provides a step toward the responsible use of synthetic data by ensuring synthetic images align with realistic morphological and anatomical constraints.",
        "authors": "Rucha Deshpande, Miguel Lago, Adarsh Subbaswamy, Seyed Kahaki, Jana G Delfino, Aldo Badano, Ghada Zamzmi",
        "Time": "Day 2 \u2014 9:00am-10:15am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.03"
    },
    {
        "number": 124,
        "UID": "F-124",
        "forum": "https://openreview.net/forum?id=BAEwCzDmPB",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "A knowledge-based method for detecting network-induced shape artifacts in synthetic images",
        "abstract": "The adoption of synthetic medical images for training or testing without thorough quality assessment risks introducing artifacts and unrealistic features that can mislead machine learning models and compromise clinical utility. This work introduces a novel knowledge-based method for detecting network-induced shape artifacts in synthetic images. The method can identify anatomically unrealistic images, detect shape artifacts irrespective of the generative model, and offer interpretability through its knowledge-driven design. We validated the method using two synthetic mammography datasets and demonstrated its effectiveness in flagging images with network-induced artifacts. A reader study further confirmed these findings and showed that the most anomalous images identified by the method were also flagged by human readers. This method provides a step toward the responsible use of synthetic data by ensuring synthetic images align with realistic morphological and anatomical constraints.",
        "authors": "Rucha Deshpande, Miguel Lago, Adarsh Subbaswamy, Seyed Kahaki, Jana G Delfino, Aldo Badano, Ghada Zamzmi",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.03"
    },
    {
        "number": 127,
        "UID": "O-127",
        "forum": "https://openreview.net/forum?id=vxSo5TJxlB",
        "Track": "Full paper",
        "Session": "Oral 2.A: Responsible Al",
        "Final Decision": "Oral",
        "title": "Evaluating Shortcut Utilization in Deep Learning Disease Classification through Counterfactual Analysis",
        "abstract": "Although deep learning models can surpass human performance in many medical image analysis tasks, they remain vulnerable to algorithmic shortcuts, where spurious correlations in the data are exploited, which may lead to reduced trust in their predictions/classifications. This issue is especially concerning when models rely on protected attributes (e.g., sex, race, or site) as shortcuts. Such shortcut reliance not only impairs their ability to generalize to unseen datasets but also raises fairness concerns, ultimately undermining their purpose for computer-aided diagnosis. Previous techniques for analyzing protected attributes, such as supervised prediction layer information tests, only highlight the presence of protected attributes in the feature space but do not confirm their role in solving the primary task. Determining the impact of protected attributes as shortcuts is particularly challenging, as it requires knowing how a model would perform without those attributes ‚Äî a counterfactual scenario typically unattainable in real-world data. As a workaround, researchers have addressed the absence of counterfactuals by generating synthetic datasets with and without protected attributes. In this study, we propose a novel approach to evaluate real-world datasets and determine the extent to which each protected attribute is used as a shortcut in a classification task. Therefore, we define and train a causal generative model to produce causally-grounded counterfactuals, removing protected attributes from activations and allowing us to measure their impact on model performance. Employing T1-weighted MRI data from 9 sites (835 subjects: 426 with Parkinson‚Äôs disease (PD) and 409 healthy), we demonstrate that counterfactually removing the 'site' attribute from the penultimate layer of a trained classification model reduced the AUROC for PD classification from 0.74 to 0.65, indicating a 9% performance improvement achieved by using 'site' as a shortcut. In contrast, counterfactually removing the 'sex' attribute had minimal impact on performance, with only a slight change of 0.004, indicating that 'sex' was not utilized as a shortcut by the classification model. The proposed method offers a robust framework for assessing shortcut utilization in medical image classification, paving the way for improved bias detection and mitigation in medical imaging tasks. The code for this work is available on https://github.com/vibujithan/shortcut-analysis.",
        "authors": "Vibujithan Vigneshwaran, Emma A.M. Stanley, Raissa Souza, Erik Ohara, Matthias Wilms, Nils Forkert",
        "Time": "Day 2 \u2014 9:00am-10:15am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.04"
    },
    {
        "number": 127,
        "UID": "F-127",
        "forum": "https://openreview.net/forum?id=vxSo5TJxlB",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Evaluating Shortcut Utilization in Deep Learning Disease Classification through Counterfactual Analysis",
        "abstract": "Although deep learning models can surpass human performance in many medical image analysis tasks, they remain vulnerable to algorithmic shortcuts, where spurious correlations in the data are exploited, which may lead to reduced trust in their predictions/classifications. This issue is especially concerning when models rely on protected attributes (e.g., sex, race, or site) as shortcuts. Such shortcut reliance not only impairs their ability to generalize to unseen datasets but also raises fairness concerns, ultimately undermining their purpose for computer-aided diagnosis. Previous techniques for analyzing protected attributes, such as supervised prediction layer information tests, only highlight the presence of protected attributes in the feature space but do not confirm their role in solving the primary task. Determining the impact of protected attributes as shortcuts is particularly challenging, as it requires knowing how a model would perform without those attributes ‚Äî a counterfactual scenario typically unattainable in real-world data. As a workaround, researchers have addressed the absence of counterfactuals by generating synthetic datasets with and without protected attributes. In this study, we propose a novel approach to evaluate real-world datasets and determine the extent to which each protected attribute is used as a shortcut in a classification task. Therefore, we define and train a causal generative model to produce causally-grounded counterfactuals, removing protected attributes from activations and allowing us to measure their impact on model performance. Employing T1-weighted MRI data from 9 sites (835 subjects: 426 with Parkinson‚Äôs disease (PD) and 409 healthy), we demonstrate that counterfactually removing the 'site' attribute from the penultimate layer of a trained classification model reduced the AUROC for PD classification from 0.74 to 0.65, indicating a 9% performance improvement achieved by using 'site' as a shortcut. In contrast, counterfactually removing the 'sex' attribute had minimal impact on performance, with only a slight change of 0.004, indicating that 'sex' was not utilized as a shortcut by the classification model. The proposed method offers a robust framework for assessing shortcut utilization in medical image classification, paving the way for improved bias detection and mitigation in medical imaging tasks. The code for this work is available on https://github.com/vibujithan/shortcut-analysis.",
        "authors": "Vibujithan Vigneshwaran, Emma A.M. Stanley, Raissa Souza, Erik Ohara, Matthias Wilms, Nils Forkert",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.04"
    },
    {
        "number": 214,
        "UID": "O-214",
        "forum": "https://openreview.net/forum?id=3LySEy7MR3",
        "Track": "Full paper",
        "Session": "Oral 2.A: Responsible Al",
        "Final Decision": "Oral",
        "title": "Conditional Diffusion Models are Medical Image Classifiers that Provide Explainability and Uncertainty for Free",
        "abstract": "Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: https://faverogian.github.io/med-diffusion-classifier.github.io/.",
        "authors": "Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel",
        "Time": "Day 2 \u2014 9:00am-10:15am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.05"
    },
    {
        "number": 214,
        "UID": "F-214",
        "forum": "https://openreview.net/forum?id=3LySEy7MR3",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Conditional Diffusion Models are Medical Image Classifiers that Provide Explainability and Uncertainty for Free",
        "abstract": "Discriminative classifiers have become a foundational tool in deep learning for medical imaging, excelling at learning separable features of complex data distributions. However, these models often need careful design, augmentation, and training techniques to ensure safe and reliable deployment. Recently, diffusion models have become synonymous with generative modeling in 2D. These models showcase robustness across a range of tasks including natural image classification, where classification is performed by comparing reconstruction errors across images generated for each possible conditioning input. This work presents the first exploration of the potential of class conditional diffusion models for 2D medical image classification. First, we develop a novel majority voting scheme shown to improve the performance of medical diffusion classifiers. Next, extensive experiments on the CheXpert and ISIC Melanoma skin cancer datasets demonstrate that foundation and trained-from-scratch diffusion models achieve competitive performance against SOTA discriminative classifiers without the need for explicit supervision. In addition, we show that diffusion classifiers are intrinsically explainable, and can be used to quantify the uncertainty of their predictions, increasing their trustworthiness and reliability in safety-critical, clinical contexts. Further information is available on our project page: https://faverogian.github.io/med-diffusion-classifier.github.io/.",
        "authors": "Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, Tal Arbel",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - O.05"
    },
    {
        "number": 2,
        "UID": "F-2",
        "forum": "https://openreview.net/forum?id=SNFA0P8twn",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "SSDD-GAN: Single-Step Denoising Diffusion GAN for Cochlear Implant Surgical Scene Completion",
        "abstract": "Recent deep learning-based image completion methods, including both inpainting and outpainting, have demonstrated promising results in restoring corrupted images by effectively filling various missing regions. Among these, Generative Adversarial Networks (GANs) and Denoising Diffusion Probabilistic Models (DDPMs) have been employed as key generative image completion approaches, excelling in the field of generating high-quality restorations with reduced artifacts and improved fine details. In previous work, we developed a method aimed at synthesizing views from novel microscope positions for mastoidectomy surgeries; however, that approach did not have the ability to restore the surrounding surgical scene environment. In this paper, we propose an efficient method to complete the surgical scene of the synthetic postmastoidectomy dataset. Our approach leverages self-supervised learning on real surgical datasets to train a Single-Step Denoising Diffusion-GAN (SSDD-GAN), combining the advantages of diffusion models with the adversarial optimization of GANs for improved Structural Similarity results of 6%. The trained model is then directly applied to the synthetic postmastoidectomy dataset using a zero-shot approach, enabling the generation of realistic and complete surgical scenes without the need for explicit ground-truth labels from the synthetic postmastoidectomy dataset. This method addresses key limitations in previous work, offering a novel pathway for full surgical microscopy scene completion and enhancing the usability of the synthetic postmastoidectomy dataset in surgical preoperative planning and intraoperative navigation.",
        "authors": "Yike Zhang, Eduardo Davalos, Jack Noble",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.01"
    },
    {
        "number": 6,
        "UID": "F-6",
        "forum": "https://openreview.net/forum?id=iEBZjNZ63T",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "PCA-YOLO: A Small Liver Tumor Detection Model with Patch-Contrastive Attention",
        "abstract": "Liver tumors, as one of the most common malignant tumor types, represent a significant clinical challenge, with the detection of small tumors being particularly problematic. \nDespite the rapid advances in deep learning (DL) offering significant support in reducing the workload of radiologists, current detection models still struggle with the detection of small tumors. This is particularly troubling as these are the cases where even experienced radiologists are more prone to errors, underscoring the critical need for improved accuracy of detection methods in this area. Addressing this critical gap, this article introduces patch-contrastive attention YOLO (PCA-YOLO), an innovative adaptation of the YOLO framework, incorporating a patch-based attention module to specifically target the detection of small liver tumors. Furthermore, we collected a specialized CT dataset focusing exclusively on small liver tumors, complemented with meticulously annotated bounding boxes, to facilitate this study. Our experimental findings demonstrate that our approach achieves a leading mean Average Precision (mAP) score of 77.2\\% at a 50\\% Intersection Over Union (IoU) threshold, surpassing all current leading detection methods tested against our specialized dataset.",
        "authors": "Xueyang Li, Han Xiao, Zongpeng Weng, Xinrong Hu, Danny Chen, Yiyu Shi",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.02"
    },
    {
        "number": 17,
        "UID": "F-17",
        "forum": "https://openreview.net/forum?id=rbs8UZa7nU",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "DeFusion: An Effective Decoupling Fusion Network for Multi-Modal Pregnancy Prediction",
        "abstract": "Temporal embryo images and parental fertility table indicators are both valuable for pregnancy prediction in in vitro fertilization embryo transfer (IVF-ET). However, current machine learning models cannot make full use of the complementary information between the two modalities to improve pregnancy prediction performance. In this paper, we propose a Decoupling Fusion Network called DeFusion to effectively integrate the multi-modal information for IVF-ET pregnancy prediction. Specifically, we propose a decoupling fusion module that decouples the information from the different modalities into related and unrelated information, thereby achieving a more delicate fusion. And we fuse temporal embryo images with a spatial-temporal position encoding, and extract fertility table indicator information with a table transformer. To evaluate the effectiveness of our model, we use a new dataset including 4046 cases collected from Southern Medical University. The experiments show that our model outperforms state-of-the-art methods. Meanwhile, the performance on the eye disease prediction dataset reflects the model's good generalization. Our code and dataset are available at https://github.com/Ou-Young-1999/DFNet.",
        "authors": "Xueqiang Ouyang, Jia Wei, Wenjie Huo, Xiaocong Wang, Rui Li, Jianlong Zhou",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.03"
    },
    {
        "number": 25,
        "UID": "F-25",
        "forum": "https://openreview.net/forum?id=cj10StANqJ",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Can Diffusion Models Generalize? Privacy and Fairness Trade-offs for Medical Data Sharing.",
        "abstract": "The recent surge in options for diffusion model-based synthetic data sharing offers significant benefits for medical research, provided privacy and fairness concerns are addressed.\nGenerative models risk memorizing sensitive training samples, potentially exposing identifiable information.\nSimultaneously, underrepresented features -- such as rare diseases, uncommon medical devices, or infrequent patient ethnicities -- are often not learned well, creating unfair biases in downstream applications.\nOur work unifies these challenges by leveraging artificially generated fingerprints (SAFs) in the training data as a controllable test for memorization and fairness.\nSpecifically, we measure whether a diffusion model reproduces these fingerprints verbatim (a privacy breach) or ignores them entirely (a fairness violation) and introduce an indicator t' to quantify finished models for the likelihood of reproducing training samples.\nExtensive experiments on real and synthetic medical imaging datasets reveal that na\\\"ive diffusion model training can lead to privacy leaks or unfair coverage.\nBy systematically incorporating SAFs and monitoring t', we demonstrate how to balance privacy and fairness objectives.\nOur evaluation framework provides actionable guidance for designing generative models that preserve patient anonymity without excluding underrepresented patient subgroups. Code is available at https://github.com/MischaD/Privacy.",
        "authors": "Mischa Dombrowski, Bernhard Kainz",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.04"
    },
    {
        "number": 26,
        "UID": "F-26",
        "forum": "https://openreview.net/forum?id=KqAUFNe63I",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Mitigating analytical variability in fMRI with style transfer",
        "abstract": "We propose a novel approach to facilitate the re-use of neuroimaging results by converting statistic maps across different functional MRI pipelines. We make the assumption that pipelines used to compute fMRI statistic maps can be considered as a style component and we propose to use different generative models, among which, Generative Adversarial Networks (GAN) and Diffusion Models (DM) to harmonize statistic maps across different pipelines. We explore the performance of multiple GAN and DM frameworks for unsupervised multi-domain style transfer. We developed an auxiliary classifier that distinguishes statistic maps from different pipelines, allowing us to validate pipeline transfer, but also to extend traditional sampling techniques used in DM to improve the transition performance. Our experiments demonstrate that our proposed methods are successful: pipelines can indeed be transferred as a style component, providing an important source of data augmentation for future studies.",
        "authors": "Elodie Germani, Camille Maumet, Elisa Fromont",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.05"
    },
    {
        "number": 28,
        "UID": "F-28",
        "forum": "https://openreview.net/forum?id=25wvW3T7AR",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "SRMRI: An Open Dataset and Benchmarks for Blind Super-Resolution of 2D MRI",
        "abstract": "Existing deep learning methods for medical image super-resolution (SR) often rely on paired datasets generated by simulating low-resolution (LR) images from corresponding high-resolution (HR) scans, which can introduce biases and degrade real-world performance. To overcome these limitations, we present an unsupervised approach based on a score-based diffusion model that does not require paired training data. We train a score-based diffusion model using denoising score matching on HR Magnetic Resonance Imaging (MRI) scans, then perform iterative refinement with a stochastic differential equation (SDE) solver while enforcing data consistency from LR scans. Our method provides faster sampling compared to existing generative approaches and achieves competitive results on key metrics, though it does not surpass fully supervised baselines in PSNR and SSIM. Notably, while supervised models often report higher numerical metrics, we observe that they can produce suboptimal reconstructions due to their reliance on fixed upscaling kernels. Finally, we introduce the SRMRI dataset, containing LR and HR images obtained from scanner for training and evaluating MR image super-resolution models. Code and dataset are available at: https://github.com/arpanpoudel/SRMRI",
        "authors": "Arpan Poudel, Mamata Shrestha, Nian Wang, Ukash Nakarmi",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.06"
    },
    {
        "number": 83,
        "UID": "F-83",
        "forum": "https://openreview.net/forum?id=PWw0GoQUXV",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Anatomy-Guided Surface Diffusion Model for Alzheimer‚Äôs Disease Normative Modeling",
        "abstract": "Normative modeling has emerged as a pivotal approach for\ncharacterizing heterogeneity and individual variance in neurodegenera-\ntive diseases, notably Alzheimer‚Äôs disease(AD). One of the challenges of\ncortical normative modeling is the anatomical structure mismatch due\nto folding pattern variability. Traditionally, registration is applied to ad-\ndress this issue and recently many studies have utilized deep generative\nmodels to generate anatomically aligned samples for analyzing disease\nprogression; however, these models are predominantly applied to volume-\nbased data, which often falls short in capturing intricate morphological\nchanges on the brain cortex. As an alternative, surface-based analysis\nhas been proven to be more sensitive in disease modeling such as AD,\nyet, like volume-based data, it also suffers from the mismatch problem.\nTo address these limitations, we propose a novel generative normative\nmodeling framework by transferring the conditional diffusion generative\nmodel to the spherical domain. Furthermore, the proposed model gener-\nates normal feature map distributions by explicitly conditioning on indi-\nvidual anatomical segmentation to ensure better geometrical alignment\nwhich helps to reduce variance between subjects in normative analyses.\nWe find that our model can generate samples that are better anatomi-\ncally aligned than registered reference data and through ablation study\nand normative assessment experiments, the samples are able to better\nmeasure individual differences from the normal distribution and increase\nsensitivity in differentiating cognitively normal (CN), mild cognitive im-\npairment (MCI), and Alzheimer‚Äôs disease (AD) patients.",
        "authors": "Jianwei Zhang, Yonggang Shi",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.07"
    },
    {
        "number": 94,
        "UID": "F-94",
        "forum": "https://openreview.net/forum?id=cBfXsr6xWb",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Semi-Supervised Skin Lesion Segmentation under Dual Mask Ensemble with Feature Discrepancy Co-Training",
        "abstract": "Skin Lesion Segmentation with supportive Deep Learning has become essential in skin lesion analysis and skin cancer diagnosis. However, in the practical scenario of clinical implementation, there is a limitation in human-annotated labels for training data, which leads to poor performance in supervised training models. In this paper, we propose Dual Mask Ensemble (DME) based on a dual-branch co-training network, which aims to enforce two models to exploit information from different views. Specifically, we introduce a novel feature discrepancy loss trained with a cross-pseudo supervision strategy, which enhances model representation by encouraging the sub-networks to learn from distinct features, thereby mitigating feature collapse. Additionally, Dual Mask Ensemble training enables the sub-models to extract more meaningful information from unlabeled data by combining mask predictions. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art performance across several metrics (Dice and Jaccard) on the ISIC2018 and HAM10000 datasets. Our code is available at https://github.com/antares0811/DME-FD.",
        "authors": "Thanh-Huy Nguyen, Thien Nguyen, Xuan Bach Nguyen, Nguyen Lan Vi Vu, Vinh Quang Dinh, Fabrice MERIAUDEAU",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.08"
    },
    {
        "number": 110,
        "UID": "F-110",
        "forum": "https://openreview.net/forum?id=Yjv27GWb3n",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "A Novel GNN Framework Integrating Neuroimaging and Behavioral Information to Understand Adolescent Psychiatric Disorders",
        "abstract": "Functional connectivity (FC) is widely used to study various psychiatric disorders, but its consistency is often undermined by significant inter-subject variability. While these differences can be reflected by behavioral characteristics, few studies have combined them with FC. To this end, we propose a novel graph learning framework that enhances differentiation of psychiatric disorders by integrating FC with behavioral characteristics. Additionally, we apply Grad-CAM to enhance model interpretability by identifying key regions of interest involved in distinguishing individuals with psychiatric disorders from healthy controls. Experiments with the Adolescent Brain Cognitive Development dataset highlighted two critical insights: the thalamus and specific ROIs within the somatomotor and cingulo-opercular networks are vital for identifying psychiatric disorders. Additionally, visualization of latent representations indicated that individuals with externalizing disorders, specifically Oppositional Defiant Disorder, are distinguishable from healthy controls. These findings highlight the potential of our graph learning framework in discerning psychiatric disorders, offering potential for enhanced diagnostic accuracy.",
        "authors": "Weifeng Yu, Gang Qu, Young-geun Kim, Lei Xu, Aiying Zhang",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.09"
    },
    {
        "number": 154,
        "UID": "F-154",
        "forum": "https://openreview.net/forum?id=AYYZA15v9h",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "PixelCAM: Pixel Class Activation Mapping for Histology Image Classification and ROI Localization",
        "abstract": "Weakly supervised object localization (WSOL) methods allow training models to classify images and localize ROIs. WSOL only requires low-cost image-class annotations, yet provides a visually interpretable classifier which is important in histology image analysis. Standard WSOL methods rely on class activation mapping (CAM) methods to produce spatial localization maps according to a single- or two-step strategy. While both strategies have made significant progress, they still face several limitations with histology images. Single-step methods can easily result in under- or over-activation due to the limited visual ROI saliency in histology images and the limited localization cues. They also face the well-known issue of asynchronous convergence between classification and localization tasks. The two-step approach is sub-optimal because it is tied to a frozen classifier, limiting the capacity for localization. Moreover, these methods also struggle when applied to out-of-distribution (OOD) datasets. In this paper, a multi-task approach for WSOL is introduced for simultaneous training of both tasks to address the asynchronous convergence problem. In particular, localization is performed in the pixel-feature space of an image encoder that is shared with classification. This allows learning discriminant features and accurate delineation of foreground/background regions to support ROI localization and image classification. We propose PixelCAM, a cost-effective foreground/background pixel-wise classifier in the pixel-feature space that allows for spatial object localization. Using partial-cross entropy, PixelCAM is trained using pixel pseudo-labels collected from a pretrained WSOL model. Both image and pixel-wise classifiers are trained simultaneously using standard gradient descent. In addition, our pixel classifier can easily be integrated into CNN- and transformer-based architectures without any modifications. Our extensive experiments on GlaS and CAMELYON16 cancer datasets show that PixelCAM can significantly improve classification and localization performance when integrated with different WSOL methods. Most importantly, it provides robustness on both tasks for OOD data linked to different cancer types, with large domain shifts between training and testing image data.",
        "authors": "alexis guichemerre, Soufiane Belharbi, Mohammadhadi Shateri, Luke McCaffrey, Eric Granger",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.10"
    },
    {
        "number": 163,
        "UID": "F-163",
        "forum": "https://openreview.net/forum?id=zt69HTmOjP",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification",
        "abstract": "Graph Neural Networks (GNNs) have recently been found to excel in histopathology. However, an important histopathological task, where GNNs have not been extensively explored, is the classification of glomeruli health as an important indicator in nephropathology. This task presents unique difficulties, particularly for the graph construction, i.e., the identification of nodes, edges, and informative features. In this work, we propose a pipeline composed of different traditional and machine learning-based computer vision techniques to identify nodes, edges, and their corresponding features to form a heterogeneous graph. We then proceed to propose a novel heterogeneous GNN architecture for glomeruli classification, called HIEGNet, that integrates both glomeruli and their surrounding immune cells. Hence, HIEGNet is able to consider the immune environment of each glomerulus in its classification. Our HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney transplant patients. Experimental results demonstrate that HIEGNet outperforms several baseline models and generalises best between patients among all baseline models. Our implementation is publicly available at https://github.com/nklsKrmnn/HIEGNet.git.",
        "authors": "Niklas Kormann, Masoud Ramuz, Zeeshan Nisar, Nadine S. Schaadt, Hendrik Annuth, Benjamin Doerr, Friedrich Feuerhake, Thomas Lampert, Johannes F. Lutzeyer",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.11"
    },
    {
        "number": 171,
        "UID": "F-171",
        "forum": "https://openreview.net/forum?id=CykTyfqXcl",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Learning from a Few Shots: Data-efficient Cervical Vertebral Maturation Assessment",
        "abstract": "The timing of treatment is a crucial decision in orthodontics. Initiating treatment during\nthe appropriate growth phase leads to optimal patient outcomes and can prevent prolonged\ntreatment durations. The most commonly used method for classifying growth phases is\ncervical vertebral maturation (CVM) assessment, which categorizes CVM into six stages\nbased on the shape and size of the cervical vertebrae. Due to the complexity of manual CVM\nanalysis, it often falls short in performance when assessed visually. Deep learning methods\ncan assist physicians in classifying CVM stages, thus improving orthodontic workflows and\ntreatments. However, a significant challenge in deep learning-based CVM assessment is\nthe limited dataset volume, resulting from difficulties in data collection and annotation.\nWhile small training datasets can greatly hinder the model‚Äôs generalization performance,\nresearch on data-efficient training methods for CVM assessment is still lacking. To the best\nof our knowledge, this paper is the first to evaluate the potential of few-shot learning and in-\ndomain transfer learning for CVM assessment. Specifically, we investigate the architectures\nResNet18 and MedSam-2D. Few-shot learning enhances classification performance by up\nto 9%. Additionally, in-domain pre-training (using chest X-ray data) results in a significant\nperformance increase of up to 4%.",
        "authors": "Helen Schneider, Aditya Parikh, Priya Priya, Maximilian Bro√ü, Tom Verhofstadt, Anna Konermann, Rafet Sifa",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.12"
    },
    {
        "number": 239,
        "UID": "F-239",
        "forum": "https://openreview.net/forum?id=R4lRVjL4KX",
        "Track": "Full paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Feature Attribution for Deep Learning Models through Total Variance Decomposition",
        "abstract": "This paper introduces a new approach to feature attribution for deep learning models, quantifying the importance of specific features in model decisions. By decomposing the total variance of model decisions into explained and unexplained fractions, conditioned on the target feature, we define the feature attribution score as the proportion of explained variance. This method offers a solid statistical foundation and normalized quantitative results. When ample data is available, we compute the score directly from test data. For scarce data, we use constrained sampling with generative diffusion models to represent the conditional distribution at a given feature value. We demonstrate the method‚Äôs effectiveness on both a synthetic image dataset with known ground truth and OASIS-3 brain MRIs.",
        "authors": "Yinzhu Jin, Shen Zhu, Tom Fletcher",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - F.13"
    },
    {
        "number": 2,
        "UID": "S-2",
        "forum": "https://openreview.net/forum?id=y1kZmZ8Bqe",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Weakly-Supervised Midline Shift Quantification through Simulating the Reversed Disease Progression",
        "abstract": "Medical segmentation masks are often scarce. To get visual and quantitative information,\nwe propose constructing trajectories from anomaly data to normal data using conditional\nflow matching on an autoencoder, augmented with an auxiliary classification head in the\nlatent space. We demonstrate the effectiveness of our method through weakly supervised\nmidline shift estimation.",
        "authors": "Chih-Chieh Chen, Chang-Fu Kuo",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.01"
    },
    {
        "number": 13,
        "UID": "S-13",
        "forum": "https://openreview.net/forum?id=9HYs8kyN9I",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Gradual Learning for Semi-Supervised 3D Segmentation",
        "abstract": "We propose an extension of the recently published Gradual Learning, a semi-supervised method for segmentation of slice-stacks. While the original Gradual Learning is based on 2D slices to leverage the high similarity within the local neighborhood, the extension utilizes 3D subvolumes instead. \nThus, a 3D segmentation network is trained on initial subvolumes and the corresponding ground truth. Afterward, pseudo labels of the expanded subvolumes are generated, which are reused for training. This process is repeated a set number of times.\nThe approach results in improved segmentation quality without the need for large expert-labeled data sets.\nThe method was evaluated on head magnetic resonance imaging scans for brain segmentation but can be easily transferred to other modalities. \nThe results showed high gains in Intersection over Union scores on a separate test data set (depending on the number of used subvolumes $n$: $n$=2: $0.30 \\rightarrow 0.58$, $n$=3: $0.41 \\rightarrow 0.63$, $n$=10: $0.55 \\rightarrow 0.76$, training with full volume: $0.86$).",
        "authors": "Johann Christopher Engster, Nele Blum, Laura Hellwege, Thorsten Buzug, Maik Stille",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.02"
    },
    {
        "number": 14,
        "UID": "S-14",
        "forum": "https://openreview.net/forum?id=DbuI6sjRce",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "FAZ Segmentation Quality Assessment in OCTA via Denoising Autoencoders and Segmentation Uncertainty Estimation",
        "abstract": "Accurate segmentation quality assessment is essential in medical imaging, particularly in preventing segmentation algorithms from failing silently. We propose a Segmentation Quality Assessment Framework method that estimates segmentation quality without relying on ground-truth labels. Our approach integrates learning-free uncertainty estimation with a Denoising Autoencoder (DAE) to generate pseudo-labels, extract key statistical features, and train a Random Forest Regressor (RDF) for quality prediction. Experimental results demonstrate that our method outperforms baseline approaches on three external datasets, showcasing its robustness to image domain shifts. our method enhances the scalability and generalizability of real-world medical imaging applications by leveraging segmentation models to handle cases where manual annotations are missing or infeasible.",
        "authors": "hana jebril, Guilherme Aresta, Hrvoje Bogunoviƒá",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.03"
    },
    {
        "number": 33,
        "UID": "S-33",
        "forum": "https://openreview.net/forum?id=EjqP4vHnHL",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "OpenDIVE: Streamlining Tractography Visualization",
        "abstract": "Diffusion-weighted magnetic resonance imaging (DW-MRI) models provide a non-invasive\nmethod for mapping the structure of white matter. Despite the prolific availability of soft-\nware tools to visualize DW-MRI, an MRI acquisition technique with increasing prevalence\nin deep learning applications, mapping meaningful summary statistics of white matter re-\nlationships can be a time-consuming process. We propose a Python-based command line\ntool to both standardize and improve accessibility to white matter models. We generate a\nstandardized display of anatomical images to overlay diffusion models based on user input,\nand we provide a mechanism to display summary statistics (percent change, p-value, deep\nlearning metrics, etc.) with a changeable color bar.",
        "authors": "Adam M. Saunders, Elyssa M. McMaster, Chris Rorden, Johaan Kathilankal Jis, Minyi Sun, Adam Sadriddinov, Lukas VanTilburg, Michael Eugene Kim, Bennett Allan Landman, Kurt Schilling",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.04"
    },
    {
        "number": 47,
        "UID": "S-47",
        "forum": "https://openreview.net/forum?id=TS04XYMNPn",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Human Aligned Reward Modeling for Automated Transfer Function Generation of 3D Rendering of Medical Image Data",
        "abstract": "We propose a reinforcement learning framework to automate the design of 2D transfer functions for direct volume rendering of medical images. By training a reward model based on human feedback, our approach enables an agent to extract transfer functions from joint histograms without manual fine-tuning. Preliminary results demonstrate that the developed method effectively captures human preferences, marking a significant step towards automated, user-aligned 3D renderings for improved patient communication, diagnosis, and treatment planning.",
        "authors": "David Melenberg, Nele Blum, Cem Adiyaman, Thorsten Buzug, Maik Stille",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.05"
    },
    {
        "number": 60,
        "UID": "S-60",
        "forum": "https://openreview.net/forum?id=JRM9HjkYSQ",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "An Interpretable Representation Learning Approach for Diffusion Tensor Imaging",
        "abstract": "Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the structural connectivity of the brain, but presents challenges in effective representation and interpretation in deep learning models.  In this work, we propose a novel 2D representation of DTI tractography that encodes tract-level fractional anisotropy (FA) values into a 9$\\times$9 grayscale image. This representation is processed through a Beta-Total Correlation Variational Autoencoder ($\\beta$-TCVAE) to learn a disentangled and interpretable latent embedding. We evaluate the quality of this embedding using supervised and unsupervised representation learning strategies, including auxiliary classification, triplet loss, and SimCLR-based contrastive learning. Compared to the 1D Group deep neural network (DNN) baselines, our approach improves the F1 score in a downstream sex classification task by 15.74\\% and shows a better disentanglement than the 3D representation.",
        "authors": "Vishwa Mohan Singh, Alberto Gaston Villagran Asiares, Luisa Sophie Schuhmacher, Kate Rendall, Simon Wei√übrod, David R√ºgamer, Inga K√∂rte",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.06"
    },
    {
        "number": 62,
        "UID": "S-62",
        "forum": "https://openreview.net/forum?id=jrMCsWiAYq",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "RoentMod: A Synthetic Chest X-Ray Modification Model to Identify Image Interpretation Model Shortcuts",
        "abstract": "Deep learning models can accurately identify pathology on chest x-ray (CXR) images in research settings but often have worse performance in external testing in part due to shortcut learning, where models rely on confounding factors in the training data rather than truly learning the appearance of target pathology. This work introduces RoentMod: a generative deep learning model to alter an existing CXR with a text prompt describing pathology like cardiomegaly or pneumonia. Using RoentMod, we find that CXR interpretation models are sensitive to the addition of off-target pathology, suggesting the use of shortcuts.",
        "authors": "Lauren H. Cooke, Matthias Jung, Jan M. Brendel, Nora M. Kerkovits, Borek Foldyna, Michael T Lu, Vineet K Raghu",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.07"
    },
    {
        "number": 63,
        "UID": "S-63",
        "forum": "https://openreview.net/forum?id=DhgX8IWpki",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Lightweight Model Adaptation for Mitigating Bias in Deep Learning Models for Chest X-Ray Analysis",
        "abstract": "Deep learning (DL) models have demonstrated significant potential in improving chest X-ray (CXR) diagnosis. However, these models may exacerbate healthcare disparities. Addressing the inherent biases of DL models is essential to ensure their safe and reliable deployment in clinical practice. We suggest a novel bias mitigation approach that combines embeddings extracted by a Convolutional Neural Network (CNN) with an eXtreme Gradient Boosting (XGBoost) classifier. Our results show that this hybrid model significantly reduces bias across the sensitive attributes sex, age, and race, while maintaining comparable overall diagnostic performance and without the need for expensive model retraining. Our approach demonstrates that integrating simple, interpretable, and computationally efficient modifications into existing models can effectively enhance fairness in medical imaging.",
        "authors": "Clemence Mottez, Louisa Fay, Jean-Benoit Delbrouck, Curtis Langlotz",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.08"
    },
    {
        "number": 64,
        "UID": "S-64",
        "forum": "https://openreview.net/forum?id=h30b7oDs4O",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "GRASP: Graph Augmentation via Sampling and Permutation",
        "abstract": "Structural brain graphs illuminate individual differences and neurological traits but are underutilized due to limited data from the challenges of MRI acquisition and preprocessing. We introduce Graph Augmentation via Sampling and Permutation (GRASP), a method that synthesizes brain graphs by sampling edge values from consistent positions across multiple adjacency matrices within the same class---assuming topological consistency. Unlike deep learning techniques, GRASP relies on straightforward manipulations of adjacency matrices, which reduces computational demands and simplifies implementation. In this paper, we examine the proof of concept of this augmentation technique on a gender classification task using structural connectomes. We demonstrate enhanced brain graph classification and confirm that within-class adjacency consistency can generate graph variants without complex modeling. The code is publicly available at: https://github.com/heliasah/GRASP-Code",
        "authors": "Ahmed Nebli, Helia Sahebghadam",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.09"
    },
    {
        "number": 81,
        "UID": "S-81",
        "forum": "https://openreview.net/forum?id=HKePQcCP0R",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Long-MS-Diff: Towards Generating Anatomically Plausible Lesion Progression in MS Imaging using Diffusion Models",
        "abstract": "Monitoring lesion progression in Multiple Sclerosis (MS) is vital for assessing disease activity and guiding treatment decisions. However, the limited availability of annotated longitudinal MRI data presents a challenge for developing robust deep learning models. We introduce Long-MS-Diff, a conditional diffusion-based framework for generating anatomically plausible follow-up brain scans in MS. The model is conditioned on baseline images, lesion change masks, and scalar lesion-level features. To address the extreme sparsity of new lesions, we incorporate an auxiliary segmentation task and propose an adaptive weighted loss to balance anatomical reconstruction with lesion-specific fidelity. A radiologist assessment of synthetic scans confirms high image quality, anatomical plausibility, and lesion adherence. In downstream segmentation experiments, moderate augmentation with Long-MS-Diff improves performance, outperforming models trained on real data alone. These results highlight the value of controlled synthetic data in modelling disease progression and demonstrate the utility of diffusion-based generation in data-scarce clinical settings.",
        "authors": "Prateek Mathur, Brendan S Kelly, Ronan P. Killeen, Aonghus Lawlor",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.10"
    },
    {
        "number": 82,
        "UID": "S-82",
        "forum": "https://openreview.net/forum?id=SifPKZmav5",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Thyroid Cancer Detection using Smartphone-captured Cytopathology Images and Foundation Model",
        "abstract": "Recent foundation models for pathology‚Äîsuch as UNI, CONCH, RudolfV, Prov-GigaPath, Atlas, and Virchow2 ‚Äîhave demonstrated impressive performance, but are typically trained on images captured by costly whole-slide imaging scanners. In contrast, many hospitals in developing countries still rely on optical microscopes and low-cost cameras or smartphones for image acquisition. To bridge the gap, we demonstrate that pathology foundation model pre-trained on whole-slide images (WSIs) can be fine-tuned on smartphone-captured cytopathology images for applications in low-resource settings. We used over 3,000 smartphone-captured cytology images from Tanzania and Vietnam for Virchow2 foundation model fine-tuning and testing. Our approach not only resulted in high classification performance (98.17\\% AUC, 92.93\\% accuracy, 94.13\\% F1-score) but also enhanced interpretability through principle component visualization of the embedding space, thereby fostering clinical trust in resource-constrained settings.",
        "authors": "Aiden Coffey, Vladimir Cuc, Hoan Ngo, Walter Lee",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.11"
    },
    {
        "number": 94,
        "UID": "S-94",
        "forum": "https://openreview.net/forum?id=bCoOmEAduj",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Evaluating Strategic Sampling of Conditional Diffusion Model-based Data Augmentation",
        "abstract": "The scarcity of large and well-annotated datasets is a concern in medical image analysis, particularly for emerging applications without substantial public dataset releases. Data synthesis has become relevant to address this problem, as conditional generative models can provide extensive amounts of data. However, the diversity of these synthetic samples can be limited to their training distribution, which restricts the benefits of synthetic data for augmentation. This paper analyses this limitation in the context of medical image classification using two datasets: chest X-ray and strep pharyngitis detection in smartphone photos. Our findings reveal that the performance improvements when augmenting training datasets with generated samples can be inconsistent. Furthermore, in some cases, using a small number of strategically chosen synthetic samples can outperform a larger, randomly selected synthetic sets. This highlights the need for effective sampling strategies in conditional diffusion models to improve training diversity and enhance performance in downstream applications.",
        "authors": "Aditya Vikas Kulkarni, Roger D. Soberanis-Mukul, Alex Pinsk, Brittany-Lee Smith, Ashlynn Cobb, Alex Pacl, Pranay Marlecha, Jaiprakash Suresh Gurav, Rashmi Sachan, Therese Canares, Mathias Unberath",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.12"
    },
    {
        "number": 97,
        "UID": "S-97",
        "forum": "https://openreview.net/forum?id=e140atghbv",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "SRA: A Novel Method to Improve Feature Embedding in Self-supervised Learning for Histopathological Images",
        "abstract": "Self-supervised learning has become a cornerstone in various areas, particularly histopathological image analysis. Image augmentation plays a crucial role in self-supervised learning, as it generates variations in image samples. However, traditional image augmentation techniques often overlook the unique characteristics of histopathological images. In this paper, we propose a new histopathology-specific image augmentation method called stain reconstruction augmentation (SRA). We integrate our SRA into various self-supervised learning models. We demonstrate that our SRA always improves the standard models across various downstream tasks and achieves superior performance to a state-of-the-art foundation model pre-trained on significantly larger histopathology datasets.",
        "authors": "Hamid Manoochehri, Bodong Zhang, Beatrice Knudsen, Tolga Tasdizen",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.13"
    },
    {
        "number": 98,
        "UID": "S-98",
        "forum": "https://openreview.net/forum?id=vZEgKxcUQJ",
        "Track": "Short paper",
        "Session": "Poster 2.A: Responsible Al",
        "Final Decision": "Poster",
        "title": "Transfer Learning and Quantization for Efficient AP vs. LA X-Ray View Classification on an Edge Device",
        "abstract": "In this paper, we present a framework for classifying X‚Äëray images as either anterior-posterior (AP) or lateral (LA) by combining transfer learning with model quantization to optimize deep learning models for deployment on an edge device. We perform transfer learning on a pre-trained MobileNetV2 using a dataset of 800 images (400 AP, 400 LA). We employ 5‚Äëfold cross‚Äëvalidation, where each fold has 640 images for training and 160 images for testing. Subsequently, we apply multiple quantization techniques including FP32, FP16, dynamic, and Int8 to reduce the model's size and enhance inference speed. Evaluating across the 5 folds (160 test images per fold), our evaluation showed that quantization preserves over 98% classification accuracy while reducing the original model size from 11.3‚ÄØMB to as low as 2.6‚ÄØMB for the quantized variants. We compare the performance on a personal computer (PC) with a graphical processing unit (GPU) and on an edge device. Although the GPU-based implementation exhibited lower warmup and steady-state inference times, the steady-state performance on the edge device remains competitive despite higher initialization overhead. Our results show that we can use transfer learning to leverage large-scale pre-trained models for specific applications. Our quantization strategies enable efficient, real-time AP/LA X‚Äëray view classification on an edge device, making it a promising solution for clinical use.",
        "authors": "Keshav Bimbraw, Daniel Steines",
        "Time": "Day 2 \u2014 10:30am-11:30am",
        "Poster time": "Day 2 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 2.A - S.14"
    },
    {
        "number": 10,
        "UID": "O-10",
        "forum": "https://openreview.net/forum?id=oqZYjWKYkg",
        "Track": "Full paper",
        "Session": "Oral 2.B: Clinical Diagnostics",
        "Final Decision": "Oral",
        "title": "MagNet: Multi-Level Attention Graph Network for Predicting High-Resolution Spatial Transcriptomics",
        "abstract": "The rapid development of spatial transcriptomics (ST) offers new opportunities to explore the gene expression patterns within the spatial microenvironment. Current research integrates pathological images to infer gene expression, addressing the high costs and time-consuming processes to generate spatial transcriptomics data. However, as spatial transcriptomics resolution continues to improve, existing methods remain primarily focused on gene expression prediction at low-resolution (55ùúám) spot levels. These methods face significant challenges, especially the information bottleneck, when they are applied to high-resolution (8ùúám) HD data. To bridge this gap, this paper introduces MagNet, a multi-level attention graph network designed for accurate prediction of high-resolution HD data. MagNet employs cross-attention layers to integrate features from multi-resolution image patches hierarchically and utilizes a GAT-Transformer module to aggregate neighborhood information. By integrating multilevel features, MagNet overcomes the limitations posed by low-resolution inputs in predicting high-resolution gene expression. We systematically evaluated MagNet and existing ST prediction models on both a private spatial transcriptomics dataset and a public dataset at three different resolution levels. The results demonstrate that MagNet achieves state-of-the-art performance at both spot level and high-resolution bin levels, providing a novel methodology and benchmark for future research and applications in high-resolution HD-level spatial transcriptomics. Code is available at https://github.com/Junchao-Zhu/MagNet.",
        "authors": "Junchao Zhu, Ruining Deng, Tianyuan Yao, Juming Xiong, Chongyu Qu, Junlin Guo, Siqi Lu, Yucheng Tang, Daguang Xu, Mengmeng Yin, Yu Wang, Shilin Zhao, Yaohong Wang, Haichun Yang, Yuankai Huo",
        "Time": "Day 2 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.01"
    },
    {
        "number": 10,
        "UID": "F-10",
        "forum": "https://openreview.net/forum?id=oqZYjWKYkg",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "MagNet: Multi-Level Attention Graph Network for Predicting High-Resolution Spatial Transcriptomics",
        "abstract": "The rapid development of spatial transcriptomics (ST) offers new opportunities to explore the gene expression patterns within the spatial microenvironment. Current research integrates pathological images to infer gene expression, addressing the high costs and time-consuming processes to generate spatial transcriptomics data. However, as spatial transcriptomics resolution continues to improve, existing methods remain primarily focused on gene expression prediction at low-resolution (55ùúám) spot levels. These methods face significant challenges, especially the information bottleneck, when they are applied to high-resolution (8ùúám) HD data. To bridge this gap, this paper introduces MagNet, a multi-level attention graph network designed for accurate prediction of high-resolution HD data. MagNet employs cross-attention layers to integrate features from multi-resolution image patches hierarchically and utilizes a GAT-Transformer module to aggregate neighborhood information. By integrating multilevel features, MagNet overcomes the limitations posed by low-resolution inputs in predicting high-resolution gene expression. We systematically evaluated MagNet and existing ST prediction models on both a private spatial transcriptomics dataset and a public dataset at three different resolution levels. The results demonstrate that MagNet achieves state-of-the-art performance at both spot level and high-resolution bin levels, providing a novel methodology and benchmark for future research and applications in high-resolution HD-level spatial transcriptomics. Code is available at https://github.com/Junchao-Zhu/MagNet.",
        "authors": "Junchao Zhu, Ruining Deng, Tianyuan Yao, Juming Xiong, Chongyu Qu, Junlin Guo, Siqi Lu, Yucheng Tang, Daguang Xu, Mengmeng Yin, Yu Wang, Shilin Zhao, Yaohong Wang, Haichun Yang, Yuankai Huo",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.01"
    },
    {
        "number": 14,
        "UID": "O-14",
        "forum": "https://openreview.net/forum?id=ghhGImwv07",
        "Track": "Full paper",
        "Session": "Oral 2.B: Clinical Diagnostics",
        "Final Decision": "Oral",
        "title": "DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic Data for Cardiac Hypertension Diagnosis",
        "abstract": "Multimodal data fusion has emerged as a key approach in recent years for enhancing diagnosis and prognosis in many medical applications. With the advent of transformer-based methods, it is now possible to combine information from different modalities that provide complementary insights. However, most existing methods rely on symmetric fusion schemes, assuming equal importance for information carried by each modality‚Äîa strong assumption that may not always hold true. In this study, we propose an alternative fusion strategy based on an asymmetric scheme. Starting with a primary modality that offers the most critical information, we integrate secondary modality contributions by disentangling shared and modality-specific information. The proposed model was validated on a dataset of 239 patients for characterizing hypertension severity by fusing time series automatically extracted from echocardiographic image sequences and tabular data from patient records. Results show that our approach outperforms existing unimodal and multimodal approaches, achieving an AUC score over 90\\% - a crucial benchmark for clinical use.",
        "authors": "J√©r√©mie Stym-Popper, Nathan Painchaud, Pierre-Yves COURAND, Cl√©ment Rambour, Nicolas THOME, Olivier Bernard",
        "Time": "Day 2 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.02"
    },
    {
        "number": 14,
        "UID": "F-14",
        "forum": "https://openreview.net/forum?id=ghhGImwv07",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "DAFTED: Decoupled Asymmetric Fusion of Tabular and Echocardiographic Data for Cardiac Hypertension Diagnosis",
        "abstract": "Multimodal data fusion has emerged as a key approach in recent years for enhancing diagnosis and prognosis in many medical applications. With the advent of transformer-based methods, it is now possible to combine information from different modalities that provide complementary insights. However, most existing methods rely on symmetric fusion schemes, assuming equal importance for information carried by each modality‚Äîa strong assumption that may not always hold true. In this study, we propose an alternative fusion strategy based on an asymmetric scheme. Starting with a primary modality that offers the most critical information, we integrate secondary modality contributions by disentangling shared and modality-specific information. The proposed model was validated on a dataset of 239 patients for characterizing hypertension severity by fusing time series automatically extracted from echocardiographic image sequences and tabular data from patient records. Results show that our approach outperforms existing unimodal and multimodal approaches, achieving an AUC score over 90\\% - a crucial benchmark for clinical use.",
        "authors": "J√©r√©mie Stym-Popper, Nathan Painchaud, Pierre-Yves COURAND, Cl√©ment Rambour, Nicolas THOME, Olivier Bernard",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.02"
    },
    {
        "number": 113,
        "UID": "O-113",
        "forum": "https://openreview.net/forum?id=YPBbNAmGXW",
        "Track": "Full paper",
        "Session": "Oral 2.B: Clinical Diagnostics",
        "Final Decision": "Oral",
        "title": "Imitating Radiological Scrolling: A Glocal-Lobal Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification",
        "abstract": "The rapid increase in the number of Computed Tomography (CT) scan examinations has created an urgent need for automated tools, such as organ segmentation, anomaly classification, and report generation, to assist radiologists with their growing workload.  Multi-label classification of Three-Dimensional (3D) CT scans is a challenging task due to the volumetric nature of the data and the variety of anomalies to be detected. Existing deep learning classification methods, relying on standard Convolutional Neural Networks or Vision Transformers, do not explicitly model the radiologist's navigational behavior while scrolling through CT scan slices. In this study, we present CT-Scroll, a novel glocal-local attention model specifically designed to emulate the scrolling behavior of radiologists during the analysis of 3D CT scans. Our approach is evaluated on two public datasets, demonstrating its efficacy through comprehensive experiments and an ablation study that highlights the contribution of each model component.",
        "authors": "Th√©o Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel",
        "Time": "Day 2 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.03"
    },
    {
        "number": 113,
        "UID": "F-113",
        "forum": "https://openreview.net/forum?id=YPBbNAmGXW",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Imitating Radiological Scrolling: A Glocal-Lobal Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification",
        "abstract": "The rapid increase in the number of Computed Tomography (CT) scan examinations has created an urgent need for automated tools, such as organ segmentation, anomaly classification, and report generation, to assist radiologists with their growing workload.  Multi-label classification of Three-Dimensional (3D) CT scans is a challenging task due to the volumetric nature of the data and the variety of anomalies to be detected. Existing deep learning classification methods, relying on standard Convolutional Neural Networks or Vision Transformers, do not explicitly model the radiologist's navigational behavior while scrolling through CT scan slices. In this study, we present CT-Scroll, a novel glocal-local attention model specifically designed to emulate the scrolling behavior of radiologists during the analysis of 3D CT scans. Our approach is evaluated on two public datasets, demonstrating its efficacy through comprehensive experiments and an ablation study that highlights the contribution of each model component.",
        "authors": "Th√©o Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.03"
    },
    {
        "number": 115,
        "UID": "O-115",
        "forum": "https://openreview.net/forum?id=YmUDkDQhCW",
        "Track": "Full paper",
        "Session": "Oral 2.B: Clinical Diagnostics",
        "Final Decision": "Oral",
        "title": "Improving brain disorder diagnosis with advanced brain function representation and Kolmogorov-Arnold Networks",
        "abstract": "Quantifying functional connectivity (FC), a vital metric for the diagnosis of various brain disorders traditionally relies on the use of a pre-defined brain atlas. However, using such atlases can lead to issues regarding selection bias and lack of regard for specificity. Ad-\ndressing this, we propose a novel transformer-based classification network (AFBR-KAN) with effective brain function representation, to aid in diagnosing autism spectrum disorder (ASD). AFBR-KAN leverages Kolmogorov-Arnold Network (KAN) blocks replacing traditional multi-layer perceptron (MLP) components. Thorough experimentation reveals the effectiveness of AFBR-KAN in improving the diagnosis of ASD under various configurations of the model architecture.",
        "authors": "Tyler Ward, Abdullah Al Zubaer Imran",
        "Time": "Day 2 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.04"
    },
    {
        "number": 115,
        "UID": "F-115",
        "forum": "https://openreview.net/forum?id=YmUDkDQhCW",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Improving brain disorder diagnosis with advanced brain function representation and Kolmogorov-Arnold Networks",
        "abstract": "Quantifying functional connectivity (FC), a vital metric for the diagnosis of various brain disorders traditionally relies on the use of a pre-defined brain atlas. However, using such atlases can lead to issues regarding selection bias and lack of regard for specificity. Ad-\ndressing this, we propose a novel transformer-based classification network (AFBR-KAN) with effective brain function representation, to aid in diagnosing autism spectrum disorder (ASD). AFBR-KAN leverages Kolmogorov-Arnold Network (KAN) blocks replacing traditional multi-layer perceptron (MLP) components. Thorough experimentation reveals the effectiveness of AFBR-KAN in improving the diagnosis of ASD under various configurations of the model architecture.",
        "authors": "Tyler Ward, Abdullah Al Zubaer Imran",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.04"
    },
    {
        "number": 205,
        "UID": "O-205",
        "forum": "https://openreview.net/forum?id=JEN5FzeFZj",
        "Track": "Full paper",
        "Session": "Oral 2.B: Clinical Diagnostics",
        "Final Decision": "Oral",
        "title": "Machine Learning with Scarce Data: Ejection Fraction Prediction Using PLAX View",
        "abstract": "We developed a machine learning model to predict left ventricular ejection fraction (LVEF/EF) from parasternal long-axis (PLAX) echocardiographic videos. Because public datasets with labeled PLAX videos are virtually non-existent, our work focuses on an innovative data generation strategy to overcome this scarcity. By leveraging a time-based correlation between clinical notes and echocardiographic videos, combined with fine-tuning view classifiers and proxy labeling, we effectively created a large labeled PLAX dataset and achieved a mean absolute error (MAE) of 7.15%. Given that Apical four-chamber methods, the clinical standard, report MAE values of 6%-7%, our results demonstrate that EF estimation from PLAX views is both feasible and clinically relevant.  This surpasses the performance of existing methods and provides a clinically useful solution for situations where apical views may not be feasible.  The EF labels for PLAX videos, derived from publicly available datasets, are accessible at https://github.com/Jeffrey4899/PLAX_EF_Labels_202501.",
        "authors": "Zhiyuan Gao, Dominic Yurk, Yaser S. Abu-Mostafa",
        "Time": "Day 2 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.05"
    },
    {
        "number": 205,
        "UID": "F-205",
        "forum": "https://openreview.net/forum?id=JEN5FzeFZj",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Machine Learning with Scarce Data: Ejection Fraction Prediction Using PLAX View",
        "abstract": "We developed a machine learning model to predict left ventricular ejection fraction (LVEF/EF) from parasternal long-axis (PLAX) echocardiographic videos. Because public datasets with labeled PLAX videos are virtually non-existent, our work focuses on an innovative data generation strategy to overcome this scarcity. By leveraging a time-based correlation between clinical notes and echocardiographic videos, combined with fine-tuning view classifiers and proxy labeling, we effectively created a large labeled PLAX dataset and achieved a mean absolute error (MAE) of 7.15%. Given that Apical four-chamber methods, the clinical standard, report MAE values of 6%-7%, our results demonstrate that EF estimation from PLAX views is both feasible and clinically relevant.  This surpasses the performance of existing methods and provides a clinically useful solution for situations where apical views may not be feasible.  The EF labels for PLAX videos, derived from publicly available datasets, are accessible at https://github.com/Jeffrey4899/PLAX_EF_Labels_202501.",
        "authors": "Zhiyuan Gao, Dominic Yurk, Yaser S. Abu-Mostafa",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - O.05"
    },
    {
        "number": 105,
        "UID": "F-105",
        "forum": "https://openreview.net/forum?id=tU3IpPQCEc",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "4D-VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
        "abstract": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
        "authors": "An Zhao, Moucheng Xu, Ahmed H. Shahin, Wim Wuyts, Mark G. Jones, Joseph Jacob, Daniel C. Alexander",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.01"
    },
    {
        "number": 135,
        "UID": "F-135",
        "forum": "https://openreview.net/forum?id=ikfiyBBHAz",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "GAMBAS: Generalised-Hilbert Mamba for Super-resolution of Paediatric Ultra-Low-Field MRI",
        "abstract": "Magnetic resonance imaging (MRI) is critical for neurodevelopmental research, however access to high-field (HF) systems in low- and middle-income countries is severely hindered by their cost. Ultra-low-field (ULF) systems mitigate such issues of access inequality, however their diminished signal-to-noise ratio limits their applicability for research and clinical use. Deep-learning approaches can enhance the quality of scans acquired at lower field strengths at no additional cost. For example, Convolutional neural networks (CNNs) fused with transformer modules have demonstrated a remarkable ability to capture both local information and long-range context. Unfortunately, the quadratic complexity of transformers leads to an undesirable trade-off between long-range sensitivity and local precision. We propose a hybrid CNN and state-space model (SSM) architecture featuring a novel 3D to 1D serialisation (GAMBAS), which learns long-range context without sacrificing spatial precision. We exhibit improved performance compared to other state-of-the- art medical image-to-image translation models. Our code is made publicly available at https://github.com/levente-1/GAMBAS.",
        "authors": "Levente Baljer, Ula Briski, Robert Leech, Niall J Bourke, Kirsten A Donald, Layla E Bradford, Simone R Williams, Sadia Parkar, Sidra Kaleem, Salman Osmani, Sean CL Deoni, Steven CR Williams, Rosalyn J Moran, Emma C. Robinson, Franti≈°ek V√°≈°a",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.02"
    },
    {
        "number": 164,
        "UID": "F-164",
        "forum": "https://openreview.net/forum?id=p6AJA6PjUI",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Clinical Measurements with Calibrated Instance-Dependent Confidence Interval",
        "abstract": "Reporting meaningful confidence intervals for the predictions of a regression neural network is critical in medical imaging applications since clinical decisions are based on network predictions. We expect to ob-\ntain larger intervals for difficult examples and smaller ones for easier examples to predict. A recently proposed calibration procedure suggests predicting the mean and the variance and scaling the variance on a validation set. Another calibration approach is based on applying conformal prediction to quantile regression. We show that assuming a Gaussian distribution to predict the variance followed by a non-parametric Con-\nformal Prediction technique to scale the estimated variance is the most effective way of achieving a small confidence interval with a coverage guarantee. We report extensive experimental results on various medical\nimaging datasets and network architectures.",
        "authors": "Rotem Nizhar, Lior Frenkel, Jacob Goldberger",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.03"
    },
    {
        "number": 166,
        "UID": "F-166",
        "forum": "https://openreview.net/forum?id=32h0bBn6YZ",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "LiFE-Net: Longitudinal information Fusion for Enhanced lesion detection in unsupervised learning contexts",
        "abstract": "Accurate detection of liver lesions in longitudinal follow-up is critical for assessing disease progression. Unlike clinical practices that compare multiple time points, most deep-learning approaches treat these time points independently. Existing longitudinal imaging methods, particularly in brain imaging, use strategies like channel-wise concatenation, recurrent architectures, or temporal difference computation. However, these methods might fall short in liver imaging due to challenges like non-rigid motions, anatomical variability, and changes in imaging conditions.\n\nTo address these challenges, we introduce LiFE-Net, the first framework to integrate longitudinal information from baseline liver CT scans through feature fusion. Our method employs intermediate feature fusion via self-attention mechanisms, leveraging baseline images to incorporate longitudinal information for more accurate predictions. We adopt an unsupervised training approach using synthetic lesions to address the lack of supervised datasets for longitudinal liver tumors.\n\nOur results show improvements in detection performance on follow-up images when baseline information is incorporated, with gains in both detection mAP and ROC AUC per exam metrics. \nAn exhaustive ablation study further highlights the impact of baseline image integration, registration quality, and architectural components in achieving these improvements. Our code for LiFE-Net is made publicly available at: https://github.com/walid-yassine/LiFE-Net",
        "authors": "Walid Yassine, Martin Charachon, CELINE HUDELOT, Roberto Ardon",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.04"
    },
    {
        "number": 180,
        "UID": "F-180",
        "forum": "https://openreview.net/forum?id=44K3Ep53Ub",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Understanding the Impact of Client Heterogeneity on Ordinal Classification in Federated Medical Image Analysis",
        "abstract": "Deep learning methods have shown remarkable success in medical image classification, aiding in early disease detection and treatment. Many of these tasks, such as cancer staging or risk stratification, exhibit an inherent ordinal structure; however, existing solutions often reduce them to binary or purely nominal classifications, ignoring the valuable ordering information. Simultaneously, privacy and regulatory concerns have spurred the adoption of Federated Learning (FL), enabling collaborative model training without centralising sensitive patient data. Yet, FL in real-world medical scenarios faces significant challenges arising from heterogeneous client data, particularly when institutions differ widely in case severity or label distribution. In this work, we conduct the first in-depth study of Federated Ordinal Learning (FOL), introducing ordinal classification paradigms into FL pipelines and systematically evaluating their performance under increasing levels of data heterogeneity. We assess the benefits of ordinal classification within four FL frameworks: standard Federated Averaging (FedAvg) and three heterogeneity-focused approaches (FedProx, MOON, and FedALA). \nOur experiments reveal that ordinal methods can effectively maintain class ordering information even when institutional data exhibit severe imbalance or missing classes, offering valuable insights for developing robust, privacy-preserving AI systems in medical imaging. However, ordinal approaches still suffer from performance degradation in highly heterogeneous FL settings, underscoring the need for dedicated research on FL methods that explicitly account for ordinality.",
        "authors": "Valentina Corbetta, Regina Beets-Tan, Jaime S Cardoso, Wilson Silva",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.05"
    },
    {
        "number": 181,
        "UID": "F-181",
        "forum": "https://openreview.net/forum?id=LbupKZUEcg",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Synthetic Data Generated from CT Scans for Patient Pose Assessment",
        "abstract": "An adequate diagnostic quality of radiographs is essential for reliable diagnoses and treatment planning. \nThe patient's pose during radiography is one of the most important factors determining the diagnostic quality. \nSince patient positioning is difficult and not standardized, an automated AI-based approach using depth images to automatically assess the patient's pose before the radiograph has been taken would be helpful.\nDue to regulatory hurdles, however, it is difficult in practice to acquire the required depth images and corresponding radiographs.\nIn this paper, we present a framework that can generate such training data synthetically from Computer Tomography scans. \nWe further show that by pretraining on our generated synthetic dataset consisting of 3077 image pairs of upper ankle joints, the pose assessment of real upper ankle joints can be improved by up to 11 percentage points.",
        "authors": "Manuel Laufer, Dominik Mairh√∂fer, Malte Sieren, Hauke Gerdes, Fabio Leal dos Reis, Arpad Bischof, Thomas K√§ster, Erhardt Barth, J√∂rg Barkhausen, Thomas Martinetz",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.06"
    },
    {
        "number": 191,
        "UID": "F-191",
        "forum": "https://openreview.net/forum?id=aMIGZcZtd1",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Enhancing Post-Treatment Visual Acuity Prediction with Multimodal Deep Learning on Small-scale Clinical and OCT Datasets",
        "abstract": "Predicting visual acuity (VA) outcomes after treatment in diabetic macular edema (DME) is crucial for optimizing patient management but remains challenging due to the heterogeneity of patient responses and the limited availability of comprehensive datasets. While existing predictive models have shown promise, their clinical deployment is hindered by their reliance on large training datasets that are often unavailable in real-world settings. We address this challenge by developing a multimodal deep learning framework specifically designed for small-scale clinical cohorts. Our approach integrates optical coherence tomography (OCT) images with carefully selected clinical parameters through a cross-modal fusion architecture that leverages attention mechanisms to enhance feature interaction and predictive accuracy. We validate our framework across two clinically distinct real-world cohorts: treatment-na√Øve patients ($n=35$) receiving intensive anti-VEGF therapy and chronically treated patients ($n=20$) receiving sustained-release corticosteroid implants. This approach achieves mean absolute errors in post-treatment VA prediction of $3.07 \\pm 0.82$ and $4.20 \\pm 2.79$ Early Treatment Diabetic Retinopathy Study (ETDRS) letters, respectively, falling within the acceptable range of clinical measurement variability and meeting thresholds for statistically significant visual change detection with $\\geq90\\%$ confidence. This work demonstrates that appropriately designed multimodal architectures can achieve clinically meaningful prediction accuracy even with limited datasets, offering a practical foundation for personalized DME management in typical clinical settings where large datasets are unavailable.",
        "authors": "Matthew Anderson, Veronica Corona, Agnieszka Stankiewicz, Maged Habib, David H. Steel, Boguslaw Obara",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.07"
    },
    {
        "number": 193,
        "UID": "F-193",
        "forum": "https://openreview.net/forum?id=Xk69ksan04",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Intelligent Lesion Selection: A Novel Method for Longitudinal Assessment of Breast Cancer Lung Metastases",
        "abstract": "Breast cancer, the second most common cancer globally, often metastasizes to the lungs, requiring frequent computed tomography (CT) scans to monitor disease progression. Manual analysis by radiologists is time-consuming and prone to variability, underscoring the need for automated systems to enhance accuracy and efficiency. The goal of such systems is to optimize processes like RECIST score calculation for tumor response assessment. This study presents a pipeline for the automated temporal analysis of breast cancer lung metastases. Existing lung nodule detection and segmentation models were adapted for detecting and segmenting breast cancer metastases. Registration-based lesion tracking was incorporated, and a novel Temporal Lesion Pair Classifier was developed to identify significant lesions and estimate tumor load evolution by summing their diameters, following an adaptation of the RECIST guidelines. Evaluated on a unique dataset of breast cancer patients, each with multiple annotated CT scans at different disease stages, the proposed pipeline demonstrated a 42% reduction in median tumor size progression discrepancy for consecutive study pairs and improved tumor response classification accuracy by 22% at the patient level.",
        "authors": "Melika Qahqaie, Veronika A Zimmer, Eduardo Castaneda, Katariina Peltonen, Joonas Laitinen, Juho L√§hteenmaa, Tobias Heimann, Andreas Maier, Dominik Neumann",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.08"
    },
    {
        "number": 200,
        "UID": "F-200",
        "forum": "https://openreview.net/forum?id=jwYtjfDvf6",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Predicting Prostate Cancer Progression During Active Surveillance Using Longitudinal bpMRI Scans and A Multi-scale Foundation Model",
        "abstract": "Active Surveillance (AS) is the recommended management strategy for patients with low- or intermediate-risk Prostate Cancer (PCa), providing a safe alternative that helps avoid the adverse effects of overtreatment. While artificial intelligence (AI)-based models for PCa detection have been extensively studied, their application in AS remains challenging, with limited research addressing the detection of PCa progression in AS scenarios. In this study, we present a novel framework for predicting PCa progression within AS protocols using bi-parametric MRI (bpMRI). Due to the limited availability of longitudinal bpMRI scans (206 patients in our study), we first developed a multi-scale foundation model trained on a large cohort of single-year bpMRI scans, comprising 5,162 patients from 10 different institutions. Building on this foundation, we designed a three-module framework: (1) a lesion detection module to identify PCa lesions in full bpMRI scans, (2) a lesion classification module to perform detailed analysis of the identified lesion regions, and (3) a multi-scan lesion progression prediction module to assess changes in lesions over time using longitudinal bpMRI patches. The proposed framework was evaluated on a cohort from an AS clinical trial and demonstrated significant performance improvements over baseline models and radiologists, highlighting its potential to enhance clinical decision-making in AS management.",
        "authors": "Yifan Wang, Bin Lou, Heinrich von Busch, Robert Grimm, Sanoj Punnen, Dorin Comaniciu, Ali Kamen, Henkjan Huisman, Angela Tong, David Winkel, Tobias Penzkofer, Ivan Shabunin, Moon Hyung Choi, Qingsong Yang, Dieter Szolar, Steven Shea, Fergus Coakley, Mukesh Harisinghani",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.09"
    },
    {
        "number": 202,
        "UID": "F-202",
        "forum": "https://openreview.net/forum?id=dAfbmDPeJL",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "A Balancing Act: Optimizing Classification and Retrieval in Cross-Modal Vision Models",
        "abstract": "Despite the promising capabilities of vision-language models (VLMs) across diverse tasks, recent studies reveal that they struggle with the fundamental task of image classification. In this study, we explore leveraging state-of-the-art task-specific classification models as a foundation for VLMs, aiming to preserve strong classification performance. Specifically, we assess the impact of contrastive tuning to enable cross-modal retrieval capabilities on a Hierarchical Image Pyramid Transformer (HIPT) trained for prostate cancer grading in Whole-Slide Images (WSIs) and a ViT-Base model trained for multi-label classification on natural images. Our results demonstrate that contrastive fine-tuning creates a clear trade-off: classification accuracy rapidly deteriorates toward zero as vision-text alignment improves. By balancing the two objectives in the loss function during fine-tuning, we achieve competitive slide-level retrieval performance while maintaining classification accuracy.",
        "authors": "Judith Lefkes, Cl√©ment Grisi, Geert Litjens",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.10"
    },
    {
        "number": 220,
        "UID": "F-220",
        "forum": "https://openreview.net/forum?id=4ZTrVATnS8",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Predicting the Year of Total Knee Replacement: A Transformer-Based Multimodal Approach",
        "abstract": "Accurate prediction of the year of total knee replacement (TKR) is challenging due to\nthe complex interplay of factors influencing the surgical decision. Current deep learning\nmodels often rely on single-modality data, limiting their predictive power. Multimodal\napproaches integrating imaging and patient data offer the potential to improve predictions\nand support clinical decisions. This study presents an end-to-end trained, transformer-\nbased multimodal model that integrates MR imaging with tabular data, including clinical\nvariables and image readings, to predict the year of TKR for each subject. Our model lever-\nages cross-modal attention to fuse features from an image encoder with a self-supervised\npretrained tabular encoder, achieving the highest accuracy of 63.4% among tested mod-\nels. We evaluated its performance against three unimodal models and four multimodal\nfusion strategies, including simple concatenation, DAFT, and multimodal interaction. The\nresults demonstrate that our model‚Äôs cross-modal interaction approach with pretrained\nTabNet not only outperformed all unimodal models but also showed improvements over\nother multimodal fusion techniques, highlighting the effectiveness of cross-modal attention\nfusion for integrating complex data modalities in TKR year prediction tasks. Source code\nis available at https://github.com/denizlab/2025_MIDL_time2TKR.",
        "authors": "Ozkan Cigdem, Refik Soyak, Kyunghyun Cho, Cem M Deniz",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.11"
    },
    {
        "number": 237,
        "UID": "F-237",
        "forum": "https://openreview.net/forum?id=TAPn4QjbOg",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Surgical Flow Masked Autoencoder for Event Recognition",
        "abstract": "Recognition and forecasting of surgical events from video sequences are crucial for advancing computer-assisted surgery. Surgical events are often characterized by specific tool-tissue interactions; for example, \"bleeding damage\" occurs when a tool unintentionally cuts a tissue, leading to blood flow. Despite progress in general event classification, recognizing and forecasting events in medical contexts remains challenging due to data scarcity and the complexity of these events. \n\nTo address these challenges, we propose a method utilizing video masked autoencoders (VideoMAE) for surgical event recognition. This approach focuses the network on the most informative areas of the video while minimizing the need for extensive annotations. We introduce a novel mask sampling technique based on an estimated prior probability map derived from optical flow. We hypothesize that leveraging prior knowledge of tool-tissue interactions will enable the network to concentrate on the most relevant regions in the video.\n\nWe propose two methods for estimating the prior probability map: (a) retaining areas with the fastest motion and (b) incorporating an additional encoding pathway for optical flow. Our extensive experiments on the public dataset CATARACTS and our in-house neurosurgical data demonstrate that optical flow-based masking consistently outperforms random masking strategies of VideoMAE in phase and event classification tasks. We find that an optical flow encoder enhances classification accuracy by directing the network's focus to the most relevant information, even in regions without rapid motion.\n\nFinally, we investigate sequential and multi-task training strategies to identify the best-performing model, which surpasses the current state-of-the-art by 5\\% on the CATARACTS dataset and 27\\% on our in-house neurosurgical data.",
        "authors": "Mayar Lotfy Mostafa, Anna Alperovich, Dmitrii Fedotov, Ghazal Ghazaei, Stefan Saur, Azade Farshad, Nassir Navab",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.12"
    },
    {
        "number": 240,
        "UID": "F-240",
        "forum": "https://openreview.net/forum?id=cyHmr0DIjM",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Contrastive Patient-level Pretraining Enables Longitudinal and Multimodal Fusion for Lung Cancer Risk Prediction",
        "abstract": "Leveraging longitudinal and multimodal data is important for clinical predictive tasks. Contrastive language-image pretraining (CLIP) has been successful in learning multimodal representations by aligning paired images and captions, i.e. medical images and corresponding radiology report. However, in real clinical settings, the alignment of unpaired modalities, such as medical images and clinical notes collected at different times, is an open challenge, even though such data are ubiquitous in practice. This study conducts contrastive pretraining between longitudinal chest CTs and clinical variables on the patient level using a large public lung cancer screening dataset. Leveraging a time-distanced transformer to encode longitudinal imaging and an open-source text embedding to encode clinical variables, we optimize contrastive loss between the embedded modalities from same patient (positive pair) against those from different patients (negative pair). We find that finetuning the CLIP representation significantly improves prediction of lung cancer risk in two types of clinical populations (0.895 and 0.893 AUC) compared to conventional multimodal fusion (0.873 and 0.875 AUC) and single modality baselines. These results demonstrate how contrastive patient-level pretraining can enable longitudinal and multimodal fusion without additional training data. We released our code and pre-trained weights at https://github.com/MASILab/lung-cplp.",
        "authors": "Thomas Li, Lianrui Zuo, Yihao Liu, Aravind Krishnan, Kim L. Sandler, Thomas A Lasko, Fabien Maldonado, Bennett Allan Landman",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.13"
    },
    {
        "number": 256,
        "UID": "F-256",
        "forum": "https://openreview.net/forum?id=UZiIFs95yK",
        "Track": "Full paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Federated Class-Heterogeneous Report Labeling with Surgical Aggregation",
        "abstract": "Labeling radiology reports is essential for creating medical imaging datasets and enabling AI-driven clinical decision support. While SBERT-based classifiers offer computationally efficient solutions for this task, a major challenge is the class heterogeneity across datasets, as different groups focus on extracting distinct disease labels. For instance, NIH and CheXpert CXR datasets share only 7 of their 14 and 13 labels, respectively. To address this, we propose to use Surgical Aggregation, a class-heterogeneous federated learning framework that collaboratively trains a global multi-label classifier without requiring alignment of labeling schemes across clients. Surgical Aggregation selectively merges shared class weights while appending new disease-specific nodes, thereby unifying distinct local labeling priorities, to dynamically incorporate all disease labels of interest. We evaluated Surgical Aggregation in multiple simulated settings with varying number of participating nodes as well as different degrees of overlapping labels. Our results demonstrate high performance confirming adaptability in class-heterogeneous environments, thereby offering a scalable and privacy-preserving solution for collaborative medical report labeling. Our code is available at https://github.com/BioIntelligence-Lab/Federated-MedEmbedX",
        "authors": "Nikhil Shah, Pranav Kulkarni, Florence Doo, Ang Li, Michael A. Jacobs, Vishwa Sanjay Parekh",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - F.14"
    },
    {
        "number": 16,
        "UID": "S-16",
        "forum": "https://openreview.net/forum?id=tbjpQbcIxW",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "A Low-Resource Training Strategy for Cell Segmentation using Patch-Based Attention U-Net",
        "abstract": "Segmentation is an essential tool for cell biologists and involves isolating cells or cellular features from microscopy images. An automated segmentation pipeline with high precision and accuracy can significantly reduce manual labor and subjectivity. Frequently, researchers would seek for a validated model available online and fine-tune it to meet their segmentation requirements. However, the established fine-tuning approach may involve online training or computationally intensive offline training. To address this, we propose an offline training pipeline requiring only tens of samples that are morphologically distinct from pre-training data. Specifically, we employed a patch-based attention U-Net trained with a threshold-based custom loss function. Finally, we evaluated this workflow along with two other state-of-the-art models, Stardist and Cellpose, on three different tasks. Our method improves image segmentation performance by 32.60\\% and 35.62\\% over Stardist and Cellpose, respectively, using the same amount of training samples.",
        "authors": "Kai-Lin Chen, Yu-Nong Lin, Pen-hsiu Grace Chao, Kevin T. Chen",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.01"
    },
    {
        "number": 22,
        "UID": "S-22",
        "forum": "https://openreview.net/forum?id=jhsi5JDjcj",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Gradual modality dropout for segmenting ischemic stroke lesions in an unseen center with missing modalities",
        "abstract": "In clinical practice, imaging modalities may not always be available for every patient due to scheduling, cost, or patient-specific constraints. Additionally, multi-center imaging studies often face inconsistencies in protocols, machine settings, and artifacts, compromising data quality. We propose a 3D U-Net model for ischemic lesion segmentation using a novel training technique, gradual modality dropout, which progressively deactivates imaging modalities during training. This approach ensures robust performances when all modalities are present and improves segmentation accuracy in scenarios where one or more modalities are missing in unfamiliar contexts. The model demonstrates adaptability and reliability when trained on MRI scans of stroke patients across different phases (hyper-acute,sub-acute, acute, and post-treatment) and various hospital settings. Code available here: https://github.com/sofiavarib/Gradual-modality-dropout",
        "authors": "Sofia Vargas Ibarra, Vincent Martin VIGNERON, Hichem MAAREF, Sonia Garcia-Salicetti, Andreia V Faria",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.02"
    },
    {
        "number": 24,
        "UID": "S-24",
        "forum": "https://openreview.net/forum?id=NwfOhbGajF",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "ProstateZones: Segmentations of the prostatic zones and urethra for the PROSTATEx dataset",
        "abstract": "Manual segmentations are considered the gold standard for training and evaluating machine learning models in medical imaging, although difficult to obtain due to the time-consuming and labor-intensive nature of the task. We present a curated dataset of manual segmentations of the prostatic zones and intraprostatic urethra for 200 patients from the publicly available PROSTATEx dataset. For 40 patients, independent duplicate segmentations are included to provide inter-reader variability data, resulting in 240 total segmentations. The terminology follows the PI-RADS v2.1 guidelines, ensuring consistency and clinical relevance. This dataset fills a critical gap by offering a publicly available resource for training, benchmarking, and external validation of prostate MRI segmentation models.",
        "authors": "William Holmlund, Attila Simk√≥, Karin S√∂derkvist, P√©ter Pal√°sti, Szilvia T√≥tin, Kamilla Kalm√°r, Zs√≥fia Domoki, Zsuzsanna Fejes, Zsigmond Tam√°s Kincses, Patrik Brynolfsson, Tufve Nyholm",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.03"
    },
    {
        "number": 38,
        "UID": "S-38",
        "forum": "https://openreview.net/forum?id=DTYFRzRPQn",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Toward Interpretable 3D Diffusion in Radiology: Token-Wise Attribution for Text-to-CT Synthesis",
        "abstract": "Diffusion-based generative models have emerged as powerful tools for synthesizing anatomically realistic computed tomography (CT) scans from free-text prompts but remain opaque when delineating token influence on the conditioned CT volume. This lack of interpretability limits their clinical applicability, trustworthiness, and adoption across diagnostic and decision-support scenarios. We present a token-wise voxel attribution method for 3D text-to-image diffusion models that leverages cross-attention in U-Net‚Äìbased architectures to extract individual token attention maps for synthetic CT scans. Our method visualizes individual, joint, or aggregated token-level voxel attributions during CT synthesis, helping to alleviate concerns about model transparency. This lays the groundwork for practical methods and structured explanations illustrating what aspects of attribution work well, where current limitations lie, and how researchers might approach explainable AI for 3D text-to-image diffusion models in radiology moving forward.",
        "authors": "Aidan Bradshaw, Katelyn Morrison, Arpit Mathur, Weicheng Dai, Motahhare Eslami, kayhan Batmanghelich, Adam Perer",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.04"
    },
    {
        "number": 43,
        "UID": "S-43",
        "forum": "https://openreview.net/forum?id=TU0z4ABShQ",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Attention-based Interpretable Deep Learning with Radiomic Features for Pulmonary Nodule Classification",
        "abstract": "Pulmonary nodule classification is critical for early lung cancer screening, enabling timely intervention and evidence-based clinical decision-making. In this study, we introduce an attention-based interpretable deep learning framework that leverages mathematically predefined, handcrafted features derived from CT imaging for pulmonary nodule classification. In contrast to conventional convolutional neural networks (CNNs) that learn complex and often opaque feature representations, our approach prioritizes transparency and reproducibility by using statistically defined intensity features. The architecture is a lightweight multilayer perceptron (MLP) with channel-wise attention. The model was trained on an in-house dataset and validated on two publicly available external datasets: LUNA (n=1,122) and ISBI (n=220), achieving an area under the receiver operating characteristic curve (AUC) of 0.964 (95% confidence interval [CI]: 0.942‚Äì0.983) and 0.974 (95% CI: 0.964‚Äì0.984), respectively. The integration of channel-wise attention within the MLP architecture enables the model to explicitly learn and assign relative importance to each input feature, supporting feature-level interpretability.",
        "authors": "Doohyun Park, Nahyuk Lee, Sungjoo Lim",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.05"
    },
    {
        "number": 45,
        "UID": "S-45",
        "forum": "https://openreview.net/forum?id=GkfADx4gZp",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "A Deep Learning Framework Integrating Multi-View Morphologic and Hemodynamic Features for Pericardial Disease Classification",
        "abstract": "Pericardial diseases require accurate and timely diagnosis, yet echocardiography analysis typically depends on expert interpretation. In this study, we introduce a novel two-stage deep learning framework designed to improve diagnostic accuracy by integrating multi-view echocardiographic video data‚Äî PLAX (parasternal long-axis), A4C (apical 4-chamber), modified A4C (modified apical 4-chamber), S4C (subcostal 4-chamber)‚Äî with hemodynamic indicators derived from IVC (inferior vena cava). In Stage 1, a tailored spatiotemporal convolutional neural network (CNN) effectively captures dynamic cardiac patterns, enabling precise classification of pericardial effusion severity and pericardial thickening (accuracy 0.921). In stage 2, embedding and integration of IVC-derived hemodynamic features substantially enhance sensitivity for detecting clinically significant cases (positive 0.969, negative 0.618). Our findings highlinght the clinical benefit of combining spatiotemporal echocardiographic features with functional indicators, potentially reducing reliance on subjective interpretation while ensuring compatibility with existing clinical workflows.",
        "authors": "Sihyeon Jeong, Jina Lee, Yeonggul Jang, Jaeik Jeon, Dawun Jeong, In Tae Moon, Seung-Ah Lee, Yeonyee E. Yoon, Hyuk-Jae Chang",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.06"
    },
    {
        "number": 52,
        "UID": "S-52",
        "forum": "https://openreview.net/forum?id=vmDVS5SALB",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Outcome Prediction in Histopathology: A Multimodal Three-Stage Deep Learning Framework",
        "abstract": "Accurate outcome prediction is paramount in histopathology for effective cancer management. We present a novel, high-performance multimodal deep learning framework that efficiently integrates information from whole slide images (WSIs) and, optionally, clinical data to significantly enhance prediction. The first stage achieves precise tumor detection using a custom UNet (ConvNeXtv2 encoder for robust segmentation; decoder with residual connections, bottleneck, and SE blocks). To optimize training and generalization, we introduce a strategic patch selection method that enhances generalization. The second stage efficiently extracts highly informative and compressed feature representations from selected regions using a ResNeXt50 network, pre-trained with DINO. The third stage aggregates these features, combines them with clinical parameters (if available), and predicts outcomes via ResNet18. Critically, the framework leverages a multimodal approach, combining WSI image features with clinical parameters for robust outcome prediction. The framework's efficacy is rigorously demonstrated through experiments on metastasis prediction (prostate cancer WSIs) and BRCA2 mutation prediction (multiple sites). Comparative evaluation against Multiple Instance Learning (MIL) approaches highlights superior performance and effective multimodal data utilization.",
        "authors": "NILANJAN CHATTOPADHYAY, Saheli Datta, Nitin Singhal",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.07"
    },
    {
        "number": 55,
        "UID": "S-55",
        "forum": "https://openreview.net/forum?id=zQ5NznBPd2",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Accurate Brain Age Prediction from MRI: Evaluating Kolmogorov-Arnold and Convolutional Networks",
        "abstract": "Brain age prediction using T1-weighted MRI has become a key biomarker for assessing\nneurological health, with application in studying neurodegeneration (Soumya Kumari and\nSundarrajan, 2024; Mishra et al., 2023; Lea et al., 2021) and brain development (Tanveer\net al., 2023). While convolutional neural networks (CNNs) remain a standard approach,\nrecent advances suggest that Kolmogorov-Arnold Networks (KANs) may offer superior\nperformance in image-based task (Bodner et al., 2025; Li et al., 2024). In this study, we\npresent the first use of KANs for brain age prediction from 3D MRI scans, comparing their\nperformance against traditional CNNs. Experimental results show that KAN-based models\nreduce estimation errors, highlighting their potential for improving brain age assessment.",
        "authors": "Alessandro Giupponi, Davide De Crescenzo, Marco Pinamonti, Manuela Moretto, Alessandra Bertoldo, Mattia Veronese, Marco Castellaro",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.08"
    },
    {
        "number": 66,
        "UID": "S-66",
        "forum": "https://openreview.net/forum?id=RN42GJ9Pjn",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Predicting Cutaneous Squamous Cell Carcinoma Progression Risk from Whole Slide Images with Federated Learning",
        "abstract": "Cutaneous squamous cell carcinoma (cSCC) is the second most common cancer globally. While surgical excision is typically successful, a significant proportion of patients experience disease progression leading to poor prognosis. Based on the fact that histopathological tumor features have been associated with increased risk of cSCC progression, we propose to predict this condition solely from Whole Slide Image (WSIs) scans of excised tumors. A major challenge in developing such predictive models is the fact that numerous clinical centers maintain patient cohorts that are often too small individually for robust deep learning (DL) applications. Here we use four small to medium-sized datasets from different clinical centers across Germany and demonstrate the feasibility of training federated DL models to predict cSCC progression. We compare various Federated Learning (FL) approaches, leveraging distributed datasets and developing center-specific models.",
        "authors": "Jakub Zacharczuk, Juan Ignacio Pisula, Doris Helbig, Lucas Sanc√©r√©, Oana-Diana Persa, Corinna B√ºrger, Anne Fr√∂hlich, Carina Lorenz, Sandra Bingmann, Dennis Niebel, Konstantin Drexler, Jennifer Landsberg, Roman Thomas, Johannes Br√§gelmann, Katarzyna Bozek",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.09"
    },
    {
        "number": 92,
        "UID": "S-92",
        "forum": "https://openreview.net/forum?id=k8Og4aWYo6",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Region-specific cardiovascular risk prediction from non-contrast chest computed tomography",
        "abstract": "Accurate cardiovascular risk scores can help direct preventive treatment to those who would maximally benefit. Current scores rely on established risk factors, but imaging may contain additional information to find high-risk patients. Here, we developed and tested a system called CT-CV-Risk to estimate cardiovascular risk from non-contrast chest CT images. We find that CT-CV-Risk predicts risk complementary to established clinical risk scores.",
        "authors": "Vineet K Raghu, Daniel W Oo, Leonard N√ºrnberg, Audra Sturniolo, Douglas P Kiel, Hugo Aerts, Pradeep Natarajan, Michael T Lu",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.10"
    },
    {
        "number": 99,
        "UID": "S-99",
        "forum": "https://openreview.net/forum?id=8uFVHA03Dy",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Robust Amyloidosis Subtype Classification via Multisequence CMR Fusion with Spatiotemporal Learning",
        "abstract": "Cardiac amyloidosis (CA) subtype classification remains a critical diagnostic challenge. We propose a multimodal deep learning framework that integrates cine, late gadolinium enhancement (LGE), and T1/T2 parametric cardiac MRI sequences to differentiate light chain (AL) and transthyretin (ATTR) amyloidosis. The model employs sequence-specific encoders and gated attention fusion, enabling robust performance even with missing input sequences. Evaluated on 123 patients with cross-validation, the xLSTM-based model achieved the highest AUC (0.8506), outperforming a Video Swin Transformer (VST) alternative. Grad-CAM visualizations highlight both cardiac and extracardiac regions, demonstrating interpretability and the potential for identifying systemic imaging biomarkers. These results support a clinically viable approach to non-invasive CA subtype diagnosis.",
        "authors": "Parker Martin, Yifan Liu, Karolina Zareba, Suzanne Smart, Akash Goyal, Orlando Simonetti, Jeremy Slivnick, Yuan Xue",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.11"
    },
    {
        "number": 119,
        "UID": "S-119",
        "forum": "https://openreview.net/forum?id=sw3u1WPnW8",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "FusionScan: A Novel AI-Based Multi-Modal Imaging Technique for Enhanced Medical Diagnostics, Inspired by Stanford's Mini-Fellowship Program in Molecular Imaging",
        "abstract": "Modern medical diagnostics rely heavily on imaging technologies, each characterized by unique advantages and inherent limitations. Inspired by the insights from Stanford's Mini-Fellowship Program in Molecular Imaging Techniques, this research proposes FusionScan, an innovative technique that integrates Magnetic Resonance Imaging (MRI) and X-Ray Computed Tomography (CT) as imaging modalities along with an AI-based system. The research demonstrates how the fused system can provide unparalleled depth, resolution, and functional data by synthesizing the strengths of these two techniques, offering a robust solution for complex medical challenges. The goal is to fuse the images of the two modalities and analyze them using AI models, hence enhancing the resolution and specificity of the molecular images. Simulation results demonstrate the value of AI in enhancing medical images.",
        "authors": "Gurnoor Singh Dang, Majid Rodgar, Michael Snyder",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.12"
    },
    {
        "number": 123,
        "UID": "S-123",
        "forum": "https://openreview.net/forum?id=LxqBytSVpH",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "Low-Rank Adaptation with Swin Transformers to Enhance Skin Cancer Diagnosis",
        "abstract": "Skin cancer is one of the most common forms of cancer worldwide. Automated diagnosis using deep learning has shown promise, but high-performing models like Vision Transformers are often computationally expensive. Swin Transformers are less computationally expensive than ViTs because they use a hierarchical structure with shifted windows for self-attention, limiting computations to local regions instead of the entire image. We propose a Parameter Efficient Fine-tuning (PEFT) method integrating Low-Rank Adaptation (LoRA) into Swin Transformers to reduce model training and inference computational complexity while maintaining high diagnostic performance. Experiments on the standard HAM10000 skin cancer dataset demonstrate the proposed model's effectiveness in skin lesion classification with improved efficiency.",
        "authors": "Prasanth Yadla",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.13"
    },
    {
        "number": 128,
        "UID": "S-128",
        "forum": "https://openreview.net/forum?id=dgusajTAqF",
        "Track": "Short paper",
        "Session": "Poster 2.B: Clinical Diagnostics",
        "Final Decision": "Poster",
        "title": "FemoraLyze: A Modular Framework for Proximal Femur Analysis",
        "abstract": "The proximal femur is exposed to an increased risk of fracture, particularly in the context of osteoporosis. As the prevalence increases with age, it is expected that the incidence of osteoporotic fractures will likely continue to rise in terms of demographic trends. \nEarly, guideline-based therapy offers strong prospects of success, but requires precise and reliable diagnostic and prognostic procedures. Automated bone metrics are suitable for this purpose and could also be used for other applications such as preoperative planning of total hip arthroplasty in patients affected by arthritis.\nIn this paper we present FemoraLyze, which is a modular deep-learning-based Python framework that combines the automated and differentiated calculation of segmentation masks, bone structure and geometry parameters of the proximal femur based on a computed tomography (CT) image. The code will be released upon acceptance.",
        "authors": "Marten Johannes Finck, Niklas Christoph Koser, Jan-Bernd H√∂vener, Claus-C. Gl√ºer, Soren Pirk",
        "Time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 2 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 2.B - S.14"
    },
    {
        "number": 74,
        "UID": "O-74",
        "forum": "https://openreview.net/forum?id=OkAyRZDO2p",
        "Track": "Full paper",
        "Session": "Oral 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Oral",
        "title": "Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields.",
        "abstract": "Longitudinal image registration enables studying temporal changes in brain morphology which is useful in applications where monitoring the growth or atrophy of specific structures is important. However this task is challenging due to; noise/artifacts in the data and quantifying small anatomical changes between sequential scans. We propose a novel longitudinal registration method that models structural changes using temporally parameterized neural displacement fields. Specifically, we implement an implicit neural representation (INR) using a multi-layer perceptron that serves as a continuous coordinate-based approximation of the deformation field at any time point. In effect, for any $N$ scans of a particular subject, our model takes as input a 3D spatial coordinate location $x, y, z$ and a corresponding temporal representation $t$ and learns to describe the continuous morphology of structures for both observed and unobserved points in time. Furthermore, we leverage the analytic derivatives of the INR to derive a new regularization function that enforces monotonic rate of change in the trajectory of the voxels, which is shown to provide more biologically plausible patterns. We demonstrate the effectiveness of our method on 4D brain MR registration. Our code is publicly available  here https://github.com/aisha-lawal/inrmorph",
        "authors": "Aisha L. Shuaibu, Kieran A. Gibb, Peter A. Wijeratne, Ivor J A Simpson",
        "Time": "Day 3 \u2014 9:00am-10:15am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.01"
    },
    {
        "number": 74,
        "UID": "F-74",
        "forum": "https://openreview.net/forum?id=OkAyRZDO2p",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields.",
        "abstract": "Longitudinal image registration enables studying temporal changes in brain morphology which is useful in applications where monitoring the growth or atrophy of specific structures is important. However this task is challenging due to; noise/artifacts in the data and quantifying small anatomical changes between sequential scans. We propose a novel longitudinal registration method that models structural changes using temporally parameterized neural displacement fields. Specifically, we implement an implicit neural representation (INR) using a multi-layer perceptron that serves as a continuous coordinate-based approximation of the deformation field at any time point. In effect, for any $N$ scans of a particular subject, our model takes as input a 3D spatial coordinate location $x, y, z$ and a corresponding temporal representation $t$ and learns to describe the continuous morphology of structures for both observed and unobserved points in time. Furthermore, we leverage the analytic derivatives of the INR to derive a new regularization function that enforces monotonic rate of change in the trajectory of the voxels, which is shown to provide more biologically plausible patterns. We demonstrate the effectiveness of our method on 4D brain MR registration. Our code is publicly available  here https://github.com/aisha-lawal/inrmorph",
        "authors": "Aisha L. Shuaibu, Kieran A. Gibb, Peter A. Wijeratne, Ivor J A Simpson",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.01"
    },
    {
        "number": 131,
        "UID": "O-131",
        "forum": "https://openreview.net/forum?id=EyaeQLYCZP",
        "Track": "Full paper",
        "Session": "Oral 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Oral",
        "title": "LOTUS: Latent Outpainting Diffusion Model for Three-Dimensional Ultrasound Stitching",
        "abstract": "3D ultrasound (3DUS) stitching can enlarge the field-of-view (FOV) by registering partially overlapping 3DUS images collected from different probe positions. However, standard registration algorithms frequently encounter difficulties with this task, primarily due to the sector-shaped FOV, which often leads to pronounced local minima, thereby obstructing optimization efforts.\nTo address these limitations, we propose LOTUS, a novel Latent Diffusion Model (LDM) specifically designed for 3DUS FOV outpainting. LOTUS innovatively encodes the 3DUS data into a compact latent space and performs outpainting at test time, effectively extending the sector-shaped FOV into a standard rectangular shape. This transformation facilitates a more robust registration by mitigating the issues of local minima associated with the original FOV shape. Experimental results show that LOTUS significantly improves the accuracy of the registration as well as the efficiency of the outpainting process compared to existing models. The code is available at https://github.com/MedICL-VU/LOTUS.",
        "authors": "Xing Yao, Runxuan Yu, Nick DiSanto, Ehsan Khodapanah Aghdam, Kanyifeechukwu Jane Oguine, Daiwei Lu, Ange Lou, Jiacheng Wang, Dewei Hu, Gabriel A Arenas, Baris Oguz, Alison Marie Pouch, Nadav Schwartz, Brett Byram, Ipek Oguz",
        "Time": "Day 3 \u2014 9:00am-10:15am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.02"
    },
    {
        "number": 131,
        "UID": "F-131",
        "forum": "https://openreview.net/forum?id=EyaeQLYCZP",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "LOTUS: Latent Outpainting Diffusion Model for Three-Dimensional Ultrasound Stitching",
        "abstract": "3D ultrasound (3DUS) stitching can enlarge the field-of-view (FOV) by registering partially overlapping 3DUS images collected from different probe positions. However, standard registration algorithms frequently encounter difficulties with this task, primarily due to the sector-shaped FOV, which often leads to pronounced local minima, thereby obstructing optimization efforts.\nTo address these limitations, we propose LOTUS, a novel Latent Diffusion Model (LDM) specifically designed for 3DUS FOV outpainting. LOTUS innovatively encodes the 3DUS data into a compact latent space and performs outpainting at test time, effectively extending the sector-shaped FOV into a standard rectangular shape. This transformation facilitates a more robust registration by mitigating the issues of local minima associated with the original FOV shape. Experimental results show that LOTUS significantly improves the accuracy of the registration as well as the efficiency of the outpainting process compared to existing models. The code is available at https://github.com/MedICL-VU/LOTUS.",
        "authors": "Xing Yao, Runxuan Yu, Nick DiSanto, Ehsan Khodapanah Aghdam, Kanyifeechukwu Jane Oguine, Daiwei Lu, Ange Lou, Jiacheng Wang, Dewei Hu, Gabriel A Arenas, Baris Oguz, Alison Marie Pouch, Nadav Schwartz, Brett Byram, Ipek Oguz",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.02"
    },
    {
        "number": 160,
        "UID": "O-160",
        "forum": "https://openreview.net/forum?id=zEtnTqMORk",
        "Track": "Full paper",
        "Session": "Oral 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Oral",
        "title": "DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention",
        "abstract": "Despite the widespread adoption of transformers in medical applications, the exploration of multi-scale learning through transformers remains limited, while hierarchical representations are thought to be advantageous for medical diagnosis. We propose a novel hierarchical transformer model that adeptly integrates the feature extraction capabilities of Convolutional Neural Networks (CNNs) with the advanced representational potential of Vision Transformers (ViTs). Addressing the lack of inductive biases and dependence on extensive training datasets in ViTs, our model employs a CNN backbone to generate hierarchical visual representations. These representations are adapted for transformer input through an innovative patch tokenization process, preserving the inherited multi-scale inductive biases. We also introduce a scale-wise attention mechanism that directly captures intra-scale and inter-scale associations. This mechanism complements patch-wise attention by enhancing spatial understanding and preserving global perception, which we refer to as local and global attention, respectively. Our model significantly outperforms baseline models in terms of classification accuracy, demonstrating its efficiency in bridging the gap between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). The components are designed as plug-and-play for different CNN architectures and can be adapted for multiple applications. The code is available at \\href{https://github.com/xiaoyatang/DuoFormer.git}{https://github.com/xiaoyatang/DuoFormer.git}.",
        "authors": "Xiaoya Tang, Bodong Zhang, Man M. Ho, Beatrice Knudsen, Tolga Tasdizen",
        "Time": "Day 3 \u2014 9:00am-10:15am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.03"
    },
    {
        "number": 160,
        "UID": "F-160",
        "forum": "https://openreview.net/forum?id=zEtnTqMORk",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "DuoFormer: Leveraging Hierarchical Visual Representations by Local and Global Attention",
        "abstract": "Despite the widespread adoption of transformers in medical applications, the exploration of multi-scale learning through transformers remains limited, while hierarchical representations are thought to be advantageous for medical diagnosis. We propose a novel hierarchical transformer model that adeptly integrates the feature extraction capabilities of Convolutional Neural Networks (CNNs) with the advanced representational potential of Vision Transformers (ViTs). Addressing the lack of inductive biases and dependence on extensive training datasets in ViTs, our model employs a CNN backbone to generate hierarchical visual representations. These representations are adapted for transformer input through an innovative patch tokenization process, preserving the inherited multi-scale inductive biases. We also introduce a scale-wise attention mechanism that directly captures intra-scale and inter-scale associations. This mechanism complements patch-wise attention by enhancing spatial understanding and preserving global perception, which we refer to as local and global attention, respectively. Our model significantly outperforms baseline models in terms of classification accuracy, demonstrating its efficiency in bridging the gap between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). The components are designed as plug-and-play for different CNN architectures and can be adapted for multiple applications. The code is available at \\href{https://github.com/xiaoyatang/DuoFormer.git}{https://github.com/xiaoyatang/DuoFormer.git}.",
        "authors": "Xiaoya Tang, Bodong Zhang, Man M. Ho, Beatrice Knudsen, Tolga Tasdizen",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.03"
    },
    {
        "number": 186,
        "UID": "O-186",
        "forum": "https://openreview.net/forum?id=EWGV97ESaP",
        "Track": "Full paper",
        "Session": "Oral 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Oral",
        "title": "Segment Anything for Histopathology",
        "abstract": "Nucleus segmentation is an important analysis task in digital pathology. However, methods for automatic segmentation often struggle with new data from a different distribution, requiring users to manually annotate nuclei and retrain data-specific models. Vision foundation models (VFMs), such as the Segment Anything Model (SAM), offer a more robust alternative for automatic and interactive segmentation. Despite their success in natural images, a foundation model for nucleus segmentation in histopathology is still missing. Initial efforts to adapt SAM have shown some success, but did not yet introduce a comprehensive model for diverse segmentation tasks. To close this gap, we introduce PathoSAM, a VFM for nucleus segmentation, based on training SAM on a diverse dataset. Our extensive experiments show that it is the new state-of-the-art model for automatic and interactive nucleus instance segmentation in histopathology. We also demonstrate how it can be adapted for other segmentation tasks, including semantic nucleus segmentation. For this task, we show that it yields results better than popular methods, while not yet beating the state-of-the-art, CellViT. Our models are open-source and compatible with popular tools for data annotation. We also provide scripts for whole-slide image segmentation.",
        "authors": "Titus Griebel, Anwai Archit, Constantin Pape",
        "Time": "Day 3 \u2014 9:00am-10:15am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.04"
    },
    {
        "number": 186,
        "UID": "F-186",
        "forum": "https://openreview.net/forum?id=EWGV97ESaP",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Segment Anything for Histopathology",
        "abstract": "Nucleus segmentation is an important analysis task in digital pathology. However, methods for automatic segmentation often struggle with new data from a different distribution, requiring users to manually annotate nuclei and retrain data-specific models. Vision foundation models (VFMs), such as the Segment Anything Model (SAM), offer a more robust alternative for automatic and interactive segmentation. Despite their success in natural images, a foundation model for nucleus segmentation in histopathology is still missing. Initial efforts to adapt SAM have shown some success, but did not yet introduce a comprehensive model for diverse segmentation tasks. To close this gap, we introduce PathoSAM, a VFM for nucleus segmentation, based on training SAM on a diverse dataset. Our extensive experiments show that it is the new state-of-the-art model for automatic and interactive nucleus instance segmentation in histopathology. We also demonstrate how it can be adapted for other segmentation tasks, including semantic nucleus segmentation. For this task, we show that it yields results better than popular methods, while not yet beating the state-of-the-art, CellViT. Our models are open-source and compatible with popular tools for data annotation. We also provide scripts for whole-slide image segmentation.",
        "authors": "Titus Griebel, Anwai Archit, Constantin Pape",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.04"
    },
    {
        "number": 203,
        "UID": "O-203",
        "forum": "https://openreview.net/forum?id=ESzOwfBhRv",
        "Track": "Full paper",
        "Session": "Oral 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Oral",
        "title": "Vector Representations of Vessel Trees",
        "abstract": "We introduce a novel framework for learning vector representations of tree-structured geometric data focusing on 3D vascular networks. Our approach employs two sequentially trained Transformer-based autoencoders. In the first stage, the Vessel Autoencoder captures continuous geometric details of individual vessel segments by learning embeddings for sampled points along each curve. In the second stage, the Vessel Tree Autoencoder encodes the topology of the vascular network as a single vector representation, leveraging the segment-level embeddings from the first model. A recursive decoding process ensures that the reconstructed topology is a valid tree structure. Compared to 3D convolutional models, this proposed approach substantially lowers GPU memory requirements, facilitating large-scale training. Experimental results on a 2D synthetic tree dataset and a 3D coronary artery dataset demonstrate superior reconstruction fidelity, accurate topology preservation, and realistic interpolations in latent space. Our scalable framework, named VeTTA, offers precise, flexible, and topologically consistent modeling of anatomical tree structures in medical imaging.",
        "authors": "James Batten, Michiel Schaap, Matthew Sinclair, Ying Bai, Ben Glocker",
        "Time": "Day 3 \u2014 9:00am-10:15am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.05"
    },
    {
        "number": 203,
        "UID": "F-203",
        "forum": "https://openreview.net/forum?id=ESzOwfBhRv",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Vector Representations of Vessel Trees",
        "abstract": "We introduce a novel framework for learning vector representations of tree-structured geometric data focusing on 3D vascular networks. Our approach employs two sequentially trained Transformer-based autoencoders. In the first stage, the Vessel Autoencoder captures continuous geometric details of individual vessel segments by learning embeddings for sampled points along each curve. In the second stage, the Vessel Tree Autoencoder encodes the topology of the vascular network as a single vector representation, leveraging the segment-level embeddings from the first model. A recursive decoding process ensures that the reconstructed topology is a valid tree structure. Compared to 3D convolutional models, this proposed approach substantially lowers GPU memory requirements, facilitating large-scale training. Experimental results on a 2D synthetic tree dataset and a 3D coronary artery dataset demonstrate superior reconstruction fidelity, accurate topology preservation, and realistic interpolations in latent space. Our scalable framework, named VeTTA, offers precise, flexible, and topologically consistent modeling of anatomical tree structures in medical imaging.",
        "authors": "James Batten, Michiel Schaap, Matthew Sinclair, Ying Bai, Ben Glocker",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - O.05"
    },
    {
        "number": 37,
        "UID": "F-37",
        "forum": "https://openreview.net/forum?id=ns6nq592HX",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Chest-OMDL: Organ-specific Multidisease Detection and Localization in Chest Computed Tomography using Weakly Supervised Deep Learning from Free-text Radiology Report",
        "abstract": "Deep learning (DL) models designed to detect abnormalities in chest computed tomography (CT) reduce radiologists‚Äô workload. However, training multidisease diagnostic models requires large expert-annotated datasets, significantly increasing model development cost. To address this challenge, we propose a weakly supervised learning (WSL) framework entitled Chest-OMDL for Organ-specific Multidisease Detection and Localization in chest CT. Chest-OMDL trains DL models using disease labels extracted by RadBERT from free-text radiology reports and multi-organ segmentation masks generated by the Segment Anything by Text (SAT) model, therefore reducing the need for manual annotation. Specifically, Chest-OMDL employs a Y-shaped Mamba model (Y-Mamba), comprising a feature extractor, an organ segmentation decoder, and a disease anomaly map generator. By incorporating multidisease anatomical knowledge, Y-Mamba is trained with a multi-task loss for organ-level weak supervision. Chest-OMDL was trained and validated on the large-scale CT-RATE dataset (25,692 non-contrast 3D chest CT scans from 21,304 patients) and tested on the external RAD-ChestCT dataset (3,630 scans), outperforming CT-CLIP (contrastive language-image pre-training) and CT-Net (full supervision). The proposed Chest-OMDL can be applied to multiple anatomical sites, potentially streamlining diagnostics.",
        "authors": "Xuguang Bai, Mingxuan Liu, Yifei Chen, Hongjia Yang, Qiyuan Tian",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.01"
    },
    {
        "number": 43,
        "UID": "F-43",
        "forum": "https://openreview.net/forum?id=Fjjs67Llpr",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Staging Liver Fibrosis with Hepatic Perivascular Adipose Tissue as a CT Biomarker",
        "abstract": "Cirrhosis is the 12th leading cause of death in the US. There are several CT imaging signs of late fibrosis, such as redistribution of liver segment volume, increased liver nodularity, and periportal space widening. Timely intervention can reverse the progression of early hepatic fibrosis, but later stages are irreversible. We hypothesize that the perivascular adipose tissue (PVAT) around the portal vein arising from periportal space widening may also be predictive of liver fibrosis. In this work, a fully automated pipeline was developed to segment the liver, spleen, portal vein and its branches. The PVAT in the vicinity of the portal vein was identified. From these structures, CT imaging biomarkers (volume, attenuation, fat fraction) were computed. They were used to build uni- and multivariate logistic regression models for diagnosing advanced fibrosis and cirrhosis. The best multivariate model for cirrhosis achieved 93.3% AUC, 78.9% sensitivity, and 93.4% specificity. For advanced fibrosis, the multivariate model obtained 88.7% AUC, 84.2% sensitivity, and 73.7% specificity. The automated approach may be useful for population-based studies of metabolic disease and opportunistic screening.",
        "authors": "Skylar Chan, Tejas Sudharshan Mathai, Praveen T.S. Balamuralikrishna, Vivek Batheja, Jianfei Liu, Meghan G Lubner, Perry J Pickhardt, Ronald Summers",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.02"
    },
    {
        "number": 60,
        "UID": "F-60",
        "forum": "https://openreview.net/forum?id=IaZBTTIh7H",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Real-time Breast Lesion Detection in Videos via Spatial-temporal Feature Aggregation",
        "abstract": "Recently, transformer-based detectors have shown impressive\nperformance for breast lesion detection in ultrasound videos. However,\nthese methods often require substantial computational resource and ex-\nhibit low inference speed, which poses challenges towards real-time ap-\nplicability. To address this issue, we introduce a fast yet accurate spatial-\ntemporal transformer, named FA-DETR, to efficiently aggregate multi-\nscale spatial-temporal features for breast lesion detection in ultrasound\nvideos. Our FA-DETR is based on a lightweight spatial-temporal self-\nattention module, which seamlessly fuses spatial and temporal features\nextracted from each video frame. In the decoding phase, we employ IoU-\naware query selection to generate independent queries for each frame.\nThese queries gain access to rich spatial-temporal information through\nthe encoder embeddings‚Äô cross-attention and frame-aware cross-attention\nmechanisms. Experiments conducted on a public breast lesion ultrasound\nvideo dataset demonstrate that our FA-DETR achieves state-of-the-art\nperformance with an absolute gain of 3.8% in terms of overall AP while\nbeing 2.5 times faster, compared to the best existing approach in the\nliterature. Our code and models will be publicly released.",
        "authors": "Chao Qin, Jiale Cao, Fahad Shahbaz Khan, Salman Khan, Huazhu Fu, Ehud Ahissar, Rao Muhammad Anwer",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.03"
    },
    {
        "number": 78,
        "UID": "F-78",
        "forum": "https://openreview.net/forum?id=RuqEg2XAWq",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Beyond the Prompt: Deploying Medical Foundation Models on Diverse Chest X-ray Populations",
        "abstract": "Foundation models (FMs) have shown impressive performance in medical image analysis tasks, but their deployment in real-world clinical settings, especially across diverse patient populations such as adult and pediatric cases, remains challenging. Key open questions include optimal prompting techniques and strategies for model adaptation or fine-tuning for clinical use. In this study, we evaluated different approaches for deploying FMs in clinical scenarios for diverse patient populations. We use the lightweight, embedding-based vision-language FM $\\textit{MedImageInsight}$ to predict pneumonia from chest X-rays, a condition common in both adult and pediatric patients.\nWe observed large variation in model predictive performance depending on the chosen prompt design, highlighting the importance of text prompt design for successful zero-shot (ZS) application. On in-domain datasets, we found performance differences of up to 46% in Matthews correlation coefficient (MCC) and 56% in true positive rates across different text prompts.\nBy introducing text and vision embedding ensembles, we achieved substantial ZS improvements, outperforming training-based methods (fine-tuning, Linear Probe) in low-data scenarios by up to 43% for adults and 35% for pediatric populations (MCC). This ensembling strategy also promotes resource-efficient equitable clinical use by supporting diverse demographic subgroups, achieving MCC improvements of 6% by sex, 17% by age, and 10% by race compared to Linear Probe.",
        "authors": "Louisa Fay, Jean-Benoit Delbrouck, Thomas K√ºstner, Bin Yang, Noel C Codella, Matthew P. Lungren, Curtis Langlotz, Sergios Gatidis",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.04"
    },
    {
        "number": 88,
        "UID": "F-88",
        "forum": "https://openreview.net/forum?id=cjlaCXuGpt",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "LUV-Net: Multi-Pattern Lung Ultrasound Video Classification through Pattern-Specific Attention with Efficient Temporal Feature Extraction",
        "abstract": "Lung ultrasound (LUS) has emerged as a crucial bedside imaging tool for critical care, yet its interpretation remains challenging due to its artifact-based nature and high operator dependency. While deep learning approaches offer promising solutions for LUS pattern analysis, existing methods are limited by their focus on single-pattern recognition or disease-specific classification, and inadequate handling of temporal dynamics in video-based models. We propose LUV-Net (Lung Ultrasound Video Network), a novel deep learning model for multi-label classification of LUS patterns, combining pattern-specific attention mechanisms with temporal feature extraction. Our approach consists of two key modules: a spatial feature extraction module utilizing independent pattern-specific attention mechanisms, and a temporal feature extraction module designed to capture sequential relationships between adjacent frames. The model was evaluated using two distinct datasets: a development set of 341 LUS videos and a temporally separated validation set of 56 videos. Through 5-fold cross-validation, LUV-Net demonstrated superior performance in identifying all four LUS patterns (A-lines, B-lines, consolidation, and pleural effusion) compared to conventional video models, achieving higher AUC scores across patterns. The model's interpretability was validated through visualization of pattern-specific attention regions, providing insights into its decision-making process. The code is publicly available at https://github.com/iamhxxn2/LungUS_Video.",
        "authors": "Jung Hoon Lee, ChanGi Kim, Jinwoo Lee, Si Mong Yoon, Kyung-Eui Lee, HYUN-JUN PARK, Kwonhyung Hyung, Chang Min Park",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.05"
    },
    {
        "number": 89,
        "UID": "F-89",
        "forum": "https://openreview.net/forum?id=trUvr1gSNI",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "RaDialog: Large Vision-Language Models for X-Ray Reporting and Dialog-Driven Assistance",
        "abstract": "Conversational AI tools for generating and discussing accurate radiology reports could transform radiology by enabling collaborative, human-in-the-loop diagnostic processes, saving time and enhancing report quality. While, to this end, Large Vision-Language Models hold promise, current methods lack clinical correctness or are single-task models without conversational abilities. We propose a novel architecture and dataset to address these limitations. First, we propose a secondary image branch, explicitly focusing on structured clinical findings, improving the clinical correctness score by 13.3%. Second, we propose a catastrophic forgetting mitigation strategy and instruct dataset with variable dialog-based tasks, to enable our model to handle a multitude of different queries. RaDialog marks a foundational step toward clinical dialog systems, outperforming existing medical LVLMs by 15.0% in clinical correctness in report generation, 23.4% in interactive report correction, and is preferred by radiologists in 84.0% of cases over a comparative method. Our model and dataset are publicly available (https://github.com/ChantalMP/RaDialog and https://physionet.org/content/radialog-instruct-dataset/1.1.0/).",
        "authors": "Chantal Pellegrini, Ege √ñzsoy, Benjamin Busam, Benedikt Wiestler, Nassir Navab, Matthias Keicher",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.06"
    },
    {
        "number": 97,
        "UID": "F-97",
        "forum": "https://openreview.net/forum?id=weoMf4wdVI",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "FERN: A Fetal Echocardiography Registration Network for 2D-to-3D Alignment",
        "abstract": "2D Freehand echocardiography remains the primary imaging modality for routine fetal cardiac care, essential in the antenatal detection of Congenital Heart Disease (CHD). However, there is a lack of spatial context which requires 3D imaging. Current 3D methods, such as Spatio-Temporal Image Correlation (STIC), face limitations in success rate, image quality, and ease of use, and come at the cost of lower spatial and temporal resolution compared to 2D acquisitions. This work studies the feasibility of aligning real high spatial and temporal resolution 2D fetal echocardiography into a reference 3D space defined by lower resolution 3D STIC. FERN, a $\\textbf{F}$etal $\\textbf{E}$chocardiography $\\textbf{R}$egistration $\\textbf{N}$etwork, employs transformers for standard fetal echocardiography view alignment. The network is trained on simulated 2D slices derived from 3D volumes at end-diastole, and validated on real 2D acquisitions from fetuses with Coarctation of the Aorta and Right Aortic Arch diagnoses, achieving a mean Euclidean distance of 2.98 $\\pm$ 1.27 mm on cardiac region-of-interest points between predicted and manually selected planes. Compared to manually aligned planes, improved image similarity to an average atlas is achieved, confirmed by blinded best plane selection. This work demonstrates that high spatial and temporal resolution 2D fetal echocardiography can be integrated into a 3D context provided by lower-resolution 3D acquisitions or fetal cardiac atlases, potentially resulting in a new 3D visualization tool for enhanced CHD diagnosis.",
        "authors": "Paula Ramirez Gilliland, David F A Lloyd, Jacqueline Matthew, Reza Razavi, Milou PM Van Poppel, Andrew P. King, Maria Deprez",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.07"
    },
    {
        "number": 147,
        "UID": "F-147",
        "forum": "https://openreview.net/forum?id=NluEuc8MyV",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Symmetric Multi-level Gradient-Inverse Consistency Network for Brain Image Registration with Large Deformation",
        "abstract": "Accurate and robust deformable image registration is crucial for brain image analysis. While deep learning has significantly advanced this field, existing methods often lack robustness for large deformations due to inter-subject variability, frequently requiring pre-registration and relying heavily on data-driven approaches. To address these limitations, we propose an end-to-end Symmetric Multis-level Gradient-Inverse Consistency Network (SM-GICNet) for accurate and robust brain image registration. SM-GICNet employs 1) a symmetric multi-level framework with an attention gate mechanism to capture complex deformations at multiple scales, 2) a symmetric registration strategy at each level to mitigate directional bias, and 3) a gradient inverse consistency strategy to reduce reliance on data-driven constraints and control deformation field complexity. Experimental results demonstrate that our method is able to eliminate the need for pre-registration and\noutperforms state-of-the-art methods on large deformation registration tasks, achieving a Dice similarity coefficient of 0.797. The implementation of our SM-GICNet is available online at https://github.com/LSYLAB/SM-GICNet.git.",
        "authors": "HaoyingBai, Tongtong Che, JichangZhang, Shuyu Li",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.08"
    },
    {
        "number": 155,
        "UID": "F-155",
        "forum": "https://openreview.net/forum?id=6fqfInxqG1",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Empirical Analysis of Scaling Vision Foundation Models for Chest X-rays",
        "abstract": "Recent advancements in multimodal transformers have shown remarkable success in computer vision and natural language tasks, yet their adaptation to the clinical world remains challenging. We introduce CXformer, a vision transformer adapted for chest X-ray analysis, through systematic investigation of architectural choices and training modifications from DINOv2. Our empirical results show that using registers in ViT training, centering the teacher model's softmax outputs, and optimizing the number of heads leads to better performance. The small version of CXformer(S) (22M parameters) achieves 83.28% mean AUROC on CheXpert test set, surpassing the baseline of 80.46% achieved with vanilla DINOv2 settings. Contrary to common assumptions, our larger model CXformer(B) with 87M parameters shows similar performance at 84% mean AUROC on CheXpert, suggesting that training optimizations matter more than model size. Furthermore compared to the current state-of-the-art RAD-DINO, our CXformer(B), with 46% reduced pretraining compute (in FLOPs) achieves an average AUROC of 87.93% (vs 87.32% by RAD-DINO) on pathology image classification task evaluated across three widely used CXR datasets i.e. CheXpert, RSNA Pneumonia, and NIH CXR8. Beyond classification, CXformer also delivers competitive, and occasionally superior, performance in semantic segmentation and radiology report generation, underscoring its versatility. CXformer base and small models can be found at https://huggingface.co/m42-health",
        "authors": "Ahmed Al Mahrooqi, Prateek Munjal, Ronnie Rajan, Marco AF Pimentel, Praveenkumar Kanithi",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.09"
    },
    {
        "number": 184,
        "UID": "F-184",
        "forum": "https://openreview.net/forum?id=TWPnpixpW2",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Parameter Efficient Fine-Tuning of Segment Anything Model",
        "abstract": "Segmentation is an important analysis task for biomedical images, enabling the study of individual organelles, cells or organs. Deep learning has massively improved segmentation methods, but challenges remain in generalization to new conditions, requiring costly data annotation.\nVision foundation models, such as Segment Anything Model (SAM), address this issue through broad segmentation capabilities. However, these models still require finetuning on annotated data, although with less annotations, to achieve optimal results for new conditions. As a downside, they require more computational resources. This makes parameter-efficient finetuning (PEFT) relevant for their application. We contribute the first comprehensive study of PEFT for SAM applied to biomedical segmentation by evaluating 9 PEFT methods on diverse datasets. We also provide an implementation of QLoRA for vision transformers and a new approach for resource-efficient finetuning of SAM.",
        "authors": "Carolin Teuber, Anwai Archit, Constantin Pape",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.10"
    },
    {
        "number": 213,
        "UID": "F-213",
        "forum": "https://openreview.net/forum?id=qiSVJGFAar",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Point-Based Shape Representation Generation with a Correspondence-Preserving Diffusion Model",
        "abstract": "We propose a diffusion model designed to generate point-based shape representations with correspondences.\nTraditional statistical shape models have considered point correspondences extensively, but current deep learning methods do not take them into account, focusing on unordered point clouds instead. \nCurrent deep generative models for point clouds do not address generating shapes with point correspondences between generated shapes.\nThis work aims to formulate a diffusion model that is capable of generating realistic point-based shape representations, which preserve point correspondences that are present in the training data.\nUsing shape representation data with correspondences derived from OASIS-3, we demonstrate that our correspondence-preserving model effectively generates point-based hippocampal shape representations that are highly realistic compared to existing methods. We further demonstrate the applications of our generative model by downstream tasks, such as boosting classification accuracy and predicting morphological changes of disease progression by counterfactual generation.",
        "authors": "Shen Zhu, Yinzhu Jin, Ifrah Zawar, Tom Fletcher",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.11"
    },
    {
        "number": 219,
        "UID": "F-219",
        "forum": "https://openreview.net/forum?id=rbV51Uz7CN",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Fast forward: Rephrasing 3D deformable image registration through density alignment and splatting",
        "abstract": "Unsupervised learning- and optimisation-based 3D registration has almost exclusively been approached using backward warping (interpolation) for transforming images. While this has practical advantages in particular the ease of implementation within common libraries it limits the robustness and accuracy in certain challenging scenarios. The alternative solution of forward splatting (extrapolation) is currently limited to very few applications, e.g. mesh or point cloud registration, requiring specific geometric learning architectures that are so far less efficient compared to dense 3D convolutional networks. In this work, we propose to use a straightforward forward splatting technique based on differentiable rasterisation. Contrary to prior work, we rephrase the problem of deformable image registration as a density alignment of rasterised volumes based on intermediate point cloud representations that can be automatically obtained through e.g. geometric vessel filters or surface segmentations. Our experimental validation demonstrates state-of-the-art performance over a wide range of registration tasks including intra- and inter-patient alignment of thorax and abdomen.",
        "authors": "Mattias P Heinrich, Alexander Bigalke, Lasse Hansen",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.12"
    },
    {
        "number": 223,
        "UID": "F-223",
        "forum": "https://openreview.net/forum?id=fKuDgubMq3",
        "Track": "Full paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "MORPH-LER: Log-Euclidean Regularization for Population-Aware Image Registration",
        "abstract": "Spatial transformations that capture population-level morphological statistics are critical for medical image analysis. Commonly used smoothness regularizers for image registration fail to integrate population statistics, leading to anatomically inconsistent transformations. Inverse consistency regularizers promote geometric consistency but lack population morphometrics integration. Regularizers that constrain deformation to low-dimensional manifold methods address this. However, they prioritize reconstruction over interpretability and neglect diffeomorphic properties, such as group composition and inverse consistency. We introduce MORPH-LER, a Log-Euclidean regularization framework for population-aware unsupervised image registration. MORPH-LER, learns population morphometrics from spatial transformations to guide and regularize registration networks, ensuring anatomically plausible deformations. It features a bottleneck autoencoder that computes the principal logarithm of deformation fields via iterative square-root predictions. It creates a linearized latent space that respects diffeomorphic properties and enforces inverse consistency. By integrating a registration network with a diffeomorphic autoencoder, MORPH-LER produces smooth, meaningful deformation fields. The framework offers two main contributions: (1) a data-driven regularization strategy that incorporates population-level anatomical statistics to enhance transformation validity and (2) a linearized latent space that enables compact and interpretable deformation fields for efficient population morphometrics analysis. We validate MORPH-LER across two families of deep learning-based registration networks, demonstrating its ability to produce anatomically accurate, computationally efficient, and statistically meaningful transformations on the OASIS-1 brain imaging dataset.",
        "authors": "Mokshagna Sai Teja Karanam, Krithika Iyer, Sarang Joshi, Shireen Elhabian",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - F.13"
    },
    {
        "number": 21,
        "UID": "S-21",
        "forum": "https://openreview.net/forum?id=DGvFGbX0EG",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Unsupervised Deformable Image Registration Revisited: Enhancing Performance with Registration-Specific Designs",
        "abstract": "Deformable image registration (DIR) is ill-posed. Many registration-specific designs and regularizations, whose rationale carries across classic optimization methods to deep-learning-based (DL) frameworks, are crucial to registration performance. This paper presents a comprehensive ‚Äúablation‚Äù type study to pinpoint the key modules for unsupervised mono-modal DL-DIR. Our findings highlight the value of incorporating multi-resolution pyramids, local correlation, and inverse consistency constraints, and show that even simple network architectures can be highly effective. We conducted controlled experiments and benchmarked performance against state-of-the-art methods. The code will be publicly available at: https://github.com/HengjieLiu/Unsupervised-DL-DIR-Revisited.",
        "authors": "Hengjie Liu, Dan Ruan, Ke Sheng",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.01"
    },
    {
        "number": 26,
        "UID": "S-26",
        "forum": "https://openreview.net/forum?id=PiTTSCWAXQ",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Whole Slide Image Domain Adaptation Tailored with Fisher Vector, Self Supervised Learning, and Novel Loss Function",
        "abstract": "Whole Slide Images (WSIs) present major challenges in computational pathology due to their high resolution, morphological diversity, and domain variability across institutions. These factors result in domain shifts that limit model generalization. We propose a domain adaptation framework that integrates self-supervised learning, clustering, and Fisher Vector encoding for robust WSI classification. Patch-level features are extracted using MoCoV3, clustered via k-means, and aggregated using Gaussian mixture-based Fisher Vectors to form compact slide-level representations. To align domains, we employ adversarial training enhanced with a tailored loss combining PLMMD and MCC. Evaluated on HER2 classification across TCGA-BRCA and Warwick datasets, our method consistently outperforms baselines, especially under label noise and domain shift, demonstrating the strength of combining self-supervised features with structured statistical encoding for cross-domain WSI analysis.",
        "authors": "Ravi Kant Gupta, Shounak Das, Shambhavi Shanker, Ardhendu Sekhar, Amit Sethi",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.02"
    },
    {
        "number": 31,
        "UID": "S-31",
        "forum": "https://openreview.net/forum?id=NJANlZzxfi",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Pretraining Deformable Image Registration Networks with Random Images",
        "abstract": "Recent advances in deep learning-based medical image registration have shown that training deep neural networks~(DNNs) does not necessarily require medical images. Previous work showed that DNNs trained on randomly generated images with carefully designed noise and contrast properties can still generalize well to unseen medical data. Building on this insight, we propose using registration between random images as a proxy task for pretraining a foundation model for image registration. Empirical results show that our pretraining strategy improves registration accuracy, reduces the amount of domain-specific data needed to achieve competitive performance, and accelerates convergence during downstream training, thereby enhancing computational efficiency.",
        "authors": "Junyu Chen, Shuwen Wei, Yihao Liu, Aaron Carass, Yong Du",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.03"
    },
    {
        "number": 35,
        "UID": "S-35",
        "forum": "https://openreview.net/forum?id=zu4J7wNk6W",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Hierarchical Transformer for Electrocardiogram Diagnosis",
        "abstract": "We propose a hierarchical Transformer for ECG analysis that combines depth-wise convo- lutions, multi-scale feature aggregation via a CLS token, and an attention-gated module to preserve inter-lead relationships and enhance interpretability. The model is lightweight, flexible, and eliminates the need for complex attention or downsampling strategies. Code is available at :https://github.com/xiaoyatang/3stageFormer.git.",
        "authors": "Xiaoya Tang, JAKE AARON BERGQUIST, Benjamin A. Steinberg, Tolga Tasdizen",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.04"
    },
    {
        "number": 36,
        "UID": "S-36",
        "forum": "https://openreview.net/forum?id=C6aV2jJ9Dz",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Bone Marrow Fibrosis Grading Using Attention-Based Multiple Instance Learning",
        "abstract": "An attention-based multiple instance learning approach is used to improve bone marrow fibrosis (BMF) grading from whole slide images of bone marrow core biopsies. Slide-level labels were parsed from biopsy reports using a large language model, and features were extracted using our fine-tuned DINOv2-based backbone. The model achieved good agreement between BMF predictions and labels $(R^2 = 0.72,\\ \\kappa = 0.58)$. Attention maps showed the model focused on diagnostic regions, highlighting its accuracy and interpretability.",
        "authors": "Lauren M. Zuromski, Alexandra E. Rangel, Muir J. Morrison, Paul English, Nicholas C. Spies, \"Brendan OFallon\", David P. Ng",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.05"
    },
    {
        "number": 39,
        "UID": "S-39",
        "forum": "https://openreview.net/forum?id=fwmKiBGvD5",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Deep Multi-Head CNN for Unified Breast Density Segmentation and BI-RADS Estimation in 2D Mammograms",
        "abstract": "Comprehensive breast density estimation is crucial in mammogram assessment and cancer risk stratification, yet many existing AI-based radiomic methods designed for this purpose often tackle tissue segmentation and classification as separate tasks. To address this limitation, we propose a multi-head convolutional neural network (MH-CNN) that integrates these functions into a unified end-to-end architecture. Built on a ResNet101 encoder, our approach learns high-level features for breast density segmentation while parallel network heads perform continuous density regression and BI-RADS classification overlaid in the resulting images. Evaluation on the VinDr mammogram dataset yielded a Dice coefficient of 84.57% for segmentation, a mean absolute error (MAE) of 5.92% for density regression, and 80.51% accuracy for BI-RADS classification. These results suggest that the MH-CNN can streamline clinical workflows by providing objective and reliable breast density assessments",
        "authors": "Obinna Agbodike, Chang-Fu Kuo, Jenhui Chen",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.06"
    },
    {
        "number": 41,
        "UID": "S-41",
        "forum": "https://openreview.net/forum?id=CWpgEgjcuJ",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Ranking-Aware Multiple Instance Learning for Histopathology Slide Classification",
        "abstract": "In digital pathology, most deep learning models adopt multiple instance learning (MIL) as it requires only slide-level labels, reducing the need for detailed annotations. However, since MIL still relies on large datasets, data-efficient strategies have emerged as promising alternatives. Although some datasets include expert annotations, their integration with MIL to take advantage of this valuable information has been overlooked. We propose Rank induction, a method that ranks annotated lesion areas against non-lesion areas to guide the model‚Äôs attention toward diagnostically meaningful areas. Our experiments on the Camelyon16 dataset show that Rank induction outperforms existing approaches in classification performance. Furthermore, the method remains robust under data-scarce conditions. Finally, attention maps generated by the model trained with Rank induction focus more accurately on cancerous areas.",
        "authors": "Ho Heon Kim, Gisu Hwang, Won Chan Jeong, Young Sin Go",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.07"
    },
    {
        "number": 48,
        "UID": "S-48",
        "forum": "https://openreview.net/forum?id=TwLJ7yVxwu",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Does Radiomic Segmentation Complexity Influence Foundation Model Performance? A Case Study with SAM-Med3D",
        "abstract": "The Segment Anything Model (SAM) has significantly expanded the application of foundation models in medical image segmentation. However, performance can vary significantly depending on the complexity of the segmentation task. This study examines how segmentation complexity, characterized through radiomic features, impacts the performance of SAM-Med3D in 3D medical imaging tasks. Specifically, it explores the relationship between segmentation complexity and model performance using five public datasets: MSD-Vessel, MSD-Colon, EPISURG, SPIDER, and PENGWIN. The analysis computed Intersection over Union (IoU) and Mean Surface Distance (MSD). Our results revealed that radiomic features such as mesh volume, sphericity, surface/volume ratio, and texture difference inside and outside the ROI significantly correlate with segmentation performance. Higher mesh volumes and lower surface/volume ratios were associated with better performance, suggesting that more compact and larger structures are segmented more accurately. These findings underscore the relevance of assessing the influence of segmentation complexity in medical imaging, as captured through radiomic features. This analysis provides valuable insights into the applicability of generalist models to specific tasks, based on the radiomic characteristics of the data.",
        "authors": "F. Javier Gil-Terr√≥n, Carles Lopez-Mateu, Maria G√≥mez Mahiques, Victor Montosa-i-Mico, Juan Miguel Garcia-Gomez, Elies Fuster-Garcia",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.08"
    },
    {
        "number": 69,
        "UID": "S-69",
        "forum": "https://openreview.net/forum?id=oGBZ8ClDCS",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Parameter Efficient Fine-Tuning of Large Vision Foundational Models for Multi-Channel Medical Image Segmentation",
        "abstract": "Multi-channel data in medial imaging where each modality encodes distinct and complementary information is critical for accurate 3D segmentation. The UNetR architecture has demonstrated success in 3D medical image segmentation by integrating transformer-based encoder with a convolutional decoder. However, full fine-tuning of UNetR for new multi-channel tasks is computationally expensive and prone to over-fitting, especially with limited data and large transformer backbones. Moreover conventional transformer models, such as Vision Transformers are typically pre-trained on single channel images, limiting their direct applicability in multi-modal imaging tasks. To address this, we propose a parameter-efficient fine-tuning strategy using channel-wise Low-Rank Adaptation adapters within the UNetR encoder framework, enabling scalable multi-channel adaptation with reduced parameter overhead.",
        "authors": "Kaushik Dutta, Devansh Agarwal, David Paulucci, Angshu Rai, Mariann Micsinai-Balan",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.09"
    },
    {
        "number": 85,
        "UID": "S-85",
        "forum": "https://openreview.net/forum?id=2kl73AjoPB",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Early diagnosis of skin cancer from phone-taken skin lesion images using Vision Transformers",
        "abstract": "Recent advances in computer vision have made Vision Transformers (ViTs) strong alternatives to CNNs in medical imaging. We compare top ViT models‚Äîincluding Token-to-Token ViT, CaiT, LeViT, ATSViT, and XCiT‚Äîon the Kaggle skin cancer dataset, focusing on classification accuracy, real score, and model complexity. While ViTs for small datasets show high accuracy, they have many parameters; LeViT offers strong performance with the fewest parameters. This review highlights current trends, deployment challenges, and future directions for transformers in skin cancer detection.",
        "authors": "Sina Garazhian, Parsa Hariri",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.10"
    },
    {
        "number": 91,
        "UID": "S-91",
        "forum": "https://openreview.net/forum?id=vWkjFvYUws",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Dynamic Scale for Transformer",
        "abstract": "To enhance the hierarchical transformer with fixed embedding sizes, we propose a dynamic CLS token that aggregates information from CLS tokens across all layers, each embedded with varying receptive fields, by leveraging a squeeze-and-excitation module. This architecture offers a more flexible approach to utilizing multi-scale features in transformers. Code is available at: https://github.com/xiaoyatang/DynamicCLS.git.",
        "authors": "Xiaoya Tang, Xiwen Li, Tolga Tasdizen",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.11"
    },
    {
        "number": 104,
        "UID": "S-104",
        "forum": "https://openreview.net/forum?id=skMxXw1qrp",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Spatial-Temporal Consistency Enhanced Segmentation for Laparoscopic Surgical videos",
        "abstract": "Accurate segmentation of laparoscopic surgical videos is essential for enhancing intraoperative guidance and improving patient outcomes. However, this task remains challenging\ndue to the constrained field of view, visual clutter, frequent occlusions, and inconsistent illumination. To address these challenges, we propose SSTC-Seg (Surgical Spatial-Temporal\nConsistency Segmentation), a lightweight deep learning framework for video-based segmentation. It integrates a memory system and a Hierarchical Dense Conditional Random\nField (HD-CRF) with skip connections for spatial details preservation to refine coarse\npredictions and model contextual relationships across frames. Evaluated on the Dresden\nSurgical Anatomy Dataset (DSAD), SSTC-Seg achieves competitive multi-organ segmentation performance with significantly fewer parameters compared to existing state-of-the-art\nmethods.",
        "authors": "Ewen Rondel, Lin Guo, Flavio Esposito, Mohammad Mahmoud",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.12"
    },
    {
        "number": 113,
        "UID": "S-113",
        "forum": "https://openreview.net/forum?id=OCOQa4w6dN",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "VoxelFeat: Voxel-wise foundation model features",
        "abstract": "Foundation models like SAM2 offer rich semantic features but suffer from fixed resolution, transformer artifacts, and inconsistent representations across views, limiting their direct use in 3D applications such as image segmentation. We extend FeatUp, a multi-view self-supervised upsampling approach, to 3D by introducing explicit 3D position encodings and through-plane augmentations. Our normalizer-free NFNet-based architecture enables consistent, denoised, and resolution-agnostic feature inference in medical CT volumes. The resulting 3D-aware representation supports interactive segmentation via point-wise, local inference at native resolution.",
        "authors": "Pascual Tejero Cervera, Samuel Joutard, Raphael Prevost, Maximilian Pietsch",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.13"
    },
    {
        "number": 126,
        "UID": "S-126",
        "forum": "https://openreview.net/forum?id=iLBipDelQu",
        "Track": "Short paper",
        "Session": "Poster 3.A: Representation Learning & Foundation Models",
        "Final Decision": "Poster",
        "title": "Anatomy-guided Test-Time Adaptation for Automated Fetal Brain MRI Morphometry",
        "abstract": "Fetal brain MRI enables prenatal diagnosis of neurodegenerative diseases through linear morphologic measurements. Traditional manual measurements derived from the visual assessment of 2D MRI slices is labor-intensive, expertise-dependent, and suffers from high inter- and intra-rater variability due to inconsistent slice selection. Deep learning-based automated fetal brain MRI morphometry has been proposed to address these limitations. However, these automated models still struggle with limited generalizability due to cross-device MRI variability and lesion heterogeneity. To solve the problem, we propose an anatomy-guided test-time adaptation (TTA) method integrating a local-global dual-network with anatomical priors via atlas registration, enhancing cross-domain adaptability. Experimental results demonstrate that the model with TTA achieves superior brain measurement accuracy, outperforming both the model without TTA and the registration-based method.",
        "authors": "Yijin Li, Mingxuan Liu, Hongjia Yang, Haoxiang Li, Xuguang Bai, Yi Liao, Haibo Qu, Qiyuan Tian",
        "Time": "Day 3 \u2014 10:30am-11:30am",
        "Poster time": "Day 3 \u2014 10:30am-11:30am",
        "Poster ID": "Poster 3.A - S.14"
    },
    {
        "number": 50,
        "UID": "O-50",
        "forum": "https://openreview.net/forum?id=jYTdkYeCv8",
        "Track": "Full paper",
        "Session": "Oral 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Oral",
        "title": "Neural fields for tissue attenuation curve reconstruction in sparsely sampled time-resolved CT",
        "abstract": "Time-resolved CT imaging can aid acute ischemic stroke diagnosis by visualizing contrast agent transport through the brain (micro)vasculature. CT perfusion imaging, while widely used for stroke diagnosis, requires approximately 30 sequential scans, leading to extensive radiation exposure and motion sensitivity. As an alternative to CTP perfusion imaging, some hospitals opt for multiphase CT angiography for time-resolved analysis with reduced radiation dose. However, multiphase CT angiography lacks standardized perfusion analysis capabilities, making it more challenging to interpret than CT perfusion imaging. We present Sparse Temporal Attenuation Reconstruction (STAR), a novel approach using conditional neural fields that reconstructs tissue attenuation curves from sparse observations, allowing for reduced radiation exposure and motion sensitivity with CT perfusion, while enabling perfusion analysis from multiphase CT angiography. Our method generates full tissue attenuation curves using only 4 out of 30 observations. The results show that perfusion maps from reconstructed data match the reference perfusion maps, potentially reducing radiation and allowing recovery of motion-corrupted images. Moreover, STAR enables perfusion analysis in centers using multiphase CT angiography. Consequently, STAR has the potential to improve the stroke imaging work-up while making perfusion analysis more widely accessible.",
        "authors": "Lucas de Vries, Rudolf Leonardus Mirjam Van Herten, P. Matthijs van der Sluijs, Ivana Isgum, Bart J. Emmer, Charles B.L.M. Majoie, Henk Marquering, Efstratios Gavves",
        "Time": "Day 3 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.01"
    },
    {
        "number": 50,
        "UID": "F-50",
        "forum": "https://openreview.net/forum?id=jYTdkYeCv8",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Neural fields for tissue attenuation curve reconstruction in sparsely sampled time-resolved CT",
        "abstract": "Time-resolved CT imaging can aid acute ischemic stroke diagnosis by visualizing contrast agent transport through the brain (micro)vasculature. CT perfusion imaging, while widely used for stroke diagnosis, requires approximately 30 sequential scans, leading to extensive radiation exposure and motion sensitivity. As an alternative to CTP perfusion imaging, some hospitals opt for multiphase CT angiography for time-resolved analysis with reduced radiation dose. However, multiphase CT angiography lacks standardized perfusion analysis capabilities, making it more challenging to interpret than CT perfusion imaging. We present Sparse Temporal Attenuation Reconstruction (STAR), a novel approach using conditional neural fields that reconstructs tissue attenuation curves from sparse observations, allowing for reduced radiation exposure and motion sensitivity with CT perfusion, while enabling perfusion analysis from multiphase CT angiography. Our method generates full tissue attenuation curves using only 4 out of 30 observations. The results show that perfusion maps from reconstructed data match the reference perfusion maps, potentially reducing radiation and allowing recovery of motion-corrupted images. Moreover, STAR enables perfusion analysis in centers using multiphase CT angiography. Consequently, STAR has the potential to improve the stroke imaging work-up while making perfusion analysis more widely accessible.",
        "authors": "Lucas de Vries, Rudolf Leonardus Mirjam Van Herten, P. Matthijs van der Sluijs, Ivana Isgum, Bart J. Emmer, Charles B.L.M. Majoie, Henk Marquering, Efstratios Gavves",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.01"
    },
    {
        "number": 73,
        "UID": "O-73",
        "forum": "https://openreview.net/forum?id=A6iHYOLrLd",
        "Track": "Full paper",
        "Session": "Oral 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Oral",
        "title": "Self-Supervised Synthetic Cerebral Vessel Tree Generation using Semantic Signed Distance Fields",
        "abstract": "Advances in in-silico clinical trails for the development of novel treatment and devices for acute ischemic stroke have driven the creation of synthetic virtual patient populations to address the lack of large real-world datasets. Recent work proposed a method for generating semantic vascular centerline tree of the major cerebral arteries using pointcloud diffusion. However, this approach relies on separate post-processing algorithms to reconstruct the vessel tree topology, which does not generalize well to more topologically complex trees. To overcome this limitation, we introduce semantic signed distance fields for modeling cerebral vessel trees in a fully self-supervised manner. Our approach bypasses the need for separate reconstruction of the tree topology, and can be trained directly on shape-surfaces. Our method combines a variational autoencoder for encoding shapes to robust latent shape representations with a latent-diffusion model for generating synthetic vessel trees.  By generating surface geometry directly, our approach eliminates the need for post-processing steps, enabling the generation of high-quality and topologically complex cerebral vessel trees.",
        "authors": "Thijs P. Kuipers, Praneeta R. Konduri, Erik J Bekkers, Henk Marquering",
        "Time": "Day 3 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.02"
    },
    {
        "number": 73,
        "UID": "F-73",
        "forum": "https://openreview.net/forum?id=A6iHYOLrLd",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Self-Supervised Synthetic Cerebral Vessel Tree Generation using Semantic Signed Distance Fields",
        "abstract": "Advances in in-silico clinical trails for the development of novel treatment and devices for acute ischemic stroke have driven the creation of synthetic virtual patient populations to address the lack of large real-world datasets. Recent work proposed a method for generating semantic vascular centerline tree of the major cerebral arteries using pointcloud diffusion. However, this approach relies on separate post-processing algorithms to reconstruct the vessel tree topology, which does not generalize well to more topologically complex trees. To overcome this limitation, we introduce semantic signed distance fields for modeling cerebral vessel trees in a fully self-supervised manner. Our approach bypasses the need for separate reconstruction of the tree topology, and can be trained directly on shape-surfaces. Our method combines a variational autoencoder for encoding shapes to robust latent shape representations with a latent-diffusion model for generating synthetic vessel trees.  By generating surface geometry directly, our approach eliminates the need for post-processing steps, enabling the generation of high-quality and topologically complex cerebral vessel trees.",
        "authors": "Thijs P. Kuipers, Praneeta R. Konduri, Erik J Bekkers, Henk Marquering",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.02"
    },
    {
        "number": 120,
        "UID": "O-120",
        "forum": "https://openreview.net/forum?id=pkWDVfOW9K",
        "Track": "Full paper",
        "Session": "Oral 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Oral",
        "title": "BinaryFormer: 1-bit self-attention for long-range transformers in medical image segmentation and 3D diffusion models",
        "abstract": "Vision transformers excel at capturing long-range interactions and have become essential for many medical image analysis tasks. Their computational cost, however, grows quadratically with sequence length - which is problematic for certain 3D problems, e.g.  high-resolution diffusion models that require dozens of sampling steps. Flash attention addressed some limitations by optimising local memory access, but left the computational burden high. Quantising weights and activations for convolutions and fully binary networks are possible, but have to be trained at higher precision and often resulted in performance drops. For transformers recent studies have been limited to quantising weights in linear layers or exploiting the potential of sparsity in self-attention scores. \n\nWe present a novel scheme that not only enables a binary precision computation of the self-attention at inference time but also extends this to the training of transformers. To achieve differentiability we combine the bitwise Hamming distance with a learnable scalar query and key weighting. In theory this yields a 16-32x more resource-efficiency in arithmetic operations and memory bandwidth. We evaluate our model on three tasks with sequence lengths of N>1000: classification of images without patch-embedding, semantic 2D MRI segmentation and 3D high-resolution diffusion models for inpainting and synthesis. Our results demonstrate competitive performance and we provide an intuitive reasoning for the effectiveness of differentiable key-, query- weighting through Bernoulli sampling and distance interpolation. https://github.com/mattiaspaul/BinaryFormer",
        "authors": "Mattias P Heinrich",
        "Time": "Day 3 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.03"
    },
    {
        "number": 120,
        "UID": "F-120",
        "forum": "https://openreview.net/forum?id=pkWDVfOW9K",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "BinaryFormer: 1-bit self-attention for long-range transformers in medical image segmentation and 3D diffusion models",
        "abstract": "Vision transformers excel at capturing long-range interactions and have become essential for many medical image analysis tasks. Their computational cost, however, grows quadratically with sequence length - which is problematic for certain 3D problems, e.g.  high-resolution diffusion models that require dozens of sampling steps. Flash attention addressed some limitations by optimising local memory access, but left the computational burden high. Quantising weights and activations for convolutions and fully binary networks are possible, but have to be trained at higher precision and often resulted in performance drops. For transformers recent studies have been limited to quantising weights in linear layers or exploiting the potential of sparsity in self-attention scores. \n\nWe present a novel scheme that not only enables a binary precision computation of the self-attention at inference time but also extends this to the training of transformers. To achieve differentiability we combine the bitwise Hamming distance with a learnable scalar query and key weighting. In theory this yields a 16-32x more resource-efficiency in arithmetic operations and memory bandwidth. We evaluate our model on three tasks with sequence lengths of N>1000: classification of images without patch-embedding, semantic 2D MRI segmentation and 3D high-resolution diffusion models for inpainting and synthesis. Our results demonstrate competitive performance and we provide an intuitive reasoning for the effectiveness of differentiable key-, query- weighting through Bernoulli sampling and distance interpolation. https://github.com/mattiaspaul/BinaryFormer",
        "authors": "Mattias P Heinrich",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.03"
    },
    {
        "number": 234,
        "UID": "O-234",
        "forum": "https://openreview.net/forum?id=w3p7GddsQ8",
        "Track": "Full paper",
        "Session": "Oral 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Oral",
        "title": "Anatomy-Guided Multi-Path CycleGAN for Lung CT Kernel Harmonization",
        "abstract": "Accurate quantitative measurement in lung computed tomography (CT) imaging often relies on consistent kernel reconstruction across scanners and manufacturers. Harmonization can reduce measurement variability caused by heterogeneous reconstruction kernels; however, harmonization across different manufacturers and scanners remains challenging due to significant differences in reconstruction protocol and positional alignment of subjects, often resulting in anatomical hallucinations. To address this, we propose a multi-path cycleGAN framework that incorporates multi-region anatomical labels and a tissue statistic loss as anatomical regularization to preserve structural integrity during harmonization. We trained our model on 100 scans each of four representative reconstruction kernels from the National Lung Screening Trial (NLST) dataset and evaluated it on 240 withheld scans. Experimental results demonstrate superior performance of our method in both within manufacturer harmonization and cross-manufacture harmonization: Harmonizing hard-to-soft kernel images within a single manufacturer significantly reduces emphysema measurement discrepancies (p < 0.05). Across manufacturers, harmonizing all kernels to a reference soft kernel yields consistent emphysema quantification (p > 0.05) and preserves anatomical structures, as demonstrated by improved Dice similarity coefficient in skeletal muscle and subcutaneous adipose tissue between harmonized and unharmonized images. These findings demonstrate that segmentation-driven anatomical regularization effectively addresses cross-manufacturer discrepancies, ensuring robust quantitative imaging. We release our\ncode and model at https://github.com/MASILab/AnatomyconstrainedMultipathGAN.",
        "authors": "Aravind Krishnan, Thomas Li, Lucas Walker Remedios, Kaiwen Xu, Lianrui Zuo, Kim L. Sandler, Fabien Maldonado, Bennett Allan Landman",
        "Time": "Day 3 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.04"
    },
    {
        "number": 234,
        "UID": "F-234",
        "forum": "https://openreview.net/forum?id=w3p7GddsQ8",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Anatomy-Guided Multi-Path CycleGAN for Lung CT Kernel Harmonization",
        "abstract": "Accurate quantitative measurement in lung computed tomography (CT) imaging often relies on consistent kernel reconstruction across scanners and manufacturers. Harmonization can reduce measurement variability caused by heterogeneous reconstruction kernels; however, harmonization across different manufacturers and scanners remains challenging due to significant differences in reconstruction protocol and positional alignment of subjects, often resulting in anatomical hallucinations. To address this, we propose a multi-path cycleGAN framework that incorporates multi-region anatomical labels and a tissue statistic loss as anatomical regularization to preserve structural integrity during harmonization. We trained our model on 100 scans each of four representative reconstruction kernels from the National Lung Screening Trial (NLST) dataset and evaluated it on 240 withheld scans. Experimental results demonstrate superior performance of our method in both within manufacturer harmonization and cross-manufacture harmonization: Harmonizing hard-to-soft kernel images within a single manufacturer significantly reduces emphysema measurement discrepancies (p < 0.05). Across manufacturers, harmonizing all kernels to a reference soft kernel yields consistent emphysema quantification (p > 0.05) and preserves anatomical structures, as demonstrated by improved Dice similarity coefficient in skeletal muscle and subcutaneous adipose tissue between harmonized and unharmonized images. These findings demonstrate that segmentation-driven anatomical regularization effectively addresses cross-manufacturer discrepancies, ensuring robust quantitative imaging. We release our\ncode and model at https://github.com/MASILab/AnatomyconstrainedMultipathGAN.",
        "authors": "Aravind Krishnan, Thomas Li, Lucas Walker Remedios, Kaiwen Xu, Lianrui Zuo, Kim L. Sandler, Fabien Maldonado, Bennett Allan Landman",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.04"
    },
    {
        "number": 253,
        "UID": "O-253",
        "forum": "https://openreview.net/forum?id=oLTpF4mxZc",
        "Track": "Full paper",
        "Session": "Oral 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Oral",
        "title": "Family of Deep Image Prior Networks for Accelerated 3D LGE-MRI Acquisition with Enhanced Reconstruction",
        "abstract": "Late Gadolinium Enhancement (LGE) MRI is essential for visualizing and treating left atrial fibrosis, but current protocols require lengthy acquisition times (7-20 minutes) and often produce suboptimal image quality. While recent advances in isotropic imaging have shown promise, scan times of 12-15 minutes still present clinical challenges. This study evaluates the efficacy of existing Deep Image Prior (DIP) frameworks for accelerated 3D LGE-MRI reconstruction. We comprehensively assess multiple DIP variants - vanilla DIP, reference-guided DIP, DIP with Total Variation, and self-guided DIP - on their ability to reconstruct high-quality isotropic (1.25mm$^3$) images from highly undersampled k-space data. Using data from 10 subjects, we demonstrate that self-guided DIP achieves superior reconstruction quality (PSNR: 32.8¬±1.2 dB, SSIM: 0.891¬±0.015 at 1/4th of acquisition time) compared to traditional compressed sensing and other DIP variants. Our evaluation shows that DIP-based reconstruction can maintain diagnostic quality with acquisition times reduced to 2-4 minutes, particularly in preserving thin left atrial wall details. These findings suggest that DIP-based methods could improve clinical workflow efficiency and patient comfort in high-resolution 3D LGE studies for atrial fibrillation patients.",
        "authors": "Md Hasibul Husain Hisham, Shireen Elhabian, Ganesh Adluru, Andrew Arai, Eugene Kholmovski, Ravi Ranjan, Edward Dibella",
        "Time": "Day 3 \u2014 1:30pm-2:45pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.05"
    },
    {
        "number": 253,
        "UID": "F-253",
        "forum": "https://openreview.net/forum?id=oLTpF4mxZc",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Family of Deep Image Prior Networks for Accelerated 3D LGE-MRI Acquisition with Enhanced Reconstruction",
        "abstract": "Late Gadolinium Enhancement (LGE) MRI is essential for visualizing and treating left atrial fibrosis, but current protocols require lengthy acquisition times (7-20 minutes) and often produce suboptimal image quality. While recent advances in isotropic imaging have shown promise, scan times of 12-15 minutes still present clinical challenges. This study evaluates the efficacy of existing Deep Image Prior (DIP) frameworks for accelerated 3D LGE-MRI reconstruction. We comprehensively assess multiple DIP variants - vanilla DIP, reference-guided DIP, DIP with Total Variation, and self-guided DIP - on their ability to reconstruct high-quality isotropic (1.25mm$^3$) images from highly undersampled k-space data. Using data from 10 subjects, we demonstrate that self-guided DIP achieves superior reconstruction quality (PSNR: 32.8¬±1.2 dB, SSIM: 0.891¬±0.015 at 1/4th of acquisition time) compared to traditional compressed sensing and other DIP variants. Our evaluation shows that DIP-based reconstruction can maintain diagnostic quality with acquisition times reduced to 2-4 minutes, particularly in preserving thin left atrial wall details. These findings suggest that DIP-based methods could improve clinical workflow efficiency and patient comfort in high-resolution 3D LGE studies for atrial fibrillation patients.",
        "authors": "Md Hasibul Husain Hisham, Shireen Elhabian, Ganesh Adluru, Andrew Arai, Eugene Kholmovski, Ravi Ranjan, Edward Dibella",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - O.05"
    },
    {
        "number": 5,
        "UID": "F-5",
        "forum": "https://openreview.net/forum?id=l7XX2lclbq",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Causal PETS: Causality-Informed PET Synthesis from Multi-modal Data",
        "abstract": "The synthesis of medical images is particularly important when certain modality data are difffcult to obtain, for example, Positron emission tomography (PET). PET is crucial for diagnosing and monitoring neurological disorders. However, the availability is limited due to factors such as high costs, radiation exposure risks, and other constraints. In this study, we propose Causal PETS, a novel causality-informed synthesis model for synthesizing PET images from multi-modal data including MRI, demographic information, and cerebrospinal fluid (CSF) biomarkers. Unlike conventional approaches that involve a straightforward conversion from T1 to PET, our model analyzes the causality between different modality data and seamlessly integrates such causality into PET image generation. Through comprehensive evaluations, we demonstrate that our Causal PETS model outperforms existing non-causal methods in terms of image clarity and accuracy, particularly in identifying regions of interest critical for neurological disorders such as Alzheimer‚Äôs Disease (AD). This work underscores the importance of causal reasoning in medical image synthesis and highlights the potential of multimodal integration to advance clinical decision making.",
        "authors": "Yujia Li, Han Li, S Kevin Zhou",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.01"
    },
    {
        "number": 18,
        "UID": "F-18",
        "forum": "https://openreview.net/forum?id=6dWhSay45H",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "MedDelinea: Scalable and Efficient Medical Image Segmentation via Controllable Diffusion Transformers",
        "abstract": "We introduce MedDelinea, a novel medical image segmentation architecture that leverages a controllable module, drawing inspiration from ControlNet, within the Diffusion Transformers (DiT) framework. By doing so, we effectively address three key challenges inherent to segmentation tasks: (1) limited availability of labeled data, (2) variability in image modalities, and (3) the need for precise boundary delineation. MedDelinea is pre-trained on a large-scale medical dataset, thereby mitigating overfitting risks and enabling efficient transfer across diverse imaging scenarios with minimal fine-tuning requirements. The modular design of MedDelinea facilitates scalable and efficient computation, while maintaining high-quality segmentation performance in both supervised and zero-shot settings. Through extensive empirical evaluations on multiple datasets, we demonstrate that MedDelinea outperforms existing state-of-the-art segmentation approaches, showcasing its potential for robust and accurate medical image analysis",
        "authors": "Gayatri Deshmukh, Onkar Kishor Susladkar, Debesh Jha, Elif Keles, Halil Ertugrul Aktas, alpay medetalibeyoglu, Daniela P. Ladner, Amir A. Borhani, Gorkem Durak, Ulas Bagci",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.02"
    },
    {
        "number": 21,
        "UID": "F-21",
        "forum": "https://openreview.net/forum?id=QhhFw2vP1d",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "MedCL: Learn Consistent Anatomy Distribution for Scribble-supervised Medical Image Segmentation",
        "abstract": "Curating large-scale fully annotated datasets is expensive, laborious, and cumbersome, especially for medical images.  Several methods have been proposed in the literature that make use of weak annotations in the form of scribbles. However, these approaches require large amounts of scribble annotations, and are only applied to the segmentation of regular organs, which are often unavailable for the disease species that fall in the long-tailed distribution. Motivated by the fact that the medical labels have anatomy distribution priors, we propose a scribble-supervised clustering-based framework, called MedCL, to learn the inherent anatomy distribution of medical labels. Our approach consists of two steps:i) Shuffle the features with intra- and inter-image mix operations, and ii) Perform feature clustering and regularize the anatomy distribution at both local and global levels. Combined with a small amount of weak supervision,  the proposed MedCL is able to segment both regular organs and challenging irregular pathologies. We implement MedCL based on SAM and UNet backbones, and evaluate the performance on three open datasets of regular structure (MSCMRseg), multiple organs (BTCV) and irregular pathology (MyoPS). It is shown that even with less scribble supervision, MedCL substantially outperforms the conventional segmentation methods. Our code will be released upon acceptance.",
        "authors": "Ke Zhang, Vishal M. Patel",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.03"
    },
    {
        "number": 36,
        "UID": "F-36",
        "forum": "https://openreview.net/forum?id=yTjotBI30L",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models",
        "abstract": "Phrase grounding, *i.e.*, mapping natural language phrases to specific image regions, holds significant potential for disease localization in medical imaging through clinical reports. While current state-of-the-art methods rely on discriminative, self-supervised contrastive models, we demonstrate that generative text-to-image diffusion models, leveraging cross-attention maps, can achieve superior zero-shot phrase grounding performance. \nContrary to prior assumptions, we show that fine-tuning diffusion models with a frozen, domain-specific language model, such as CXR-BERT, substantially outperforms domain-agnostic counterparts. This setup achieves remarkable improvements, with mIoU scores doubling those of current discriminative methods. These findings highlight the underexplored potential of generative models for phrase grounding tasks. \nTo further enhance performance, we introduce Bimodal Bias Merging (BBM), a novel post-processing technique that aligns text and image biases to identify regions of high certainty. BBM refines cross-attention maps, achieving even greater localization accuracy. Our results establish generative approaches as a more effective paradigm for phrase grounding in the medical imaging domain, paving the way for more robust and interpretable applications in clinical practice. The source code and model weights are available at https://github.com/Felix-012/generate_to_ground.",
        "authors": "Felix N√ºtzel, Mischa Dombrowski, Bernhard Kainz",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.04"
    },
    {
        "number": 45,
        "UID": "F-45",
        "forum": "https://openreview.net/forum?id=lAQ29DUZCa",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "I2I-Galip: Unsupervised Medical Image Translation Using Generative Adversarial CLIP",
        "abstract": "Unpaired image-to-image translation is a challenging task due to the absence of paired examples, which complicates learning the complex mappings between the distinct distributions of the source and target domains. One of the most commonly used approaches for this task is cycle-consistent models which require the training of a new pair of generator-discriminator networks for each translation. In this paper, we propose a new image-to-image translation framework named Image-to-Image-Generative-Adversarial-CLIP (I2I-Galip) where we utilize pre-trained multi-model foundation models to mitigate the need of separate generator-discriminator pairs for each source-target mapping while achieving better and more efficient multi-domain translation. By utilizing the massive knowledge gathered during pre-training a foundation model, our approach makes use of a single lightweight generator network with ~13M parameters for the multi-domain image translation task. Comprehensive experiments on translation performance in public MRI and CT datasets show the superior performance of the proposed framework over the existing approaches.",
        "authors": "Yilmaz Korkmaz, Vishal M. Patel",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.05"
    },
    {
        "number": 49,
        "UID": "F-49",
        "forum": "https://openreview.net/forum?id=DSG6HJ65a6",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning",
        "abstract": "Automating medical report generation from histopathology images is a critical challenge requiring effective visual representations and domain-specific knowledge. Inspired by the common practices of human experts, we propose an in-context learning framework called PathGenIC that integrates context derived from the training set with a multimodal in-context learning (ICL) mechanism. Our method dynamically retrieves semantically similar whole slide image (WSI)-report pairs and incorporates adaptive feedback to enhance contextual relevance and generation quality. Evaluated on the HistGen benchmark, the framework achieves state-of-the-art results, with significant improvements across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across diverse report lengths and disease categories. By maximizing training data utility and bridging vision and language with ICL, our work offers a solution for AI-driven histopathology reporting, setting a strong foundation for future advancements in multimodal clinical applications.",
        "authors": "Shih-Wen Liu, Hsuan-Yu Fan, Wei-Ta Chu, Fu-En Yang, Yu-Chiang Frank Wang",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.06"
    },
    {
        "number": 64,
        "UID": "F-64",
        "forum": "https://openreview.net/forum?id=pNXQ0yzn9b",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Joint Supervised and Self-supervised Learning for MRI Reconstruction",
        "abstract": "Magnetic Resonance Imaging (MRI) is a crucial modality but, its inherently slow acquisition process poses challenges in obtaining fully-sampled $k$-space data under motion. The lack of fully-sampled acquisitions, serving as ground truths, complicates the training of deep learning (DL) algorithms in a supervised manner.  To address this limitation, self-supervised learning (SSL) methods have emerged as a viable alternative, leveraging available subsampled $k$-space data to train neural networks for MRI reconstruction. Nevertheless, these approaches often fall short when compared to supervised learning (SL). We propose Joint Supervised and Self-supervised Learning (JSSL), a novel training approach for DL-based MRI reconstruction algorithms aimed at enhancing reconstruction quality in cases where target datasets containing fully-sampled $k$-space measurements are unavailable. JSSL operates by simultaneously training a model in a SSL setting, using subsampled data from the target dataset(s), and in a SL manner, utilizing proxy datasets with fully-sampled $k$-space data. We demonstrate JSSL's efficacy using two distinct combinations of target and proxy data. Quantitative and qualitative results showcase substantial improvements over conventional SSL methods. Furthermore, we provide \"rule-of-thumb\" guidelines for training MRI reconstruction models. Our code is available at https://github.com/NKI-AI/direct.",
        "authors": "George Yiasemis, Nikita Moriakov, Clara I. S√°nchez, Jan-Jakob Sonke, Jonas Teuwen",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.07"
    },
    {
        "number": 77,
        "UID": "F-77",
        "forum": "https://openreview.net/forum?id=PBdGho6avn",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Advancing Medical Image Segmentation with Self-Supervised Learning: A 3D Student-Teacher Approach for Cardiac and Neurological Imaging",
        "abstract": "We propose 3D-SegSync, a novel self-supervised learning (SSL) framework designed to improve segmentation accuracy for both cardiac and neurological structures. 3D-SegSync combines the state-of-the-art DINOv2 student-teacher model architecture with a 3D Vision-LSTM (xLSTM) backbone, which excels at capturing spatiotemporal dependencies and complex anatomical patterns. The SSL phase leverages large-scale unlabeled datasets to pre-train the model, while fine-tuning on labeled data ensures excellent performance across multiple imaging modalities, including CT and MRI. Our framework achieves state-of-the-art results in cardiac and brain image segmentation. 3D-SegSync sets a new benchmark for robust, modality-agnostic medical image segmentation. Code can be found here: https://github.com/Moona-Mazher/3D-SegSync SSL.",
        "authors": "Moona Mazher, Daniel C. Alexander, Abdul Qayyum, Steven A Niederer",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.08"
    },
    {
        "number": 102,
        "UID": "F-102",
        "forum": "https://openreview.net/forum?id=Is2LjSJTMi",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Mapping Functional Language Areas with non-Functional Brain MRI",
        "abstract": "Mapping eloquent brain areas has become a standard of care in brain surgery. Current imaging-based techniques usually rely on functional MRI (fMRI), which measures neural activity via the blood oxygenation level-dependent signal. fMRI protocols are time-intensive, require active patient collaboration, and involve laborious manual post-processing and expertise, making them difficult to implement in some clinical scenarios. In this research, we propose a fully automated deep neural pipeline for the mapping of Broca and Wernicke functional language areas using multiple non-functional MRI modalities. The proposed method is evaluated on a cohort of 30 drug-resistant epilepsy patients, showing encouraging qualitative and quantitative results and suggesting its potential applicability as an effective and practical tool for neurosurgical planning and navigation.",
        "authors": "Omri Leshem, Atira Sara Bick, Nahum Kiryati, Netta Levin, Arnaldo Mayer",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.09"
    },
    {
        "number": 106,
        "UID": "F-106",
        "forum": "https://openreview.net/forum?id=82zBApu3Hk",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "DiffRGenNet: Difference-aware Medical Report Generation",
        "abstract": "Medical report generation is a critical task in healthcare, aiming to automatically pro-\nduce accurate diagnostic reports from medical images, thereby alleviating the burden on\nradiologists. However, due to the high similarity among medical images of the same anatom-\nical region and the substantial variations captured from the same region across different time\npoints for individual patients, capturing these differences poses a significant challenge. We\npropose a Difference-aware Report Generation Network (DiffRGenNet), which retrieves\nsimilar reports through image search, identifies differences using the Feature Diff module,\nand dynamically orchestrates global and local dependencies via the FlexiRoute Aggregation\nModule to determine the optimal routing path for each sample, selecting the most suitable\nreport to describe the variations and connections. Finally, by leveraging the consistency\nof classification information and the discrepancy information from the diff module, DiffR-\nGenNet enhances the ability to learn differences in rare diseases, generating more precise\nreports. Experiments demonstrate that DiffRGenNet outperforms existing methods on the\nMIMIC-CXR and IU X-Ray datasets, confirming its effectiveness and potential.",
        "authors": "Minghao Bian, Kun Zhang, Dexin Zhao, S Kevin Zhou",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.10"
    },
    {
        "number": 108,
        "UID": "F-108",
        "forum": "https://openreview.net/forum?id=iajnNQ1u3Y",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "IdFOPNet: Integrating Identity Attention and Fairness  Optimization in Anatomical Landmark Detection",
        "abstract": "Fairness is the key to addressing bias in anatomical landmark point detection. Existing methods tend to ignore individual identity information, resulting in significant bias exhibited between different age groups and genders. Moreover, current studies mainly focus on improving the accuracy of models and lack the dynamic optimisation mechanism of fairness, resulting in the bias problem not being solved effectively. To this end, we propose an anatomical landmark detection network that integrates Identity features and Fairness OPtimization (IdFOPNet). This method leverages a prototype network to detect landmarks by comparing image features with a set of global landmark prototypes. To enhance model fairness, we introduce the Identity Attention mechanism, incorporating identity information as prior knowledge into the detection process. Additionally, we design a penalty-based gradient modulation strategy to dynamically suppress the model‚Äôs over-reliance on specific biased information during training. We evaluate the IdFOPNet on the CephAdoAdu and Hand X-Rays datasets. Extensive experimental results demonstrate that our method outperforms SOTA approaches in anatomical landmark detection across different ages and genders.\n  Keywords: Anatomical landmark detection, Identity attention mechanism, Fairness optimization, Penalty-based gradient modulation.",
        "authors": "Dexin Zhao, Hongbo Ye, Minghao Bian, Wu Yuxin, S Kevin Zhou",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.11"
    },
    {
        "number": 129,
        "UID": "F-129",
        "forum": "https://openreview.net/forum?id=QbBPeAIdrk",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "A Vision Foundation Model for Cataract Surgery Using Joint-Embedding Predictive Architecture",
        "abstract": "Vision foundation models can automate analysis of surgical videos and enable multiple applications that support patient care and surgical training. For cataract surgery, existing models are limited by reliance on small datasets, privacy concerns, and poor generalizability across surgical settings. In this paper, we introduce JHU-VPT(JEPA), a self-supervised vi- sion foundation model leveraging Joint-Embedding Predictive Architecture (JEPA) to learn spatiotemporal representations via latent feature prediction on a large corpus of unlabeled cataract videos, without requiring extensive labeled datasets or pixel-level reconstruction. JHU-VPT(JEPA) is pretrained on 2591 videos from multiple sites that capture different surgical technique and style variations. Comprehensive evaluations on step recognition, sur- gical feedback, and skill assessment tasks demonstrate that JHU-VPT(JEPA) outperforms existing methods. JHU-VPT(JEPA)‚Äôs effectiveness is evident even when using attentive probing with a frozen encoder, highlighting the robustness of the learned features and ad- dressing privacy concerns by not requiring access to raw videos during downstream tasks. Our approach offers a scalable, generalizable, and privacy-preserving solution for surgical video analysis, with significant potential to advance patient care and surgical education.",
        "authors": "Nisarg A Shah, Mingze Xia, Subhasri Vijay, Shameema Sikder, S. Swaroop Vedula, Vishal M. Patel",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.12"
    },
    {
        "number": 141,
        "UID": "F-141",
        "forum": "https://openreview.net/forum?id=tLYEAmMupF",
        "Track": "Full paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Co-distilled attention guided masked image modeling with noisy teacher for self-supervised learning on medical images",
        "abstract": "Masked image modeling (MIM) is a highly effective self-supervised learning (SSL) approach to extract useful feature representations from unannotated data. Predominantly used random masking methods make SSL less effective for medical images due to the contextual similarity of neighboring patches, leading to information leakage and SSL simplification. Hence, we propose an attention guided masking mechanism within a co-distillation learning framework to selectively mask semantically co-occurring and discriminative patches, aiming to reduce information leakage and increase the difficulty of SSL pretraining. However, attention guided masking inevitably reduces the diversity of attention heads, which negatively impacts downstream task performance. To address this, we integrate a noisy teacher into the co-distillation framework (termed DAGMaN) to enable attentive masking while preserving high attention head diversity. We demonstrate the capability of DAGMaN on multiple tasks including full- and few-shot lung nodule classification, immunotherapy outcome prediction,  tumor segmentation, and unsupervised clustering of organs.",
        "authors": "Jue Jiang, Aneesh Rangnekar, Harini Veeraraghavan",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - F.13"
    },
    {
        "number": 6,
        "UID": "S-6",
        "forum": "https://openreview.net/forum?id=I13Y1nU6gs#discussion",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "UNIFORM: A Unified Deep Learning Framework for Multi-organ and Multi-contrast MRI Reconstruction",
        "abstract": "MRI is an essential medical imaging modality, yet long acquisition times and organ-specific reconstruction methods often hinder clinical efficiency. In this paper, we propose training a unified deep learning framework (UNIFORM) for reconstructing undersampled multi-coil MRI data across diverse anatomical sites and multiple contrasts. Leveraging a state-of-the-art MRI reconstruction algorithm (vSHARP), UNIFORM was trained on diverse multi-coil $k$-space datasets, including knee, brain, prostate, and cardiac MRI. Evaluated across multiple acceleration factors ($2\\times, 4\\times, 6\\times, 8\\times$), it demonstrated robust performance in terms of quantitative evaluation. Additionally, UNIFORM supports zero-shot self-supervised learning (SSL), enabling effective reconstruction of unseen organs. Zero-shot SSL experiments were conducted on prospectively undersampled breast MRI acquisitions at high acceleration factors ($10\\times, 17\\times$), demonstrating improved anatomical detail and reduced noise compared to conventional zero-filling approaches. UNIFORM offers a promising avenue for clinically robust, accelerated multi-organ and multimodal MRI workflows.",
        "authors": "George Yiasemis, Jonatan Ferm, Nikita Moriakov, Ritse Mann, Jan-Jakob Sonke, Jonas Teuwen",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.01"
    },
    {
        "number": 9,
        "UID": "S-9",
        "forum": "https://openreview.net/forum?id=mnbihKbPTc",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Self-supervised Semantic Texture Decomposition for Ultrasound Segmentation and Analysis",
        "abstract": "Ultrasound is an especially challenging modality to interpret because it requires unique domain knowledge to draw conclusions on the anatomy based on analysis of B-mode intensity. For this reason, there is great value in transforming B-mode images to a color scheme that is more closely aligned with the anatomy and its unique tissue properties like speed-of-sound and scattering coefficient to simplify ultrasound analysis. In this work, we introduce texture ultrasound semantic analysis (TUSA), a self-supervised transformer model trained to decompose B-mode ultrasound into distinct channels that are defined by the texture they represent. We train our model on 10 freely available ultrasound datasets and demonstrate superior segmentation performance and consistency compared to training on B-mode intensity on an additional 11th dataset. We conclude that by incorporating TUSA into the training pipeline, downstream models can focus on recognizing the anatomy instead of extracting features from intensity",
        "authors": "Tal Grutman, Tali Ilovitsh",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.02"
    },
    {
        "number": 11,
        "UID": "S-11",
        "forum": "https://openreview.net/forum?id=RY54DHewSk",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Comprehensive Evaluation of Unsupervised Image Enhancement for Volumetric Fetal Brain MRI",
        "abstract": "MRI provides superior soft tissue contrast over ultrasound, making it essential for evaluating fetal brain development and pathology. However, clinical use of 2D thick-slice T2-weighted imaging remains constrained by motion-induced artifacts that degrade both image quality and subsequent quantitative analyses. While existing volumetric reconstruction pipelines (e.g., NiftyMIC, NeSVoR) incorporate motion correction, their outputs often retain residual noise due to a lack of post-reconstruction enhancement solutions. Although the recently proposed foundation model BME-X represents the first dedicated approach for fetal MRI enhancement, its generalizability to heterogeneous clinical datasets remains unproven. To bridge this gap, we conduct the first comprehensive comparison of BME-X and other unsupervised image enhancement methods on normal and pathological fetal brain MRI, based on tissue segmentation accuracy, tissue contrast t-score (TCT), lesion fidelity, and reader assessment. Results show that a pre-trained 3D convolutional variational autoencoder (VAE) achieves more effective enhancement compared to BME-X. Code and pre-trained weights are available at: https://github.com/yingqihao2022/FetalBrainEnhancement.",
        "authors": "Yingqi Hao, Mingxuan Liu, Hongjia Yang, Haoxiang Li, Xuguang Bai, Yi Liao, Haibo Qu, Qiyuan Tian",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.03"
    },
    {
        "number": 17,
        "UID": "S-17",
        "forum": "https://openreview.net/forum?id=z2awA18iWY",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "A-IDE : Agent-Integrated Denoising Experts",
        "abstract": "Recent advances in deep learning-based denoising methods have improved the quality of Low-Dose CT (LDCT) images. However, due to anatomical variability and limited data availability, a single model often struggles to generalize effectively across multiple anatomical regions. To address this limitation, we propose the Agent-Integrated Denoising Experts (A-IDE) framework. A-IDE integrates three region-specialized RED-CNN models under the control of a decision-making large language model (LLM) agent. This agent analyzes anatomical priors extracted from BiomedCLIP and dynamically routes incoming LDCT scans to the most suitable specialized model. We highlight three major advantages. First, A-IDE shows robust performance in heterogeneous and data-scarce environments. Second, the framework reduces risk of overfitting by distributing tasks among multiple experts. Finally, the fully automated agent-driven routing eliminates the need for manual intervention. Experimental results in the Mayo-2016 dataset confirm that A-IDE achieves superior performance in RMSE, PSNR, and SSIM compared to a single unified denoiser.",
        "authors": "Uihyun Cho, Namhun Kim",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.04"
    },
    {
        "number": 19,
        "UID": "S-19",
        "forum": "https://openreview.net/forum?id=c1FPI1OKMK",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Deep Implicit Neural Representations for End-to-End Anatomical Shape Estimation from Volumetric Images",
        "abstract": "We present ImplicitMeshNet, an end-to-end approach for anatomical shape estimation from volumetric images using deep implicit neural representations. Our neural network directly reconstructs shapes as 3D meshes and is trained on voxel-based segmentation maps by utilizing a deep signed distance field transform, eliminating the need for explicit ground truth meshes. Evaluated on cardiac CT scans from the MMWHS challenge dataset, our method achieves a Dice score of 0.92 for the extraction of the left atrium and ventricle, while maintaining anatomical fidelity. This enables more accurate cardiac modeling for visualization and downstream analysis in clinical settings.",
        "authors": "Max-Heinrich Laves, Steffen Schuler, Ahmed Abbas, David Paik, Raphael Prevost, Oliver Zettinig",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.05"
    },
    {
        "number": 28,
        "UID": "S-28",
        "forum": "https://openreview.net/forum?id=cfzJZ47xBU",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Bone supression in planar X-ray images with Stable Diffusion",
        "abstract": "Bone suppression is a processing technique that aims to enhance the visualization of chest radiographic images by attenuating bones while preserving soft tissue details. This has been achieved with deep learning methods but they either introduce blurring or do not fully remove bones. In this work, we propose a bone removal method for radiography based on Stable Diffusion. To address the lack of publicly available bone suppression datasets, the model is pre-trained using a synthetic dataset simulated from computed tomography scans. Preliminary evaluation demonstrates the ability of the proposed model to fully remove bones while preserving spatial resolution.",
        "authors": "Daniel Sanderson, Mariana Elizalde, Paula Ochotorena, Manuel Desco, Monica Abella",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.06"
    },
    {
        "number": 30,
        "UID": "S-30",
        "forum": "https://openreview.net/forum?id=GZvMvB27ln",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Beyond segmentation: an uncertainty-aware, end-to-end approach to functional lung image quantification",
        "abstract": "Functional lung imaging modalities, such as hyperpolarized gas MRI, facilitate the visualization and quantification of regional lung ventilation. The ventilation defect percentage (VDP) is a highly-sensitive biomarker for quantifying small changes in lung function, derived from spatially co-registered functional hyperpolarized Xenon-129 ($^1$$^2$$^9$Xe)-MRI and structural proton ($^1$H)-MRI. However, manual-editing associated with segmentation-based workflows represents a time-consuming obstacle to delivering functional lung MRI results to clinicians. End-to-end deep learning (DL), which predicts final outputs without intermediary steps, frequently demonstrates improved performance on computer vision tasks; however, intermediary steps can no longer be interrogated. In this work, we developed the first end-to-end, uncertainty-aware DL framework for directly predicting VDP and its associated confidence. The direct prediction of VDP can potentially provide clinicians with important clinical data faster than segmentation-based methods.",
        "authors": "Joshua R Astley, Helen Marshall, Laurie J Smith, Alberto M Biancardi, Guilhem J Collier, Laura C Saunders, Jim M Wild, Bilal Tahir",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.07"
    },
    {
        "number": 34,
        "UID": "S-34",
        "forum": "https://openreview.net/forum?id=KZXJDxQegd",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Introducing Beyond FA: Crowdsourcing Imaging Biomarkers for Alzheimer's Disease",
        "abstract": "The inclusion of diffusion weighted magnetic resonance imaging (DW-MRI) in neuroimaging\nstudies has enabled deeper understanding of white matter structure of the human brain.\nFractional anisotropy (FA), a commonly used metric for assessing white matter integrity,\noffers high sensitivity but often suffers from low specificity in detecting pathology. FA‚Äôs\nlimitations as a biomarker are often reflected in its inconsistency across different age\ngroups and disease stages, especially in heterogeneous populations. With the emergence\nof alternative white matter models and metrics‚Äîsuch as complex network measures and\ntract-specific tractography‚Äîresearchers now have access to various methods for assessing\nwhite matter integrity, especially in the context of lower-quality and heterogeneous data.\nGiven the growing number of white matter models and metrics, there is a pressing need\nto explore and evaluate these metrics in order to move beyond the constraints of FA. To\naddress this, we introduce Beyond FA, an open challenge that invites teams to submit the\nimaging biomarker(s) of their choice. The goal is to compare and gain deeper understanding\nof white matter metrics and their association with Alzheimer‚Äôs Disease (AD).",
        "authors": "Elyssa M. McMaster, Nancy Newlin, Trent Schwartz, Adam M. Saunders, Gaurav Rudravaram, Yehyun Suh, Jongyeon Yoon, Michael Eugene Kim, Chloe Cho, Karthik Ramadass, Yihao Liu, Lianrui Zuo, Eleftherios Garyfallidis, Talia M. Nir, Neda Jahanshad, Kurt Schilling, Daniel Moyer, Bennett Allan Landman",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.08"
    },
    {
        "number": 71,
        "UID": "S-71",
        "forum": "https://openreview.net/forum?id=L5xyzjMCwd",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Zero-shot Gait Classification with Diffusion Models",
        "abstract": "Movement disorders such as Parkinson‚Äôs disease are characterised by complex abnormalities of body motion that resist precise, replicable, and scalable quantification. Subjective clinical scores--the established standard--are limited in expressivity and vulnerable to intra-observer variation; wearable sensor-based methods offer objectivity but with limited anatomical sampling. Remote video-based approaches could deliver both highly expressive and objective quantification of motion, but sufficient labelled samples are hard to obtain under clinical data regimes. Here we develop a diffusion model-based, zero-shot, and human-interpretable approach to gait assessment from video-derived pose data and evaluate it in Parkinson's Disease. Capable of detecting subtle changes in body motion without explicit training, it shows potential for an accurate, robust, and scalable solution, addressing the major limitations of existing methods.",
        "authors": "Xiaodong Guan, Robert James Gray, Ashwani Jha, Parashkev Nachev",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.09"
    },
    {
        "number": 73,
        "UID": "S-73",
        "forum": "https://openreview.net/forum?id=fKxgUmAZx6",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Physics-Informed Neural Network for Quantifying Time-Encoded Arterial Spin Labeling: A Simulation Study",
        "abstract": "Arterial Spin Labeling (ASL) MRI enables non-invasive quantification of cerebral perfusion. Hadamard time-encoding improves acquisition efficiency and allows the simultaneous estimation of cerebral blood flow (CBF) and arterial transit time (ATT) via the Buxton model. Physics-informed neural networks (PINNs) integrate physical laws into neural networks, improving parameter estimation under noisy and sparse data conditions. We propose a two-stage PINN framework trained on synthetic ASL data from the Boston ASL Template and Simulator. Leveraging coupled neural networks and differential equation constraints, our method produces smoother and more robust CBF and ATT maps compared to regularized nonlinear least squares (NLLS), demonstrating its potential for clinical ASL quantification. While this work focuses on simulation data, it represents a first step toward extending such models to in vivo applications using a similar architecture.",
        "authors": "Alessandro Giupponi, Chiara Da Villa, Mattia Veronese, Marco Castellaro",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.10"
    },
    {
        "number": 89,
        "UID": "S-89",
        "forum": "https://openreview.net/forum?id=q5WRufIHlm",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Cycle-CTFlow: A CycleGAN-Normalizing Flow Harmonization Framework for Improved Nodule Detection",
        "abstract": "Although many deep learning models exist for nodule detection, characterization, and other related tasks, their widespread clinical adoption is hindered by substantial variability in computed tomography (CT) acquisitions. Differences in scanner hardware, reconstruction methods, dose levels, and patient demographics cause domain shifts, often leading models trained on one dataset to underperform on another. This highlights the need for harmonization and adaptation strategies to ensure consistent performance across diverse clinical settings. To address this challenge, we propose a training scheme in which knowledge of the downstream model‚Äôs training distribution is incorporated into the training process of the harmonization model. The goal is to guide the harmonization model to transform input images so that their distribution closely aligns with that of the downstream model. We demonstrate that the proposed approach, Cycle-CTFlow, leads to improvements in nodule detection performance: a 5.2% increase in sensitivity and a 2.6% increase in CPM compared to no harmonization on the MiniDeepLesion dataset, and a 1.9% increase in sensitivity and an 8.5% increase in CPM on the UCLA in-house dataset.",
        "authors": "Anil Yadav, Nathan Tran, Spencer Welland, John M. Hoffman, Grace Hyun J. Kim, Ashley E. Prosper, Denise R. Aberle, Michael McNitt-Gray, William Hsu",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.11"
    },
    {
        "number": 106,
        "UID": "S-106",
        "forum": "https://openreview.net/forum?id=fn8aDbnylG",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "Self-Supervised Cortical Surface Reconstruction for Ultra High-resolution ex vivo 7T MRI",
        "abstract": "Ex vivo brain MRI enables sub-millimeter ultra-high-resolution studies, uncovering structural details unattainable with in vivo MRI. Cortical surface reconstruction (CSR) based on these detailed images is crucial for studying cortical anatomy and structure. Despite this potential, methodological development in ex vivo MRI has been constrained by several factors: scarcity of datasets, limited imaging resources, pronounced susceptibility artifacts, and signal inhomogeneity. While learning-based CSR methods have been proposed to accelerate reconstruction processes, they face a fundamental limitation‚Äîrequiring CSR results from classic methods like FreeSurfer as training references, making them mostly only suitable for in vivo adult MRI data and unsuitable for the unique characteristics of ex vivo brain imaging. To address this challenge, we propose SelfCSR, a self-supervised deep learning framework for accurate ex vivo 7T MRI CSR without the need for manually labeled training data.",
        "authors": "Haoxiang Li, Mingxuan Liu, Hongjia Yang, Yi Liao, Haibo Qu, Jonathan R. Polimeni, Qiyuan Tian",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.12"
    },
    {
        "number": 108,
        "UID": "S-108",
        "forum": "https://openreview.net/forum?id=jv7DIZS4wG",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "DisTorch: A fast GPU implementation of  3D Hausdorff Distance",
        "abstract": "In this short paper, we present a simple yet fast implementation of the Hausdorff distance, leveraging the KeOps package. After briefly describing the core ideas of the implementation, we compare the runtime and memory cost of different implementations, on different 3D volumes from different tasks. Most notably, we achieve sub 50ms per 3D scan across tasks, while having a much simpler codebase than the fastest alternative. Our implementation is freely available under the BSD license: https://github.com/jeromerony/distorch.",
        "authors": "J√©r√¥me Rony, Hoel Kervadec",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.13"
    },
    {
        "number": 122,
        "UID": "S-122",
        "forum": "https://openreview.net/forum?id=XNNsQqs1UP",
        "Track": "Short paper",
        "Session": "Poster 3.B: Applied Imaging: Synthesis, Reconstruction & Efficiency",
        "Final Decision": "Poster",
        "title": "AIRAQc: Pre-Analytical Tool for Accurate Identification and Quantification of Artefacts in  Histopathology",
        "abstract": "Quality control in digitized Whole Slide Images (WSI) is critical to eliminate the impact of slide processing and scanning artefacts. Current artefact detection methods often lack comprehensive categorization or analyze images at a single magnification, limiting their efficacy.  AIRAQc addresses these limitations through a multi-magnification approach, training on large-scale dataset of over 2500 WSI from diverse sources. The solution yields superior performance as compared to existing approaches, testing on common artefacts types - air bubble, fold, pen mark and out-of-focus,  and low inference time of 0.2032 sec/ mm2 of tissue.",
        "authors": "Kuldeep Gautam, Geetank Raipuria, Nitin Singhal",
        "Time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster time": "Day 3 \u2014 3:00pm-4:00pm",
        "Poster ID": "Poster 3.B - S.14"
    }
]
